{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "from treeqn_traj import TreeQN\n",
    "import random\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_world(tensor,max_val): \n",
    "    assert tensor.shape[0] == tensor.shape[1]\n",
    "    val = tensor.max() + 1\n",
    "    state = torch.zeros_like(tensor).unsqueeze(0).unsqueeze(0) #to match treeqn input size\n",
    "    new_state = torch.ones_like(state)\n",
    "    middle = int(tensor.shape[0] / 2)\n",
    "    # Create transitions by modifying slices of new_state\n",
    "    new_state[:,:,:middle, :middle] += val\n",
    "    transition_one = new_state.clone()\n",
    "    new_state[:,:,middle:, :middle] += val\n",
    "    transition_two = new_state.clone()\n",
    "    new_state[:,:,:middle, middle:] += val\n",
    "    transition_three = new_state.clone()\n",
    "    new_state[:,:,middle:, middle:] += val #transition 4\n",
    "    return [transition_one/max_val, transition_two/max_val, transition_three/max_val, new_state/max_val]\n",
    "def image_world_samples(size_tensor,samples,max_val=1,x_input=-1):\n",
    "    return_data = []\n",
    "    for i in range(samples):\n",
    "        loc_tensor = torch.zeros_like(size_tensor)+0.1\n",
    "        x = int(random.random()*size_tensor.shape[0])\n",
    "        if x_input != -1:\n",
    "            x = x_input\n",
    "        loc_tensor[x] += x\n",
    "        result = image_world(loc_tensor,max_val)\n",
    "        loc_tensor = loc_tensor.unsqueeze(0).unsqueeze(0)/max_val\n",
    "        return_data.append([loc_tensor,result])\n",
    "    return return_data\n",
    "\n",
    "size_tensor = torch.zeros(20,20)\n",
    "train_data = image_world_samples(size_tensor,1000,size_tensor.shape[0],-1)\n",
    "\n",
    "test_data = [] #simply test once on all 20 possible initial states.\n",
    "for i in range(size_tensor.shape[0]):\n",
    "    test_data.append(image_world_samples(size_tensor,1,size_tensor.shape[0],i)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikef\\Desktop\\BiB Work\\treeqn\\treeqn_traj.py:33: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  b_init(module.bias, b_scale)\n"
     ]
    }
   ],
   "source": [
    "input_shape = torch.zeros(1, 1,20, 20).shape# minimum size #train_data[0][0].shape\n",
    "num_actions = 4\n",
    "tree_depth = 4\n",
    "embedding_dim = 64\n",
    "td_lambda = 0.8\n",
    "gamma = 1    #0.99\n",
    "model = TreeQN(input_shape=input_shape, num_actions=num_actions, tree_depth=tree_depth, embedding_dim=embedding_dim, td_lambda=td_lambda,gamma=gamma)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "#optimizer = RMSprop(model.parameters(), lr=1e-4,alpha =0.99, eps = 1e-5) | loss from treeqn paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikef\\AppData\\Local\\Temp/ipykernel_8388/2012049690.py:33: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([4, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  first_loss = (F.mse_loss(decoded_values[1], t[1][0], reduction='none') * first).sum()\n",
      "C:\\Users\\mikef\\AppData\\Local\\Temp/ipykernel_8388/2012049690.py:34: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([16, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  second_loss = (F.mse_loss(decoded_values[2], t[1][1], reduction='none') * second).sum()\n",
      "C:\\Users\\mikef\\AppData\\Local\\Temp/ipykernel_8388/2012049690.py:35: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([64, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  third_loss = (F.mse_loss(decoded_values[3], t[1][2], reduction='none') * third).sum()\n",
      "C:\\Users\\mikef\\AppData\\Local\\Temp/ipykernel_8388/2012049690.py:36: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([256, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  fourth_loss = (F.mse_loss(decoded_values[4], t[1][3], reduction='none') * fourth).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 188.22951885080337, Average Raw Loss: 188.22951885080337\n",
      "Epoch 2, Average Loss: 65.69384790945053, Average Raw Loss: 65.69384790945053\n",
      "Epoch 3, Average Loss: 43.64963609266281, Average Raw Loss: 43.64963609266281\n",
      "Epoch 4, Average Loss: 25.287013169288635, Average Raw Loss: 25.287013169288635\n",
      "Epoch 5, Average Loss: 14.379377325177193, Average Raw Loss: 14.379377325177193\n",
      "Epoch 6, Average Loss: 9.615210075259208, Average Raw Loss: 9.615210075259208\n",
      "Epoch 7, Average Loss: 7.31389129447937, Average Raw Loss: 7.31389129447937\n",
      "Epoch 8, Average Loss: 6.248950891703367, Average Raw Loss: 6.248950891703367\n",
      "Epoch 9, Average Loss: 5.780133644521237, Average Raw Loss: 5.780133644521237\n",
      "Epoch 10, Average Loss: 5.530201500386, Average Raw Loss: 5.530201500386\n",
      "Epoch 11, Average Loss: 5.388844492539763, Average Raw Loss: 5.388844492539763\n",
      "Epoch 12, Average Loss: 5.25323865441978, Average Raw Loss: 5.25323865441978\n",
      "Epoch 13, Average Loss: 5.20107275120914, Average Raw Loss: 5.20107275120914\n",
      "Epoch 14, Average Loss: 5.067649198710918, Average Raw Loss: 5.067649198710918\n",
      "Epoch 15, Average Loss: 5.023660962358117, Average Raw Loss: 5.023660962358117\n",
      "Epoch 16, Average Loss: 5.003615193784237, Average Raw Loss: 5.003615193784237\n",
      "Epoch 17, Average Loss: 4.936649213969708, Average Raw Loss: 4.936649213969708\n",
      "Epoch 18, Average Loss: 4.7999717261195185, Average Raw Loss: 4.7999717261195185\n",
      "Epoch 19, Average Loss: 4.729000287339091, Average Raw Loss: 4.729000287339091\n",
      "Epoch 20, Average Loss: 4.59638411217928, Average Raw Loss: 4.59638411217928\n",
      "Epoch 21, Average Loss: 4.494458175867796, Average Raw Loss: 4.494458175867796\n",
      "Epoch 22, Average Loss: 4.393969260022044, Average Raw Loss: 4.393969260022044\n",
      "Epoch 23, Average Loss: 4.253085484936833, Average Raw Loss: 4.253085484936833\n",
      "Epoch 24, Average Loss: 4.1730549923032525, Average Raw Loss: 4.1730549923032525\n",
      "Epoch 25, Average Loss: 4.100013273671269, Average Raw Loss: 4.100013273671269\n",
      "Epoch 26, Average Loss: 4.015573509588838, Average Raw Loss: 4.015573509588838\n",
      "Epoch 27, Average Loss: 3.9563032538294793, Average Raw Loss: 3.9563032538294793\n",
      "Epoch 28, Average Loss: 3.8787858303785323, Average Raw Loss: 3.8787858303785323\n",
      "Epoch 29, Average Loss: 3.812659671127796, Average Raw Loss: 3.812659671127796\n",
      "Epoch 30, Average Loss: 3.7685646377131343, Average Raw Loss: 3.7685646377131343\n",
      "Epoch 31, Average Loss: 3.726839742228389, Average Raw Loss: 3.726839742228389\n",
      "Epoch 32, Average Loss: 3.6979696037396788, Average Raw Loss: 3.6979696037396788\n",
      "Epoch 33, Average Loss: 3.6232642235830426, Average Raw Loss: 3.6232642235830426\n",
      "Epoch 34, Average Loss: 3.6113239739015697, Average Raw Loss: 3.6113239739015697\n",
      "Epoch 35, Average Loss: 3.583330740511417, Average Raw Loss: 3.583330740511417\n",
      "Epoch 36, Average Loss: 3.5459610055461526, Average Raw Loss: 3.5459610055461526\n",
      "Epoch 37, Average Loss: 3.4993251379057764, Average Raw Loss: 3.4993251379057764\n",
      "Epoch 38, Average Loss: 3.463975446231663, Average Raw Loss: 3.463975446231663\n",
      "Epoch 39, Average Loss: 3.3992634749636053, Average Raw Loss: 3.3992634749636053\n",
      "Epoch 40, Average Loss: 3.3579875809326767, Average Raw Loss: 3.3579875809326767\n",
      "Epoch 41, Average Loss: 3.296764762878418, Average Raw Loss: 3.296764762878418\n",
      "Epoch 42, Average Loss: 3.2330908894240857, Average Raw Loss: 3.2330908894240857\n",
      "Epoch 43, Average Loss: 3.1479760663583876, Average Raw Loss: 3.1479760663583876\n",
      "Epoch 44, Average Loss: 3.0817492053583266, Average Raw Loss: 3.0817492053583266\n",
      "Epoch 45, Average Loss: 3.00165824405849, Average Raw Loss: 3.00165824405849\n",
      "Epoch 46, Average Loss: 2.9196106497570873, Average Raw Loss: 2.9196106497570873\n",
      "Epoch 47, Average Loss: 2.832586057141423, Average Raw Loss: 2.832586057141423\n",
      "Epoch 48, Average Loss: 2.7517618513181805, Average Raw Loss: 2.7517618513181805\n",
      "Epoch 49, Average Loss: 2.688410565711558, Average Raw Loss: 2.688410565711558\n",
      "Epoch 50, Average Loss: 2.619800737157464, Average Raw Loss: 2.619800737157464\n"
     ]
    }
   ],
   "source": [
    "#Main training loop\n",
    "#Looking at difference between detaching at each transition or not in treeqn file. This is with detach (so far seems to make no diff)\n",
    "for epoch in range(50):  # epochs\n",
    "    avg_loss = 0\n",
    "    temp_loss = 0\n",
    "    temp_raw_loss = 0\n",
    "    sample_count = 0\n",
    "\n",
    "    avg_raw_loss = 0\n",
    "\n",
    "    for t in random.sample(train_data, len(train_data)): #sample through all data in random order each epoch\n",
    "        #Get reconstruction loss to help ground abstract state\n",
    "        decoded_values, all_policies = model(t[0])\n",
    "        decode_loss = F.mse_loss(decoded_values[0], t[0], reduction='sum')\n",
    "\n",
    "        #Get transition probabilities for each state\n",
    "        first_policy = all_policies[0]\n",
    "        second_policy = all_policies[1].view(4, -1)\n",
    "        third_policy = all_policies[2].view(4, 4, -1)\n",
    "        fourth_policy = all_policies[3].view(4, 4, 4, -1)\n",
    "\n",
    "        #These should all add to 1 (in testing there seems to be some small rounding error)\n",
    "        second_layer_probs = first_policy * second_policy   \n",
    "        third_layer_probs = second_layer_probs * third_policy\n",
    "        fourth_layer_probs = third_layer_probs * fourth_policy\n",
    "        \n",
    "        #Flatten transition probabilities to then weigh with loss of each predicted state at each layer\n",
    "        first = torch.flatten(first_policy).view(4, 1, 1, 1)\n",
    "        second = torch.flatten(second_layer_probs).view(16, 1, 1, 1)\n",
    "        third = torch.flatten(third_layer_probs).view(64, 1, 1, 1)\n",
    "        fourth = torch.flatten(fourth_layer_probs).view(256, 1, 1, 1) \n",
    "        \n",
    "        first_loss = (F.mse_loss(decoded_values[1], t[1][0], reduction='none') * first).sum() \n",
    "        second_loss = (F.mse_loss(decoded_values[2], t[1][1], reduction='none') * second).sum() \n",
    "        third_loss = (F.mse_loss(decoded_values[3], t[1][2], reduction='none') * third).sum() \n",
    "        fourth_loss = (F.mse_loss(decoded_values[4], t[1][3], reduction='none') * fourth).sum() \n",
    "\n",
    "\n",
    "        #For experimenting with different weights on different layers\n",
    "        raw_loss = (decode_loss + first_loss + second_loss + third_loss + fourth_loss).detach().item()\n",
    "        l2w , l3w ,l4w = 1,1,1\n",
    "        total_loss = decode_loss + first_loss + second_loss*l2w + third_loss*l3w + fourth_loss*l4w\n",
    "\n",
    "        temp_loss += total_loss\n",
    "        temp_raw_loss += raw_loss\n",
    "        sample_count += 1\n",
    "\n",
    "        if sample_count % 1 == 0:\n",
    "            optimizer.zero_grad()\n",
    "            temp_loss.backward()\n",
    "\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            avg_loss += temp_loss.item()\n",
    "            avg_raw_loss += temp_raw_loss\n",
    "            temp_loss = 0\n",
    "            temp_raw_loss = 0\n",
    "\n",
    "    # To handle the case where the number of samples is not a multiple of 10\n",
    "    if sample_count % 1 != 0:\n",
    "        optimizer.zero_grad()\n",
    "        temp_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        avg_loss += temp_loss.item()\n",
    "        avg_raw_loss += temp_raw_loss\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss / len(train_data)}, Average Raw Loss: {avg_raw_loss / len(train_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Weight Sums tensor([-3.3220, -2.9970,  3.0080, -6.8990])\n"
     ]
    }
   ],
   "source": [
    "#View Action Weights (This hasn't been informative yet)\n",
    "dec, all_policies = model(train_data[0][0]) \n",
    "# dot = make_dot((dec[0],dec[1],dec[2],dec[3],dec[4],all_policies[0],all_policies[1],all_policies[2],all_policies[3]),params=dict(model.named_parameters()))\n",
    "# dot.render('model', format='png')\n",
    "print(f\"Action Weight Sums {torch.round(model.transition_fun.data,decimals=3).sum(dim=0).sum(dim=0)}\")  #might be summing the wrong way, or just not interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Actions: 2 1 0 0\n"
     ]
    }
   ],
   "source": [
    "best_first_action = all_policies[0].argmax()\n",
    "best_second_action = all_policies[1].view(4,-1)[best_first_action].argmax() \n",
    "best_third_action = all_policies[2].view(4,4,-1)[best_first_action][best_second_action].argmax()\n",
    "best_fourth_action = all_policies[3].view(4,4,4,-1)[best_first_action][best_second_action][best_third_action].argmax() \n",
    "# print(torch.round(all_q[0],decimals=3).detach(), f\"Argmax {all_q[0].argmax().item()}\")\n",
    "# print(torch.round(all_q[1],decimals=3).view(4,-1).detach(),f\"Argmax {all_q[1].view(4,-1)[1].argmax().item()}\")\n",
    "# print(torch.round(all_q[2],decimals=3).view(4,4,-1)[0].detach(),f\"Argmax {all_q[2].view(4,4,-1)[1][0].argmax().item()}\")\n",
    "# print(torch.round(all_q[3],decimals=3).view(4,4,4,-1)[0][0].detach(),f\"Argmax {all_q[3].view(4,4,4,-1)[1][1][0].argmax().item()}\")\n",
    "print(f\"Best Actions: {best_first_action.item()} {best_second_action.item()} {best_third_action.item()} {best_fourth_action.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikef\\AppData\\Local\\Temp/ipykernel_8388/438136519.py:23: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([4, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  first_loss = (F.mse_loss(dec[1], i[1][0], reduction='none') * first).sum()\n",
      "C:\\Users\\mikef\\AppData\\Local\\Temp/ipykernel_8388/438136519.py:24: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([16, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  second_loss = (F.mse_loss(dec[2], i[1][1], reduction='none') * second).sum()\n",
      "C:\\Users\\mikef\\AppData\\Local\\Temp/ipykernel_8388/438136519.py:25: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([64, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  third_loss = (F.mse_loss(dec[3], i[1][2], reduction='none') * third).sum()\n",
      "C:\\Users\\mikef\\AppData\\Local\\Temp/ipykernel_8388/438136519.py:26: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([256, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  fourth_loss = (F.mse_loss(dec[4], i[1][3], reduction='none') * fourth).sum()\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "loss_data = []\n",
    "with torch.no_grad():\n",
    "    eval_loss = 0\n",
    "    for i,k in zip(test_data,range(len(test_data))):\n",
    "        dec, all_q = model(i[0])\n",
    "        #decode_loss = torch.abs(i[0] - dec[0]).sum()\n",
    "        decode_loss = F.mse_loss(dec[0], i[0], reduction='sum')\n",
    "        first = all_q[0]\n",
    "        second = all_q[1].view(4, -1)\n",
    "        third = all_q[2].view(4, 4, -1)\n",
    "        fourth = all_q[3].view(4, 4, 4, -1)\n",
    "\n",
    "        second = first * second\n",
    "        third = second * third\n",
    "        fourth = third * fourth\n",
    "\n",
    "        first = torch.flatten(first).view(4, 1, 1, 1)\n",
    "        second = torch.flatten(second).view(16, 1, 1, 1)\n",
    "        third = torch.flatten(third).view(64, 1, 1, 1)\n",
    "        fourth = torch.flatten(fourth).view(256, 1, 1, 1)\n",
    "\n",
    "        first_loss = (F.mse_loss(dec[1], i[1][0], reduction='none') * first).sum()\n",
    "        second_loss = (F.mse_loss(dec[2], i[1][1], reduction='none') * second).sum()\n",
    "        third_loss = (F.mse_loss(dec[3], i[1][2], reduction='none') * third).sum()\n",
    "        fourth_loss = (F.mse_loss(dec[4], i[1][3], reduction='none') * fourth).sum()\n",
    "        #no grad\n",
    "        raw_loss = (decode_loss + first_loss + second_loss + third_loss + fourth_loss).detach().item()\n",
    "        loss_data.append([k,decode_loss.item(),first_loss.item(),second_loss.item(),third_loss.item(),fourth_loss.item(),raw_loss])\n",
    "        eval_loss += raw_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decode Loss</th>\n",
       "      <th>First Loss</th>\n",
       "      <th>Second Loss</th>\n",
       "      <th>Third Loss</th>\n",
       "      <th>Fourth Loss</th>\n",
       "      <th>Total Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Starting Input</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.57</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.73</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.56</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.39</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.93</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.74</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.58</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.78</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.20</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.40</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.26</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Decode Loss  First Loss  Second Loss  Third Loss  Fourth Loss  \\\n",
       "Starting Input                                                                  \n",
       "0                      0.06        0.02         0.03         0.0          0.0   \n",
       "1                      0.08        0.01         0.02         0.0          0.0   \n",
       "2                      0.17        0.02         0.02         0.0          0.0   \n",
       "3                      0.41        0.02         0.01         0.0          0.0   \n",
       "4                      0.66        0.02         0.02         0.0          0.0   \n",
       "5                      1.03        0.02         0.02         0.0          0.0   \n",
       "6                      1.21        0.02         0.02         0.0          0.0   \n",
       "7                      0.80        0.03         0.02         0.0          0.0   \n",
       "8                      2.03        0.05         0.04         0.0          0.0   \n",
       "9                      2.57        0.06         0.03         0.0          0.0   \n",
       "10                     2.73        0.03         0.03         0.0          0.0   \n",
       "11                     1.56        0.06         0.04         0.0          0.0   \n",
       "12                     4.39        0.04         0.05         0.0          0.0   \n",
       "13                     4.93        0.04         0.02         0.0          0.0   \n",
       "14                     3.74        0.12         0.07         0.0          0.0   \n",
       "15                     2.58        0.06         0.06         0.0          0.0   \n",
       "16                     6.78        0.03         0.04         0.0          0.0   \n",
       "17                     8.20        0.03         0.04         0.0          0.0   \n",
       "18                     4.40        0.09         0.05         0.0          0.0   \n",
       "19                     4.26        0.10         0.05         0.0          0.0   \n",
       "\n",
       "                Total Loss  \n",
       "Starting Input              \n",
       "0                     0.11  \n",
       "1                     0.11  \n",
       "2                     0.21  \n",
       "3                     0.44  \n",
       "4                     0.70  \n",
       "5                     1.08  \n",
       "6                     1.25  \n",
       "7                     0.85  \n",
       "8                     2.12  \n",
       "9                     2.65  \n",
       "10                    2.79  \n",
       "11                    1.67  \n",
       "12                    4.47  \n",
       "13                    4.99  \n",
       "14                    3.93  \n",
       "15                    2.70  \n",
       "16                    6.85  \n",
       "17                    8.27  \n",
       "18                    4.54  \n",
       "19                    4.41  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df = pd.DataFrame(loss_data,columns=[\"Starting Input\",\"Decode Loss\",\"First Loss\",\"Second Loss\",\"Third Loss\",\"Fourth Loss\",\"Total Loss\"]).set_index(\"Starting Input\").round(2)\n",
    "loss_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([55.487335205078125, 41.805625915527344, 77.20865631103516, 57.6768798828125],\n",
       " [97.11146545410156,\n",
       "  143.53211975097656,\n",
       "  261.96331787109375,\n",
       "  324.1368713378906])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A check on if the training loop is valid\n",
    "#Checking loss of each state unweighted, with both absolute difference loss and mse loss\n",
    "min_losses = []\n",
    "max_losses = []\n",
    "for i in range(1,len(dec)):\n",
    "    decoded_states = dec[i]\n",
    "    true_state = train_data[0][1][i-1]\n",
    "    curr_min = float('inf')\n",
    "    curr_max = float('-inf')\n",
    "    for state in decoded_states:\n",
    "        loss = torch.abs(state-true_state).sum()\n",
    "        if loss < curr_min:\n",
    "            curr_min = loss.item()\n",
    "        if loss > curr_max:\n",
    "            curr_max = loss.item()\n",
    "    min_losses.append(curr_min)\n",
    "    max_losses.append(curr_max)\n",
    "min_losses,max_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikef\\AppData\\Local\\Temp/ipykernel_8388/2585299852.py:10: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(state,true_state)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.03765950724482536,\n",
       "  0.02320833131670952,\n",
       "  0.06321296095848083,\n",
       "  0.0374317392706871],\n",
       " [0.14017599821090698,\n",
       "  0.3327014148235321,\n",
       "  0.5786989331245422,\n",
       "  0.8257387280464172])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_losses = []\n",
    "max_losses = []\n",
    "for i in range(1,len(dec)):\n",
    "    decoded_states = dec[i]\n",
    "    true_state = train_data[0][1][i-1]\n",
    "    curr_min = float('inf')\n",
    "    curr_max = float('-inf')\n",
    "    for state in decoded_states:\n",
    "        loss = F.mse_loss(state,true_state)\n",
    "        if loss < curr_min:\n",
    "            curr_min = loss.item()\n",
    "        if loss > curr_max:\n",
    "            curr_max = loss.item()\n",
    "    min_losses.append(curr_min)\n",
    "    max_losses.append(curr_max)\n",
    "min_losses,max_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
