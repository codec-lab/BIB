{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape: torch.Size([1, 3, 100, 100])\n",
      "post embedding shape: torch.Size([1, 10])\n",
      "q values (backup values shape): torch.Size([1, 4])\n",
      "first transition torch.Size([4, 10])\n",
      "last transition torch.Size([1048576, 10])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1 #for explanatory purposes\n",
    "tensor = torch.rand(batch_size,3,100,100)\n",
    "###Assumed input shape is (batch_size, in_channels, height, width)\n",
    "\n",
    "print('initial shape:', tensor.shape)\n",
    "in_channels = tensor.shape[1]\n",
    "\n",
    "###################State Embedding, X -> Z(1 x embedding_dim) ######################\n",
    "embedding_dim = 10\n",
    "\n",
    "#Create CNN encoder and use it first, then flatten the CNN output to complete final embedding transformation\n",
    "class CNN_Encoder(nn.Module): #did everything except some w2 division thing, but shapes will be same\n",
    "    def __init__(self, in_channels):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 16, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)\n",
    "        self.relu = nn.ReLU(True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x\n",
    "cnn_encoder = CNN_Encoder(in_channels)\n",
    "flat_conv_dim = int(np.prod(cnn_encoder(torch.zeros(tensor.shape)).shape[1:]))\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, in_channels, embedding_dim):\n",
    "        super(Embed, self).__init__()\n",
    "        self.cnn_encoder = CNN_Encoder(in_channels)\n",
    "        self.linear = nn.Linear(flat_conv_dim, embedding_dim)\n",
    "        self.relu = nn.ReLU(True)\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_encoder(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "embed = Embed(in_channels, embedding_dim)\n",
    "tensor = embed(tensor)\n",
    "\n",
    "print('post embedding shape:', tensor.shape)\n",
    "####################State Embed Over -- Now Branch Into Actions ############################\n",
    "num_actions = 4\n",
    "###Create Reward Function, Transition Function, Value Function\n",
    "# Transition function branches state for each action, multiplying batch size by num_actions.\n",
    "# (Each branch is saved then backed up later) \n",
    "####Get Reward given init state for each action########\n",
    "class MLPRewardFn(nn.Module):\n",
    "    def __init__(self, embed_dim, num_actions):\n",
    "        super(MLPRewardFn, self).__init__()\n",
    "        self.embedding_dim = embed_dim\n",
    "        self.num_actions = num_actions\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.embedding_dim)\n",
    "        return self.mlp(x).view(-1, self.num_actions)\n",
    "reward_fun = MLPRewardFn(embedding_dim, num_actions)\n",
    "\n",
    "transition_fun = Parameter(torch.zeros(embedding_dim, embedding_dim, num_actions))\n",
    "transition_fun = nn.init.xavier_normal_(transition_fun)\n",
    "\n",
    "def tree_transition(tensor):\n",
    "    temp = nn.Tanh()(torch.einsum(\"ij,jab->iba\", tensor, transition_fun))\n",
    "    temp = temp.contiguous() ###HMMMMMMM!!!!!!!!\n",
    "    next_state = temp\n",
    "    return next_state\n",
    "value_fn = nn.Linear(embedding_dim, 1) #literally its just this except with the w_scale\n",
    "\n",
    "\n",
    "tree_depth = 10 #Big Hyperparameter\n",
    "###Log each application of r,t,v to save intermediate branches to back up later\n",
    "reward_list = []\n",
    "transition_list = [tensor]\n",
    "value_list = []\n",
    "value_list.append(value_fn(tensor))\n",
    "for i in range(tree_depth):\n",
    "    reward = reward_fun(tensor)\n",
    "    reward_list.append(reward.view(-1,1))\n",
    "    tensor = tree_transition(tensor)\n",
    "    tensor = tensor.view(-1, embedding_dim)\n",
    "    transition_list.append(tensor)\n",
    "    #i think it assumes return intermediate values is true\n",
    "    value_list.append(value_fn(tensor))\n",
    "\n",
    "\n",
    "tree_result = {\n",
    "    \"embeddings\": transition_list,\n",
    "    \"values\": value_list,\n",
    "    \"rewards\": reward_list\n",
    "}\n",
    "\n",
    "td_lambda = 0.3\n",
    "# ################Backup############################ \n",
    "#q_values = tree_backup(tree_result, batch_size) | function saved var and input params from original code\n",
    "###Planning to comment more later on final part\n",
    "backup_values = tree_result[\"values\"][-1] #last value in the list\n",
    "# ##Loop through range (1, tree_depth + 1)  \n",
    "for i in range(1, tree_depth + 1):\n",
    "    one_step_backup = tree_result['rewards'][-i] + backup_values # * gamma\n",
    "    if i < tree_depth:\n",
    "        one_step_backup = one_step_backup.view(batch_size, -1, num_actions)\n",
    "        max_backup = (one_step_backup * F.softmax(one_step_backup, dim = 2)).sum(dim = 2)\n",
    "        backup_values = ((1-td_lambda) * tree_result['values'][-i-1] + #return intermediate values seems to be necessary despite it being presented as an option\n",
    "                         (td_lambda) * max_backup.view(-1, 1))\n",
    "    else:\n",
    "        backup_values = one_step_backup\n",
    "backup_values = backup_values.view(batch_size, num_actions)\n",
    "#softmax backup values, get \"max action\"\n",
    "#look into V values\n",
    "print('q values (backup values shape):', backup_values.shape)\n",
    "print('first transition', tree_result['embeddings'][1].shape)\n",
    "print('last transition', tree_result['embeddings'][-1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TreeQN2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
