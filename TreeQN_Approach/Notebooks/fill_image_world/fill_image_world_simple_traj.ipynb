{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "from treeQN.treeqn_traj_simple import TreeQN\n",
    "import random\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16])\n",
      "torch.Size([16, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the tensor\n",
    "tensor = torch.ones(16,1, 4, 4)\n",
    "\n",
    "# Flatten dimensions 1 and 2\n",
    "\n",
    "#reverse process\n",
    "\n",
    "def flattener(tensor):\n",
    "    return torch.flatten(tensor, start_dim=2, end_dim=3).squeeze(1)\n",
    "def unflattener(tensor):\n",
    "    return tensor.view(-1, 1, 4, 4)\n",
    "\n",
    "flat = flattener(tensor)\n",
    "print(flat.shape)  # Output: torch.Size([4, 16])\n",
    "unflat = unflattener(flat)\n",
    "print(unflat.shape)  # Output: torch.Size([4, 1, 4, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_world(tensor,max_val): \n",
    "    assert tensor.shape[0] == tensor.shape[1]\n",
    "    val = tensor.max() + 1\n",
    "    state = torch.zeros_like(tensor).unsqueeze(0).unsqueeze(0) #to match treeqn input size\n",
    "    new_state = torch.ones_like(state)\n",
    "    middle = int(tensor.shape[0] / 2)\n",
    "    # Create transitions by modifying slices of new_state\n",
    "    new_state[:,:,:middle, :middle] += val\n",
    "    transition_one = new_state.clone()\n",
    "    new_state[:,:,middle:, :middle] += val\n",
    "    transition_two = new_state.clone()\n",
    "    new_state[:,:,:middle, middle:] += val\n",
    "    transition_three = new_state.clone()\n",
    "    new_state[:,:,middle:, middle:] += val #transition 4\n",
    "    return [transition_one/max_val, transition_two/max_val, transition_three/max_val, new_state/max_val]\n",
    "def image_world_samples(size_tensor,samples,max_val=1,x_input=-1):\n",
    "    return_data = []\n",
    "    for i in range(samples):\n",
    "        loc_tensor = torch.zeros_like(size_tensor)+0.1\n",
    "        x = int(random.random()*size_tensor.shape[0])\n",
    "        if x_input != -1:\n",
    "            x = x_input\n",
    "        loc_tensor[x] += x\n",
    "        result = image_world(loc_tensor,max_val)\n",
    "        loc_tensor = loc_tensor.unsqueeze(0).unsqueeze(0)/max_val\n",
    "        return_data.append([loc_tensor,result])\n",
    "    return return_data\n",
    "\n",
    "size_tensor = torch.zeros(20,20)\n",
    "train_data = image_world_samples(size_tensor,1000,size_tensor.shape[0],-1)\n",
    "\n",
    "test_data = [] #simply test once on all 20 possible initial states.\n",
    "for i in range(size_tensor.shape[0]):\n",
    "    test_data.append(image_world_samples(size_tensor,1,size_tensor.shape[0],i)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/Desktop/TreeQN/BIB/TreeQN_Approach/treeQN/treeqn_traj_simple.py:33: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  b_init(module.bias, b_scale)\n"
     ]
    }
   ],
   "source": [
    "input_shape = torch.zeros(1, 1,20, 20).shape# minimum size #train_data[0][0].shape\n",
    "num_actions = 4\n",
    "tree_depth = 4\n",
    "embedding_dim = 400\n",
    "td_lambda = 0.8\n",
    "gamma = 1    #0.99\n",
    "model = TreeQN(input_shape=input_shape, num_actions=num_actions, tree_depth=tree_depth, embedding_dim=embedding_dim, td_lambda=td_lambda,gamma=gamma)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "#optimizer = RMSprop(model.parameters(), lr=1e-4,alpha =0.99, eps = 1e-5) | loss from treeqn paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_316002/395139831.py:34: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([4, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  first_loss = (F.mse_loss(decoded_values[1], t[1][0], reduction='none') * first).sum()\n",
      "/tmp/ipykernel_316002/395139831.py:35: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([16, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  second_loss = (F.mse_loss(decoded_values[2], t[1][1], reduction='none') * second).sum()\n",
      "/tmp/ipykernel_316002/395139831.py:36: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([64, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  third_loss = (F.mse_loss(decoded_values[3], t[1][2], reduction='none') * third).sum()\n",
      "/tmp/ipykernel_316002/395139831.py:37: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([256, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  fourth_loss = (F.mse_loss(decoded_values[4], t[1][3], reduction='none') * fourth).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 424.0273838863373, Average Raw Loss: 424.0273838863373\n",
      "Epoch 2, Average Loss: 423.89982308959964, Average Raw Loss: 423.89982308959964\n",
      "Epoch 3, Average Loss: 423.71750062179564, Average Raw Loss: 423.71750062179564\n",
      "Epoch 4, Average Loss: 422.95883463954925, Average Raw Loss: 422.95883463954925\n",
      "Epoch 5, Average Loss: 421.4079816145897, Average Raw Loss: 421.4079816145897\n",
      "Epoch 6, Average Loss: 419.27885300350187, Average Raw Loss: 419.27885300350187\n",
      "Epoch 7, Average Loss: 416.78679663562775, Average Raw Loss: 416.78679663562775\n",
      "Epoch 8, Average Loss: 413.98007303905484, Average Raw Loss: 413.98007303905484\n",
      "Epoch 9, Average Loss: 410.93938877391815, Average Raw Loss: 410.93938877391815\n",
      "Epoch 10, Average Loss: 407.7604294695854, Average Raw Loss: 407.7604294695854\n",
      "Epoch 11, Average Loss: 404.44913383197786, Average Raw Loss: 404.44913383197786\n",
      "Epoch 12, Average Loss: 401.0200046405792, Average Raw Loss: 401.0200046405792\n",
      "Epoch 13, Average Loss: 397.51603592729566, Average Raw Loss: 397.51603592729566\n",
      "Epoch 14, Average Loss: 393.89596107912064, Average Raw Loss: 393.89596107912064\n",
      "Epoch 15, Average Loss: 390.1892473564148, Average Raw Loss: 390.1892473564148\n",
      "Epoch 16, Average Loss: 386.38881243515016, Average Raw Loss: 386.38881243515016\n",
      "Epoch 17, Average Loss: 382.4855620932579, Average Raw Loss: 382.4855620932579\n",
      "Epoch 18, Average Loss: 378.4923129916191, Average Raw Loss: 378.4923129916191\n",
      "Epoch 19, Average Loss: 374.4322191739082, Average Raw Loss: 374.4322191739082\n",
      "Epoch 20, Average Loss: 370.32544844293596, Average Raw Loss: 370.32544844293596\n",
      "Epoch 21, Average Loss: 366.14034133481977, Average Raw Loss: 366.14034133481977\n",
      "Epoch 22, Average Loss: 361.8764402313232, Average Raw Loss: 361.8764402313232\n",
      "Epoch 23, Average Loss: 357.5922668204308, Average Raw Loss: 357.5922668204308\n",
      "Epoch 24, Average Loss: 353.2669268770218, Average Raw Loss: 353.2669268770218\n",
      "Epoch 25, Average Loss: 348.9354722084999, Average Raw Loss: 348.9354722084999\n",
      "Epoch 26, Average Loss: 344.5794727959633, Average Raw Loss: 344.5794727959633\n",
      "Epoch 27, Average Loss: 340.220441382885, Average Raw Loss: 340.220441382885\n",
      "Epoch 28, Average Loss: 335.8534011526108, Average Raw Loss: 335.8534011526108\n",
      "Epoch 29, Average Loss: 331.4810073695183, Average Raw Loss: 331.4810073695183\n",
      "Epoch 30, Average Loss: 327.1297840218544, Average Raw Loss: 327.1297840218544\n",
      "Epoch 31, Average Loss: 322.75789864778517, Average Raw Loss: 322.75789864778517\n",
      "Epoch 32, Average Loss: 318.41249758577345, Average Raw Loss: 318.41249758577345\n",
      "Epoch 33, Average Loss: 314.0676400022507, Average Raw Loss: 314.0676400022507\n",
      "Epoch 34, Average Loss: 309.7285618143082, Average Raw Loss: 309.7285618143082\n",
      "Epoch 35, Average Loss: 305.4255599746704, Average Raw Loss: 305.4255599746704\n",
      "Epoch 36, Average Loss: 301.1370977034569, Average Raw Loss: 301.1370977034569\n",
      "Epoch 37, Average Loss: 296.868814722538, Average Raw Loss: 296.868814722538\n",
      "Epoch 38, Average Loss: 292.6420340075493, Average Raw Loss: 292.6420340075493\n",
      "Epoch 39, Average Loss: 288.4778292899132, Average Raw Loss: 288.4778292899132\n",
      "Epoch 40, Average Loss: 284.33716232872007, Average Raw Loss: 284.33716232872007\n",
      "Epoch 41, Average Loss: 280.23276953935624, Average Raw Loss: 280.23276953935624\n",
      "Epoch 42, Average Loss: 276.19049416828153, Average Raw Loss: 276.19049416828153\n",
      "Epoch 43, Average Loss: 272.2009815397263, Average Raw Loss: 272.2009815397263\n",
      "Epoch 44, Average Loss: 268.2831738948822, Average Raw Loss: 268.2831738948822\n",
      "Epoch 45, Average Loss: 264.42834415626527, Average Raw Loss: 264.42834415626527\n",
      "Epoch 46, Average Loss: 260.63617013692857, Average Raw Loss: 260.63617013692857\n",
      "Epoch 47, Average Loss: 256.9217847428322, Average Raw Loss: 256.9217847428322\n",
      "Epoch 48, Average Loss: 253.27510717201233, Average Raw Loss: 253.27510717201233\n",
      "Epoch 49, Average Loss: 249.704583920002, Average Raw Loss: 249.704583920002\n",
      "Epoch 50, Average Loss: 246.21863290452958, Average Raw Loss: 246.21863290452958\n",
      "Epoch 51, Average Loss: 242.81309096765517, Average Raw Loss: 242.81309096765517\n",
      "Epoch 52, Average Loss: 239.49539956140518, Average Raw Loss: 239.49539956140518\n",
      "Epoch 53, Average Loss: 236.27364285755158, Average Raw Loss: 236.27364285755158\n",
      "Epoch 54, Average Loss: 233.13625199890137, Average Raw Loss: 233.13625199890137\n",
      "Epoch 55, Average Loss: 230.06856448173522, Average Raw Loss: 230.06856448173522\n",
      "Epoch 56, Average Loss: 227.0834261379242, Average Raw Loss: 227.0834261379242\n",
      "Epoch 57, Average Loss: 224.1869572868347, Average Raw Loss: 224.1869572868347\n",
      "Epoch 58, Average Loss: 221.36233547496795, Average Raw Loss: 221.36233547496795\n",
      "Epoch 59, Average Loss: 218.61180640268327, Average Raw Loss: 218.61180640268327\n",
      "Epoch 60, Average Loss: 215.92854535675048, Average Raw Loss: 215.92854535675048\n",
      "Epoch 61, Average Loss: 213.31364323091506, Average Raw Loss: 213.31364323091506\n",
      "Epoch 62, Average Loss: 210.77009144735337, Average Raw Loss: 210.77009144735337\n",
      "Epoch 63, Average Loss: 208.28996486377716, Average Raw Loss: 208.28996486377716\n",
      "Epoch 64, Average Loss: 205.8747127518654, Average Raw Loss: 205.8747127518654\n",
      "Epoch 65, Average Loss: 203.51901284885406, Average Raw Loss: 203.51901284885406\n",
      "Epoch 66, Average Loss: 201.2049832620621, Average Raw Loss: 201.2049832620621\n",
      "Epoch 67, Average Loss: 198.95528561210634, Average Raw Loss: 198.95528561210634\n",
      "Epoch 68, Average Loss: 196.761594871521, Average Raw Loss: 196.761594871521\n",
      "Epoch 69, Average Loss: 194.61423550748825, Average Raw Loss: 194.61423550748825\n",
      "Epoch 70, Average Loss: 192.51735772562026, Average Raw Loss: 192.51735772562026\n",
      "Epoch 71, Average Loss: 190.47231871414184, Average Raw Loss: 190.47231871414184\n",
      "Epoch 72, Average Loss: 188.47439493012428, Average Raw Loss: 188.47439493012428\n",
      "Epoch 73, Average Loss: 186.52286707425117, Average Raw Loss: 186.52286707425117\n",
      "Epoch 74, Average Loss: 184.62662172818185, Average Raw Loss: 184.62662172818185\n",
      "Epoch 75, Average Loss: 182.77664414525032, Average Raw Loss: 182.77664414525032\n",
      "Epoch 76, Average Loss: 180.97283324861527, Average Raw Loss: 180.97283324861527\n",
      "Epoch 77, Average Loss: 179.2180794110298, Average Raw Loss: 179.2180794110298\n",
      "Epoch 78, Average Loss: 177.51391814184188, Average Raw Loss: 177.51391814184188\n",
      "Epoch 79, Average Loss: 175.8496379709244, Average Raw Loss: 175.8496379709244\n",
      "Epoch 80, Average Loss: 174.23208307933808, Average Raw Loss: 174.23208307933808\n",
      "Epoch 81, Average Loss: 172.6575074594021, Average Raw Loss: 172.6575074594021\n",
      "Epoch 82, Average Loss: 171.13467572689058, Average Raw Loss: 171.13467572689058\n",
      "Epoch 83, Average Loss: 169.65423370838164, Average Raw Loss: 169.65423370838164\n",
      "Epoch 84, Average Loss: 168.20996250247956, Average Raw Loss: 168.20996250247956\n",
      "Epoch 85, Average Loss: 166.8154992735386, Average Raw Loss: 166.8154992735386\n",
      "Epoch 86, Average Loss: 165.4578083000183, Average Raw Loss: 165.4578083000183\n",
      "Epoch 87, Average Loss: 164.14242962670326, Average Raw Loss: 164.14242962670326\n",
      "Epoch 88, Average Loss: 162.86840492081643, Average Raw Loss: 162.86840492081643\n",
      "Epoch 89, Average Loss: 161.62995582175256, Average Raw Loss: 161.62995582175256\n",
      "Epoch 90, Average Loss: 160.42484692788125, Average Raw Loss: 160.42484692788125\n",
      "Epoch 91, Average Loss: 159.25577133631705, Average Raw Loss: 159.25577133631705\n",
      "Epoch 92, Average Loss: 158.12567361474038, Average Raw Loss: 158.12567361474038\n",
      "Epoch 93, Average Loss: 157.0288520383835, Average Raw Loss: 157.0288520383835\n",
      "Epoch 94, Average Loss: 155.9678245215416, Average Raw Loss: 155.9678245215416\n",
      "Epoch 95, Average Loss: 154.937772544384, Average Raw Loss: 154.937772544384\n",
      "Epoch 96, Average Loss: 153.93908382344245, Average Raw Loss: 153.93908382344245\n",
      "Epoch 97, Average Loss: 152.9758655514717, Average Raw Loss: 152.9758655514717\n",
      "Epoch 98, Average Loss: 152.04499573731422, Average Raw Loss: 152.04499573731422\n",
      "Epoch 99, Average Loss: 151.14428207540513, Average Raw Loss: 151.14428207540513\n",
      "Epoch 100, Average Loss: 150.2741207613945, Average Raw Loss: 150.2741207613945\n",
      "Epoch 101, Average Loss: 149.43214231729507, Average Raw Loss: 149.43214231729507\n",
      "Epoch 102, Average Loss: 148.61995687007905, Average Raw Loss: 148.61995687007905\n",
      "Epoch 103, Average Loss: 147.8411913871765, Average Raw Loss: 147.8411913871765\n",
      "Epoch 104, Average Loss: 147.08898519563675, Average Raw Loss: 147.08898519563675\n",
      "Epoch 105, Average Loss: 146.36187062191962, Average Raw Loss: 146.36187062191962\n",
      "Epoch 106, Average Loss: 145.66168048214914, Average Raw Loss: 145.66168048214914\n",
      "Epoch 107, Average Loss: 144.9889150943756, Average Raw Loss: 144.9889150943756\n",
      "Epoch 108, Average Loss: 144.3399860920906, Average Raw Loss: 144.3399860920906\n",
      "Epoch 109, Average Loss: 143.71803412890435, Average Raw Loss: 143.71803412890435\n",
      "Epoch 110, Average Loss: 143.12037491464613, Average Raw Loss: 143.12037491464613\n",
      "Epoch 111, Average Loss: 142.54350206899642, Average Raw Loss: 142.54350206899642\n",
      "Epoch 112, Average Loss: 141.99000828409194, Average Raw Loss: 141.99000828409194\n",
      "Epoch 113, Average Loss: 141.45692411112785, Average Raw Loss: 141.45692411112785\n",
      "Epoch 114, Average Loss: 140.94700400710107, Average Raw Loss: 140.94700400710107\n",
      "Epoch 115, Average Loss: 140.45924261808395, Average Raw Loss: 140.45924261808395\n",
      "Epoch 116, Average Loss: 139.98971815872193, Average Raw Loss: 139.98971815872193\n",
      "Epoch 117, Average Loss: 139.53897508096696, Average Raw Loss: 139.53897508096696\n",
      "Epoch 118, Average Loss: 139.10850065684318, Average Raw Loss: 139.10850065684318\n"
     ]
    }
   ],
   "source": [
    "#Main training loop\n",
    "#Looking at difference between detaching at each transition or not in treeqn file. This is with detach (so far seems to make no diff)\n",
    "raw_losses = []\n",
    "for epoch in range(3000):  # epochs\n",
    "    avg_loss = 0\n",
    "    temp_loss = 0\n",
    "    temp_raw_loss = 0\n",
    "    sample_count = 0\n",
    "\n",
    "    avg_raw_loss = 0\n",
    "\n",
    "    for t in random.sample(train_data, len(train_data)): #sample through all data in random order each epoch\n",
    "        #Get reconstruction loss to help ground abstract state\n",
    "        decoded_values, all_policies = model(t[0])\n",
    "        decode_loss = F.mse_loss(decoded_values[0], t[0], reduction='sum')\n",
    "\n",
    "        #Get transition probabilities for each state\n",
    "        first_policy = all_policies[0]\n",
    "        second_policy = all_policies[1].view(4, -1)\n",
    "        third_policy = all_policies[2].view(4, 4, -1)\n",
    "        fourth_policy = all_policies[3].view(4, 4, 4, -1)\n",
    "\n",
    "        #These should all add to 1 (in testing there seems to be some small rounding error)\n",
    "        second_layer_probs = first_policy * second_policy   \n",
    "        third_layer_probs = second_layer_probs * third_policy\n",
    "        fourth_layer_probs = third_layer_probs * fourth_policy\n",
    "        \n",
    "        #Flatten transition probabilities to then weigh with loss of each predicted state at each layer\n",
    "        first = torch.flatten(first_policy).view(4, 1, 1, 1)\n",
    "        second = torch.flatten(second_layer_probs).view(16, 1, 1, 1)\n",
    "        third = torch.flatten(third_layer_probs).view(64, 1, 1, 1)\n",
    "        fourth = torch.flatten(fourth_layer_probs).view(256, 1, 1, 1) \n",
    "        \n",
    "        first_loss = (F.mse_loss(decoded_values[1], t[1][0], reduction='none') * first).sum() \n",
    "        second_loss = (F.mse_loss(decoded_values[2], t[1][1], reduction='none') * second).sum() \n",
    "        third_loss = (F.mse_loss(decoded_values[3], t[1][2], reduction='none') * third).sum() \n",
    "        fourth_loss = (F.mse_loss(decoded_values[4], t[1][3], reduction='none') * fourth).sum() \n",
    "\n",
    "\n",
    "        #For experimenting with different weights on different layers\n",
    "        raw_loss = (decode_loss + first_loss + second_loss + third_loss + fourth_loss).detach().item()\n",
    "        raw_losses.append(raw_loss)\n",
    "        l2w , l3w ,l4w = 1,1,1\n",
    "        total_loss = decode_loss + first_loss + second_loss*l2w + third_loss*l3w + fourth_loss*l4w\n",
    "\n",
    "        temp_loss += total_loss\n",
    "        temp_raw_loss += raw_loss\n",
    "        sample_count += 1\n",
    "\n",
    "        if sample_count % 1 == 0:\n",
    "            optimizer.zero_grad()\n",
    "            temp_loss.backward()\n",
    "\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            avg_loss += temp_loss.item()\n",
    "            avg_raw_loss += temp_raw_loss\n",
    "            temp_loss = 0\n",
    "            temp_raw_loss = 0\n",
    "\n",
    "    # To handle the case where the number of samples is not a multiple of 10\n",
    "    if sample_count % 1 != 0:\n",
    "        optimizer.zero_grad()\n",
    "        temp_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        avg_loss += temp_loss.item()\n",
    "        avg_raw_loss += temp_raw_loss\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss / len(train_data)}, Average Raw Loss: {avg_raw_loss / len(train_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Weight Sums tensor([-4.0040, -5.2650, -5.1780,  5.3700])\n"
     ]
    }
   ],
   "source": [
    "#View Action Weights (This hasn't been informative yet)\n",
    "dec, all_policies = model(train_data[0][0]) \n",
    "# dot = make_dot((dec[0],dec[1],dec[2],dec[3],dec[4],all_policies[0],all_policies[1],all_policies[2],all_policies[3]),params=dict(model.named_parameters()))\n",
    "# dot.render('model', format='png')\n",
    "print(f\"Action Weight Sums { torch.round(model.transition_fun.data,decimals=3).sum(dim=0).sum(dim=0)}\")  #might be summing the wrong way, or just not interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Actions: 2 2 3 0\n"
     ]
    }
   ],
   "source": [
    "best_first_action = all_policies[0].argmax()\n",
    "best_second_action = all_policies[1].view(4,-1)[best_first_action].argmax() \n",
    "best_third_action = all_policies[2].view(4,4,-1)[best_first_action][best_second_action].argmax()\n",
    "best_fourth_action = all_policies[3].view(4,4,4,-1)[best_first_action][best_second_action][best_third_action].argmax() \n",
    "# print(torch.round(all_q[0],decimals=3).detach(), f\"Argmax {all_q[0].argmax().item()}\")\n",
    "# print(torch.round(all_q[1],decimals=3).view(4,-1).detach(),f\"Argmax {all_q[1].view(4,-1)[1].argmax().item()}\")\n",
    "# print(torch.round(all_q[2],decimals=3).view(4,4,-1)[0].detach(),f\"Argmax {all_q[2].view(4,4,-1)[1][0].argmax().item()}\")\n",
    "# print(torch.round(all_q[3],decimals=3).view(4,4,4,-1)[0][0].detach(),f\"Argmax {all_q[3].view(4,4,4,-1)[1][1][0].argmax().item()}\")\n",
    "print(f\"Best Actions: {best_first_action.item()} {best_second_action.item()} {best_third_action.item()} {best_fourth_action.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177661/1188555054.py:27: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([4, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  first_loss = (F.mse_loss(decoded_values[1], t[1][0], reduction='none') * first).sum()\n",
      "/tmp/ipykernel_177661/1188555054.py:28: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([16, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  second_loss = (F.mse_loss(decoded_values[2], t[1][1], reduction='none') * second).sum()\n",
      "/tmp/ipykernel_177661/1188555054.py:29: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([64, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  third_loss = (F.mse_loss(decoded_values[3], t[1][2], reduction='none') * third).sum()\n",
      "/tmp/ipykernel_177661/1188555054.py:30: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([256, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  fourth_loss = (F.mse_loss(decoded_values[4], t[1][3], reduction='none') * fourth).sum()\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "loss_data = []\n",
    "with torch.no_grad():\n",
    "    eval_loss = 0\n",
    "    for i,k in zip(test_data,range(len(test_data))):\n",
    "        #Get reconstruction loss to help ground abstract state\n",
    "        decoded_values, all_policies = model(t[0])\n",
    "        decode_loss = F.mse_loss(decoded_values[0], t[0], reduction='sum')\n",
    "\n",
    "        #Get transition probabilities for each state\n",
    "        first_policy = all_policies[0]\n",
    "        second_policy = all_policies[1].view(4, -1)\n",
    "        third_policy = all_policies[2].view(4, 4, -1)\n",
    "        fourth_policy = all_policies[3].view(4, 4, 4, -1)\n",
    "\n",
    "        #These should all add to 1 (in testing there seems to be some small rounding error)\n",
    "        second_layer_probs = first_policy * second_policy   \n",
    "        third_layer_probs = second_layer_probs * third_policy\n",
    "        fourth_layer_probs = third_layer_probs * fourth_policy\n",
    "        \n",
    "        #Flatten transition probabilities to then weigh with loss of each predicted state at each layer\n",
    "        first = torch.flatten(first_policy).view(4, 1, 1, 1)\n",
    "        second = torch.flatten(second_layer_probs).view(16, 1, 1, 1)\n",
    "        third = torch.flatten(third_layer_probs).view(64, 1, 1, 1)\n",
    "        fourth = torch.flatten(fourth_layer_probs).view(256, 1, 1, 1) \n",
    "        \n",
    "        first_loss = (F.mse_loss(decoded_values[1], t[1][0], reduction='none') * first).sum() \n",
    "        second_loss = (F.mse_loss(decoded_values[2], t[1][1], reduction='none') * second).sum() \n",
    "        third_loss = (F.mse_loss(decoded_values[3], t[1][2], reduction='none') * third).sum() \n",
    "        fourth_loss = (F.mse_loss(decoded_values[4], t[1][3], reduction='none') * fourth).sum() \n",
    "\n",
    "        #For experimenting with different weights on different layers\n",
    "        raw_loss = (decode_loss + first_loss + second_loss + third_loss + fourth_loss).detach().item()\n",
    "\n",
    "        loss_data.append([k,decode_loss.item(),first_loss.item(),second_loss.item(),third_loss.item(),fourth_loss.item(),raw_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decode Loss</th>\n",
       "      <th>First Loss</th>\n",
       "      <th>Second Loss</th>\n",
       "      <th>Third Loss</th>\n",
       "      <th>Fourth Loss</th>\n",
       "      <th>Total Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Starting Input</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Decode Loss  First Loss  Second Loss  Third Loss  Fourth Loss  \\\n",
       "Starting Input                                                                  \n",
       "0                       0.1        0.04         0.01         0.0          0.0   \n",
       "1                       0.1        0.04         0.01         0.0          0.0   \n",
       "2                       0.1        0.04         0.01         0.0          0.0   \n",
       "3                       0.1        0.04         0.01         0.0          0.0   \n",
       "4                       0.1        0.04         0.01         0.0          0.0   \n",
       "5                       0.1        0.04         0.01         0.0          0.0   \n",
       "6                       0.1        0.04         0.01         0.0          0.0   \n",
       "7                       0.1        0.04         0.01         0.0          0.0   \n",
       "8                       0.1        0.04         0.01         0.0          0.0   \n",
       "9                       0.1        0.04         0.01         0.0          0.0   \n",
       "10                      0.1        0.04         0.01         0.0          0.0   \n",
       "11                      0.1        0.04         0.01         0.0          0.0   \n",
       "12                      0.1        0.04         0.01         0.0          0.0   \n",
       "13                      0.1        0.04         0.01         0.0          0.0   \n",
       "14                      0.1        0.04         0.01         0.0          0.0   \n",
       "15                      0.1        0.04         0.01         0.0          0.0   \n",
       "16                      0.1        0.04         0.01         0.0          0.0   \n",
       "17                      0.1        0.04         0.01         0.0          0.0   \n",
       "18                      0.1        0.04         0.01         0.0          0.0   \n",
       "19                      0.1        0.04         0.01         0.0          0.0   \n",
       "\n",
       "                Total Loss  \n",
       "Starting Input              \n",
       "0                     0.15  \n",
       "1                     0.15  \n",
       "2                     0.15  \n",
       "3                     0.15  \n",
       "4                     0.15  \n",
       "5                     0.15  \n",
       "6                     0.15  \n",
       "7                     0.15  \n",
       "8                     0.15  \n",
       "9                     0.15  \n",
       "10                    0.15  \n",
       "11                    0.15  \n",
       "12                    0.15  \n",
       "13                    0.15  \n",
       "14                    0.15  \n",
       "15                    0.15  \n",
       "16                    0.15  \n",
       "17                    0.15  \n",
       "18                    0.15  \n",
       "19                    0.15  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df = pd.DataFrame(loss_data,columns=[\"Starting Input\",\"Decode Loss\",\"First Loss\",\"Second Loss\",\"Third Loss\",\"Fourth Loss\",\"Total Loss\"]).set_index(\"Starting Input\").round(2)\n",
    "loss_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.005280017852783, 1.376326322555542, 91.25557708740234, 161.20518493652344],\n",
       " [154.4391326904297, 222.06932067871094, 311.039306640625, 375.0008850097656])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A check on if the training loop is valid\n",
    "#Checking loss of each state unweighted, with both absolute difference loss and mse loss\n",
    "#The earlier transitoins might be easier since it's easier to learn that a lot the environment is the same\n",
    "min_losses = []\n",
    "max_losses = []\n",
    "for i in range(1,len(dec)):\n",
    "    decoded_states = dec[i]\n",
    "    true_state = train_data[0][1][i-1]\n",
    "    curr_min = float('inf')\n",
    "    curr_max = float('-inf')\n",
    "    for state in decoded_states:\n",
    "        loss = torch.abs(state-true_state).sum()\n",
    "        if loss < curr_min:\n",
    "            curr_min = loss.item()\n",
    "        if loss > curr_max:\n",
    "            curr_max = loss.item()\n",
    "    min_losses.append(curr_min)\n",
    "    max_losses.append(curr_max)\n",
    "min_losses,max_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_177661/3588761361.py:9: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(state,true_state)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([7.934335735626519e-05,\n",
       "  2.415754170215223e-05,\n",
       "  0.11819437146186829,\n",
       "  0.2685106098651886],\n",
       " [0.3638641834259033,\n",
       "  0.5465582013130188,\n",
       "  0.7721645832061768,\n",
       "  0.9390590786933899])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_losses = []\n",
    "max_losses = []\n",
    "for i in range(1,len(dec)):\n",
    "    decoded_states = dec[i]\n",
    "    true_state = train_data[0][1][i-1]\n",
    "    curr_min = float('inf')\n",
    "    curr_max = float('-inf')\n",
    "    for state in decoded_states:\n",
    "        loss = F.mse_loss(state,true_state)\n",
    "        if loss < curr_min:\n",
    "            curr_min = loss.item()\n",
    "        if loss > curr_max:\n",
    "            curr_max = loss.item()\n",
    "    min_losses.append(curr_min)\n",
    "    max_losses.append(curr_max)\n",
    "min_losses,max_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
