{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "from treeQN.treeqn_traj import TreeQN\n",
    "import random\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[3., 3., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [3., 3., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [3., 3., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "tensor([[[[3., 3., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [3., 3., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [3., 3., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 3., 3., 3.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 3., 3., 3.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 3., 3., 3.]]]])\n",
      "tensor([[[[3., 3., 3., 1., 1., 1., 1., 1., 1., 3., 3., 3.],\n",
      "          [3., 3., 3., 1., 1., 1., 1., 1., 1., 3., 3., 3.],\n",
      "          [3., 3., 3., 1., 1., 1., 1., 1., 1., 3., 3., 3.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 3., 3., 3.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 3., 3., 3.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 3., 3., 3.]]]])\n",
      "tensor([[[[3., 3., 3., 1., 1., 1., 1., 1., 1., 3., 3., 3.],\n",
      "          [3., 3., 3., 1., 1., 1., 1., 1., 1., 3., 3., 3.],\n",
      "          [3., 3., 3., 1., 1., 1., 1., 1., 1., 3., 3., 3.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "          [3., 3., 3., 1., 1., 1., 1., 1., 1., 3., 3., 3.],\n",
      "          [3., 3., 3., 1., 1., 1., 1., 1., 1., 3., 3., 3.],\n",
      "          [3., 3., 3., 1., 1., 1., 1., 1., 1., 3., 3., 3.]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(1,1,12,12)\n",
    "a[:,:,:3,:3] *= 3\n",
    "print(a)\n",
    "a[:,:,-3:,-3:] *= 3\n",
    "print(a)\n",
    "a[:,:,:3,-3:] *= 3\n",
    "print(a)\n",
    "a[:,:,-3:,:3] *= 3\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_world(tensor,max_val): \n",
    "    assert tensor.shape[0] == tensor.shape[1]\n",
    "    val = tensor.max() + 1\n",
    "    state = torch.zeros_like(tensor).unsqueeze(0).unsqueeze(0) #to match treeqn input size\n",
    "    new_state = torch.ones_like(state)\n",
    "    middle = int(tensor.shape[0] / 4)\n",
    "    # Create transitions by modifying slices of new_state\n",
    "    new_state[:,:,:middle, :middle] += val\n",
    "    transition_one = new_state.clone()\n",
    "    new_state[:,:,-middle:,-middle:] += val\n",
    "    transition_two = new_state.clone()\n",
    "    new_state[:,:,:middle, -middle:] += val\n",
    "    transition_three = new_state.clone()\n",
    "    new_state[:,:,-middle:, :middle] += val #transition 4\n",
    "    return [transition_one/max_val, transition_two/max_val, transition_three/max_val, new_state/max_val]\n",
    "def image_world_samples(size_tensor,samples,max_val=1,x_input=-1):\n",
    "    return_data = []\n",
    "    for i in range(samples):\n",
    "        loc_tensor = torch.zeros_like(size_tensor)+0.1\n",
    "        x = int(random.random()*size_tensor.shape[0])\n",
    "        if x_input != -1:\n",
    "            x = x_input\n",
    "        loc_tensor[x] += x\n",
    "        result = image_world(loc_tensor,max_val)\n",
    "        loc_tensor = loc_tensor.unsqueeze(0).unsqueeze(0)/max_val\n",
    "        return_data.append([loc_tensor,result])\n",
    "    return return_data\n",
    "\n",
    "size_tensor = torch.zeros(20,20)\n",
    "train_data = image_world_samples(size_tensor,1000,size_tensor.shape[0],-1)\n",
    "\n",
    "test_data = [] #simply test once on all 20 possible initial states.\n",
    "for i in range(size_tensor.shape[0]):\n",
    "    test_data.append(image_world_samples(size_tensor,1,size_tensor.shape[0],i)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[20.1000, 20.1000, 20.1000, 20.1000, 20.1000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000, 20.1000, 20.1000, 20.1000, 20.1000, 20.1000],\n",
       "          [20.1000, 20.1000, 20.1000, 20.1000, 20.1000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000, 20.1000, 20.1000, 20.1000, 20.1000, 20.1000],\n",
       "          [20.1000, 20.1000, 20.1000, 20.1000, 20.1000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000, 20.1000, 20.1000, 20.1000, 20.1000, 20.1000],\n",
       "          [20.1000, 20.1000, 20.1000, 20.1000, 20.1000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000, 20.1000, 20.1000, 20.1000, 20.1000, 20.1000],\n",
       "          [20.1000, 20.1000, 20.1000, 20.1000, 20.1000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000, 20.1000, 20.1000, 20.1000, 20.1000, 20.1000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
       "          [20.1000, 20.1000, 20.1000, 20.1000, 20.1000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000, 20.1000, 20.1000, 20.1000, 20.1000, 20.1000],\n",
       "          [20.1000, 20.1000, 20.1000, 20.1000, 20.1000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000, 20.1000, 20.1000, 20.1000, 20.1000, 20.1000],\n",
       "          [20.1000, 20.1000, 20.1000, 20.1000, 20.1000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000, 20.1000, 20.1000, 20.1000, 20.1000, 20.1000],\n",
       "          [20.1000, 20.1000, 20.1000, 20.1000, 20.1000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000, 20.1000, 20.1000, 20.1000, 20.1000, 20.1000],\n",
       "          [20.1000, 20.1000, 20.1000, 20.1000, 20.1000,  1.0000,  1.0000,\n",
       "            1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
       "            1.0000, 20.1000, 20.1000, 20.1000, 20.1000, 20.1000]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][1][3]*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/Desktop/TreeQN/BIB/TreeQN_Approach/treeqn_traj.py:33: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  b_init(module.bias, b_scale)\n"
     ]
    }
   ],
   "source": [
    "input_shape = torch.zeros(1, 1,20, 20).shape# minimum size #train_data[0][0].shape\n",
    "num_actions = 4\n",
    "tree_depth = 4\n",
    "embedding_dim = 256\n",
    "td_lambda = 0.8\n",
    "gamma = 1    #0.99\n",
    "model = TreeQN(input_shape=input_shape, num_actions=num_actions, tree_depth=tree_depth, embedding_dim=embedding_dim, td_lambda=td_lambda,gamma=gamma)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "#optimizer = RMSprop(model.parameters(), lr=1e-4,alpha =0.99, eps = 1e-5) | loss from treeqn paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178325/395139831.py:34: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([4, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  first_loss = (F.mse_loss(decoded_values[1], t[1][0], reduction='none') * first).sum()\n",
      "/tmp/ipykernel_178325/395139831.py:35: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([16, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  second_loss = (F.mse_loss(decoded_values[2], t[1][1], reduction='none') * second).sum()\n",
      "/tmp/ipykernel_178325/395139831.py:36: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([64, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  third_loss = (F.mse_loss(decoded_values[3], t[1][2], reduction='none') * third).sum()\n",
      "/tmp/ipykernel_178325/395139831.py:37: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([256, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  fourth_loss = (F.mse_loss(decoded_values[4], t[1][3], reduction='none') * fourth).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 71.41816495108604, Average Raw Loss: 71.41816495108604\n",
      "Epoch 2, Average Loss: 22.543918864011765, Average Raw Loss: 22.543918864011765\n",
      "Epoch 3, Average Loss: 17.638831160306932, Average Raw Loss: 17.638831160306932\n",
      "Epoch 4, Average Loss: 14.206823327064514, Average Raw Loss: 14.206823327064514\n",
      "Epoch 5, Average Loss: 11.171888219118118, Average Raw Loss: 11.171888219118118\n",
      "Epoch 6, Average Loss: 8.809447243988513, Average Raw Loss: 8.809447243988513\n",
      "Epoch 7, Average Loss: 7.321980329692364, Average Raw Loss: 7.321980329692364\n",
      "Epoch 8, Average Loss: 6.349622824877501, Average Raw Loss: 6.349622824877501\n",
      "Epoch 9, Average Loss: 5.784014388263225, Average Raw Loss: 5.784014388263225\n",
      "Epoch 10, Average Loss: 5.461697890579701, Average Raw Loss: 5.461697890579701\n",
      "Epoch 11, Average Loss: 5.213577868610621, Average Raw Loss: 5.213577868610621\n",
      "Epoch 12, Average Loss: 5.002314271688461, Average Raw Loss: 5.002314271688461\n",
      "Epoch 13, Average Loss: 4.799968217298389, Average Raw Loss: 4.799968217298389\n",
      "Epoch 14, Average Loss: 4.57255229562521, Average Raw Loss: 4.57255229562521\n",
      "Epoch 15, Average Loss: 4.292438653707504, Average Raw Loss: 4.292438653707504\n",
      "Epoch 16, Average Loss: 4.01046462573111, Average Raw Loss: 4.01046462573111\n",
      "Epoch 17, Average Loss: 3.7423846781402825, Average Raw Loss: 3.7423846781402825\n",
      "Epoch 18, Average Loss: 3.501131173938513, Average Raw Loss: 3.501131173938513\n",
      "Epoch 19, Average Loss: 3.2647964196503163, Average Raw Loss: 3.2647964196503163\n",
      "Epoch 20, Average Loss: 3.024238546177745, Average Raw Loss: 3.024238546177745\n",
      "Epoch 21, Average Loss: 2.7866798336207865, Average Raw Loss: 2.7866798336207865\n",
      "Epoch 22, Average Loss: 2.5708690805733205, Average Raw Loss: 2.5708690805733205\n",
      "Epoch 23, Average Loss: 2.3794704355150462, Average Raw Loss: 2.3794704355150462\n",
      "Epoch 24, Average Loss: 2.2243387639671566, Average Raw Loss: 2.2243387639671566\n",
      "Epoch 25, Average Loss: 2.1119726486653088, Average Raw Loss: 2.1119726486653088\n",
      "Epoch 26, Average Loss: 2.0111866018176077, Average Raw Loss: 2.0111866018176077\n",
      "Epoch 27, Average Loss: 1.939260933279991, Average Raw Loss: 1.939260933279991\n",
      "Epoch 28, Average Loss: 1.8694532890617848, Average Raw Loss: 1.8694532890617848\n",
      "Epoch 29, Average Loss: 1.8209564494416117, Average Raw Loss: 1.8209564494416117\n",
      "Epoch 30, Average Loss: 1.7783854340687395, Average Raw Loss: 1.7783854340687395\n",
      "Epoch 31, Average Loss: 1.7338916214331985, Average Raw Loss: 1.7338916214331985\n",
      "Epoch 32, Average Loss: 1.711083364725113, Average Raw Loss: 1.711083364725113\n",
      "Epoch 33, Average Loss: 1.68129038977623, Average Raw Loss: 1.68129038977623\n",
      "Epoch 34, Average Loss: 1.6622708731144666, Average Raw Loss: 1.6622708731144666\n",
      "Epoch 35, Average Loss: 1.6409311966151, Average Raw Loss: 1.6409311966151\n",
      "Epoch 36, Average Loss: 1.6261000045239926, Average Raw Loss: 1.6261000045239926\n",
      "Epoch 37, Average Loss: 1.611203727915883, Average Raw Loss: 1.611203727915883\n",
      "Epoch 38, Average Loss: 1.600270294353366, Average Raw Loss: 1.600270294353366\n",
      "Epoch 39, Average Loss: 1.584528073333204, Average Raw Loss: 1.584528073333204\n",
      "Epoch 40, Average Loss: 1.5722480685412883, Average Raw Loss: 1.5722480685412883\n",
      "Epoch 41, Average Loss: 1.5604669468030334, Average Raw Loss: 1.5604669468030334\n",
      "Epoch 42, Average Loss: 1.5469965387359261, Average Raw Loss: 1.5469965387359261\n",
      "Epoch 43, Average Loss: 1.5201247909627855, Average Raw Loss: 1.5201247909627855\n",
      "Epoch 44, Average Loss: 1.4838329641744494, Average Raw Loss: 1.4838329641744494\n",
      "Epoch 45, Average Loss: 1.4321651333868504, Average Raw Loss: 1.4321651333868504\n",
      "Epoch 46, Average Loss: 1.359299543067813, Average Raw Loss: 1.359299543067813\n",
      "Epoch 47, Average Loss: 1.2751023654639722, Average Raw Loss: 1.2751023654639722\n",
      "Epoch 48, Average Loss: 1.1533929396942257, Average Raw Loss: 1.1533929396942257\n",
      "Epoch 49, Average Loss: 1.0149369240924715, Average Raw Loss: 1.0149369240924715\n",
      "Epoch 50, Average Loss: 0.8631862292587758, Average Raw Loss: 0.8631862292587758\n",
      "Epoch 51, Average Loss: 0.7180310355573892, Average Raw Loss: 0.7180310355573892\n",
      "Epoch 52, Average Loss: 0.5917828118428587, Average Raw Loss: 0.5917828118428587\n",
      "Epoch 53, Average Loss: 0.5024057984128595, Average Raw Loss: 0.5024057984128595\n",
      "Epoch 54, Average Loss: 0.4329761441648006, Average Raw Loss: 0.4329761441648006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m temp_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m temp_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     57\u001b[0m avg_raw_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m temp_raw_loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:394\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    393\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 394\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    397\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Main training loop\n",
    "#Looking at difference between detaching at each transition or not in treeqn file. This is with detach (so far seems to make no diff)\n",
    "raw_losses = []\n",
    "for epoch in range(3000):  # epochs\n",
    "    avg_loss = 0\n",
    "    temp_loss = 0\n",
    "    temp_raw_loss = 0\n",
    "    sample_count = 0\n",
    "\n",
    "    avg_raw_loss = 0\n",
    "\n",
    "    for t in random.sample(train_data, len(train_data)): #sample through all data in random order each epoch\n",
    "        #Get reconstruction loss to help ground abstract state\n",
    "        decoded_values, all_policies = model(t[0])\n",
    "        decode_loss = F.mse_loss(decoded_values[0], t[0], reduction='sum')\n",
    "\n",
    "        #Get transition probabilities for each state\n",
    "        first_policy = all_policies[0]\n",
    "        second_policy = all_policies[1].view(4, -1)\n",
    "        third_policy = all_policies[2].view(4, 4, -1)\n",
    "        fourth_policy = all_policies[3].view(4, 4, 4, -1)\n",
    "\n",
    "        #These should all add to 1 (in testing there seems to be some small rounding error)\n",
    "        second_layer_probs = first_policy * second_policy   \n",
    "        third_layer_probs = second_layer_probs * third_policy\n",
    "        fourth_layer_probs = third_layer_probs * fourth_policy\n",
    "        \n",
    "        #Flatten transition probabilities to then weigh with loss of each predicted state at each layer\n",
    "        first = torch.flatten(first_policy).view(4, 1, 1, 1)\n",
    "        second = torch.flatten(second_layer_probs).view(16, 1, 1, 1)\n",
    "        third = torch.flatten(third_layer_probs).view(64, 1, 1, 1)\n",
    "        fourth = torch.flatten(fourth_layer_probs).view(256, 1, 1, 1) \n",
    "        \n",
    "        first_loss = (F.mse_loss(decoded_values[1], t[1][0], reduction='none') * first).sum() \n",
    "        second_loss = (F.mse_loss(decoded_values[2], t[1][1], reduction='none') * second).sum() \n",
    "        third_loss = (F.mse_loss(decoded_values[3], t[1][2], reduction='none') * third).sum() \n",
    "        fourth_loss = (F.mse_loss(decoded_values[4], t[1][3], reduction='none') * fourth).sum() \n",
    "\n",
    "\n",
    "        #For experimenting with different weights on different layers\n",
    "        raw_loss = (decode_loss + first_loss + second_loss + third_loss + fourth_loss).detach().item()\n",
    "        raw_losses.append(raw_loss)\n",
    "        l2w , l3w ,l4w = 1,1,1\n",
    "        total_loss = decode_loss + first_loss + second_loss*l2w + third_loss*l3w + fourth_loss*l4w\n",
    "\n",
    "        temp_loss += total_loss\n",
    "        temp_raw_loss += raw_loss\n",
    "        sample_count += 1\n",
    "\n",
    "        if sample_count % 1 == 0:\n",
    "            optimizer.zero_grad()\n",
    "            temp_loss.backward()\n",
    "\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            avg_loss += temp_loss.item()\n",
    "            avg_raw_loss += temp_raw_loss\n",
    "            temp_loss = 0\n",
    "            temp_raw_loss = 0\n",
    "\n",
    "    # To handle the case where the number of samples is not a multiple of 10\n",
    "    if sample_count % 1 != 0:\n",
    "        optimizer.zero_grad()\n",
    "        temp_loss.backward()\n",
    "        \n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        avg_loss += temp_loss.item()\n",
    "        avg_raw_loss += temp_raw_loss\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss / len(train_data)}, Average Raw Loss: {avg_raw_loss / len(train_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Weight Sums tensor([-6.4030, -8.2310, 16.7770,  8.1270])\n"
     ]
    }
   ],
   "source": [
    "#View Action Weights (This hasn't been informative yet)\n",
    "dec, all_policies = model(train_data[0][0]) \n",
    "# dot = make_dot((dec[0],dec[1],dec[2],dec[3],dec[4],all_policies[0],all_policies[1],all_policies[2],all_policies[3]),params=dict(model.named_parameters()))\n",
    "# dot.render('model', format='png')\n",
    "print(f\"Action Weight Sums { torch.round(model.transition_fun.data,decimals=3).sum(dim=0).sum(dim=0)}\")  #might be summing the wrong way, or just not interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Actions: 3 3 2 1\n"
     ]
    }
   ],
   "source": [
    "best_first_action = all_policies[0].argmax()\n",
    "best_second_action = all_policies[1].view(4,-1)[best_first_action].argmax() \n",
    "best_third_action = all_policies[2].view(4,4,-1)[best_first_action][best_second_action].argmax()\n",
    "best_fourth_action = all_policies[3].view(4,4,4,-1)[best_first_action][best_second_action][best_third_action].argmax() \n",
    "# print(torch.round(all_q[0],decimals=3).detach(), f\"Argmax {all_q[0].argmax().item()}\")\n",
    "# print(torch.round(all_q[1],decimals=3).view(4,-1).detach(),f\"Argmax {all_q[1].view(4,-1)[1].argmax().item()}\")\n",
    "# print(torch.round(all_q[2],decimals=3).view(4,4,-1)[0].detach(),f\"Argmax {all_q[2].view(4,4,-1)[1][0].argmax().item()}\")\n",
    "# print(torch.round(all_q[3],decimals=3).view(4,4,4,-1)[0][0].detach(),f\"Argmax {all_q[3].view(4,4,4,-1)[1][1][0].argmax().item()}\")\n",
    "print(f\"Best Actions: {best_first_action.item()} {best_second_action.item()} {best_third_action.item()} {best_fourth_action.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178325/1188555054.py:27: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([4, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  first_loss = (F.mse_loss(decoded_values[1], t[1][0], reduction='none') * first).sum()\n",
      "/tmp/ipykernel_178325/1188555054.py:28: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([16, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  second_loss = (F.mse_loss(decoded_values[2], t[1][1], reduction='none') * second).sum()\n",
      "/tmp/ipykernel_178325/1188555054.py:29: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([64, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  third_loss = (F.mse_loss(decoded_values[3], t[1][2], reduction='none') * third).sum()\n",
      "/tmp/ipykernel_178325/1188555054.py:30: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([256, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  fourth_loss = (F.mse_loss(decoded_values[4], t[1][3], reduction='none') * fourth).sum()\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "loss_data = []\n",
    "with torch.no_grad():\n",
    "    eval_loss = 0\n",
    "    for i,k in zip(test_data,range(len(test_data))):\n",
    "        #Get reconstruction loss to help ground abstract state\n",
    "        decoded_values, all_policies = model(t[0])\n",
    "        decode_loss = F.mse_loss(decoded_values[0], t[0], reduction='sum')\n",
    "\n",
    "        #Get transition probabilities for each state\n",
    "        first_policy = all_policies[0]\n",
    "        second_policy = all_policies[1].view(4, -1)\n",
    "        third_policy = all_policies[2].view(4, 4, -1)\n",
    "        fourth_policy = all_policies[3].view(4, 4, 4, -1)\n",
    "\n",
    "        #These should all add to 1 (in testing there seems to be some small rounding error)\n",
    "        second_layer_probs = first_policy * second_policy   \n",
    "        third_layer_probs = second_layer_probs * third_policy\n",
    "        fourth_layer_probs = third_layer_probs * fourth_policy\n",
    "        \n",
    "        #Flatten transition probabilities to then weigh with loss of each predicted state at each layer\n",
    "        first = torch.flatten(first_policy).view(4, 1, 1, 1)\n",
    "        second = torch.flatten(second_layer_probs).view(16, 1, 1, 1)\n",
    "        third = torch.flatten(third_layer_probs).view(64, 1, 1, 1)\n",
    "        fourth = torch.flatten(fourth_layer_probs).view(256, 1, 1, 1) \n",
    "        \n",
    "        first_loss = (F.mse_loss(decoded_values[1], t[1][0], reduction='none') * first).sum() \n",
    "        second_loss = (F.mse_loss(decoded_values[2], t[1][1], reduction='none') * second).sum() \n",
    "        third_loss = (F.mse_loss(decoded_values[3], t[1][2], reduction='none') * third).sum() \n",
    "        fourth_loss = (F.mse_loss(decoded_values[4], t[1][3], reduction='none') * fourth).sum() \n",
    "\n",
    "        #For experimenting with different weights on different layers\n",
    "        raw_loss = (decode_loss + first_loss + second_loss + third_loss + fourth_loss).detach().item()\n",
    "\n",
    "        loss_data.append([k,decode_loss.item(),first_loss.item(),second_loss.item(),third_loss.item(),fourth_loss.item(),raw_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decode Loss</th>\n",
       "      <th>First Loss</th>\n",
       "      <th>Second Loss</th>\n",
       "      <th>Third Loss</th>\n",
       "      <th>Fourth Loss</th>\n",
       "      <th>Total Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Starting Input</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Decode Loss  First Loss  Second Loss  Third Loss  Fourth Loss  \\\n",
       "Starting Input                                                                  \n",
       "0                       0.1        0.02         0.02        0.02         0.01   \n",
       "1                       0.1        0.02         0.02        0.02         0.01   \n",
       "2                       0.1        0.02         0.02        0.02         0.01   \n",
       "3                       0.1        0.02         0.02        0.02         0.01   \n",
       "4                       0.1        0.02         0.02        0.02         0.01   \n",
       "5                       0.1        0.02         0.02        0.02         0.01   \n",
       "6                       0.1        0.02         0.02        0.02         0.01   \n",
       "7                       0.1        0.02         0.02        0.02         0.01   \n",
       "8                       0.1        0.02         0.02        0.02         0.01   \n",
       "9                       0.1        0.02         0.02        0.02         0.01   \n",
       "10                      0.1        0.02         0.02        0.02         0.01   \n",
       "11                      0.1        0.02         0.02        0.02         0.01   \n",
       "12                      0.1        0.02         0.02        0.02         0.01   \n",
       "13                      0.1        0.02         0.02        0.02         0.01   \n",
       "14                      0.1        0.02         0.02        0.02         0.01   \n",
       "15                      0.1        0.02         0.02        0.02         0.01   \n",
       "16                      0.1        0.02         0.02        0.02         0.01   \n",
       "17                      0.1        0.02         0.02        0.02         0.01   \n",
       "18                      0.1        0.02         0.02        0.02         0.01   \n",
       "19                      0.1        0.02         0.02        0.02         0.01   \n",
       "\n",
       "                Total Loss  \n",
       "Starting Input              \n",
       "0                     0.16  \n",
       "1                     0.16  \n",
       "2                     0.16  \n",
       "3                     0.16  \n",
       "4                     0.16  \n",
       "5                     0.16  \n",
       "6                     0.16  \n",
       "7                     0.16  \n",
       "8                     0.16  \n",
       "9                     0.16  \n",
       "10                    0.16  \n",
       "11                    0.16  \n",
       "12                    0.16  \n",
       "13                    0.16  \n",
       "14                    0.16  \n",
       "15                    0.16  \n",
       "16                    0.16  \n",
       "17                    0.16  \n",
       "18                    0.16  \n",
       "19                    0.16  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_df = pd.DataFrame(loss_data,columns=[\"Starting Input\",\"Decode Loss\",\"First Loss\",\"Second Loss\",\"Third Loss\",\"Fourth Loss\",\"Total Loss\"]).set_index(\"Starting Input\").round(2)\n",
    "loss_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5.650661468505859,\n",
       "  3.1693928241729736,\n",
       "  7.215744495391846,\n",
       "  23.009117126464844],\n",
       " [47.17615509033203, 61.34219741821289, 65.8885498046875, 77.55485534667969])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A check on if the training loop is valid\n",
    "#Checking loss of each state unweighted, with both absolute difference loss and mse loss\n",
    "min_losses = []\n",
    "max_losses = []\n",
    "for i in range(1,len(dec)):\n",
    "    decoded_states = dec[i]\n",
    "    true_state = train_data[0][1][i-1]\n",
    "    curr_min = float('inf')\n",
    "    curr_max = float('-inf')\n",
    "    for state in decoded_states:\n",
    "        loss = torch.abs(state-true_state).sum()\n",
    "        if loss < curr_min:\n",
    "            curr_min = loss.item()\n",
    "        if loss > curr_max:\n",
    "            curr_max = loss.item()\n",
    "    min_losses.append(curr_min)\n",
    "    max_losses.append(curr_max)\n",
    "min_losses,max_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_178325/3588761361.py:9: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(state,true_state)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.0005000769160687923,\n",
       "  0.00011153663217555732,\n",
       "  0.0008009815355762839,\n",
       "  0.00929505005478859],\n",
       " [0.05528023838996887,\n",
       "  0.06488228589296341,\n",
       "  0.07099036127328873,\n",
       "  0.10481913387775421])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_losses = []\n",
    "max_losses = []\n",
    "for i in range(1,len(dec)):\n",
    "    decoded_states = dec[i]\n",
    "    true_state = train_data[0][1][i-1]\n",
    "    curr_min = float('inf')\n",
    "    curr_max = float('-inf')\n",
    "    for state in decoded_states:\n",
    "        loss = F.mse_loss(state,true_state)\n",
    "        if loss < curr_min:\n",
    "            curr_min = loss.item()\n",
    "        if loss > curr_max:\n",
    "            curr_max = loss.item()\n",
    "    min_losses.append(curr_min)\n",
    "    max_losses.append(curr_max)\n",
    "min_losses,max_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
