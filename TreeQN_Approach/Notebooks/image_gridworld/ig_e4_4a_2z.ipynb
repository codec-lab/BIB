{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "from treeQN.treeqn_traj_simplest import TreeQN\n",
    "import random\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start(size):\n",
    "    goal_point = (size//2, size//2)\n",
    "    start_point = -1\n",
    "    while True:\n",
    "        start_point = (random.randint(0,size), random.randint(0,size))\n",
    "        if goal_point != start_point:\n",
    "            break\n",
    "    return start_point, goal_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_policy(state,goal_point):\n",
    "    goal_x, goal_y = goal_point\n",
    "    x,y = state\n",
    "    x_right = goal_x > x # if goal is right\n",
    "    x_left = goal_x < x # if goal is left\n",
    "    y_up = goal_y > y # if goal is above\n",
    "    y_down = goal_y < y # if goal is below\n",
    "    possible_next_states = []\n",
    "    if x_right:\n",
    "        possible_next_states.append((x+1,y))\n",
    "    if x_left:\n",
    "        possible_next_states.append((x-1,y))\n",
    "    if y_up:\n",
    "        possible_next_states.append((x,y+1))\n",
    "    if y_down:\n",
    "        possible_next_states.append((x,y-1))\n",
    "    if len(possible_next_states) == 0:\n",
    "        return -1\n",
    "    return random.choice(possible_next_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_to_tensor(point,goal,size,noise=0):\n",
    "    x,y = point\n",
    "    x_goal, y_goal = goal\n",
    "    tensor = torch.zeros(size,size)\n",
    "    scale = 1\n",
    "    tensor[x][y] = 1 * scale\n",
    "    # tensor[x+1][y] = 1 * scale\n",
    "    # tensor[x][y+1] = 1 * scale\n",
    "    # tensor[x+1][y+1] = 1\n",
    "    tensor[x_goal][y_goal] = -1 * scale\n",
    "    # tensor[x_goal+1][y_goal] = -1 * scale\n",
    "    # tensor[x_goal][y_goal+1] = -1 * scale\n",
    "    # tensor[x_goal+1][y_goal+1] = -1 * scale\n",
    "    #add gaussian noise\n",
    "    tensor = tensor + torch.randn(tensor.size()) * noise\n",
    "    \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(size,start_point = None, goal_point = None):\n",
    "    trajectory = []\n",
    "    if start_point is None:\n",
    "        start, goal = get_start(size)\n",
    "    else:\n",
    "        start, goal = start_point, goal_point\n",
    "    trajectory.append(start)\n",
    "    while start != goal:\n",
    "        start = hard_policy(start,goal)\n",
    "        trajectory.append(start)\n",
    "    if len(trajectory) != 6:\n",
    "        return get_trajectory(size)\n",
    "    trajectory = trajectory[:5] #dont turn into goal\n",
    "    return [point_to_tensor(p,goal,size).unsqueeze(0).unsqueeze(0) for p in trajectory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_starting_points(size):\n",
    "    start_points = []\n",
    "    for i in range(10000):\n",
    "        start_points.append(get_start(size)[0])\n",
    "    start_points = set(start_points)\n",
    "    goal_point = (size//2, size//2)\n",
    "    return start_points, goal_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 20\n",
    "s, goal_point = max_starting_points(size)\n",
    "start_points = list(s)\n",
    "train_start_points = start_points[:len(start_points)//2]\n",
    "test_start_points = start_points[len(start_points)//2:]\n",
    "\n",
    "train_data = [get_trajectory(size,start_point,goal_point) for start_point in train_start_points]\n",
    "valid_data = [get_trajectory(size,start_point,goal_point) for start_point in test_start_points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Model Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 20, 20])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einsum Transiton\n"
     ]
    }
   ],
   "source": [
    "input_shape = train_data[0][0].shape# minimum size #train_data[0][0].shape\n",
    "num_actions = 1\n",
    "tree_depth = 4\n",
    "embedding_dim = 64\n",
    "td_lambda = 1\n",
    "gamma = 0.9    #0.99\n",
    "decode_dropout = 0.5\n",
    "t1 = False #True is Einsum. False +dx \n",
    "model = TreeQN(input_shape=input_shape, num_actions=num_actions, tree_depth=tree_depth, embedding_dim=embedding_dim, td_lambda=td_lambda,gamma=gamma,decode_dropout=decode_dropout,t1=False)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "#optimizer = RMSprop(model.parameters(), lr=1e-4,alpha =0.99, eps = 1e-5) | loss from treeqn paper\n",
    "# Collect all encoder and decoder parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.decoder(model.encoder(train_data[0][0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, valid_data):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for t in valid_data:\n",
    "            decoded_values, all_policies = model(t[0])\n",
    "            decode_loss = F.mse_loss(decoded_values[0], t[0], reduction='sum')\n",
    "\n",
    "            first_policy = all_policies[0]\n",
    "            second_policy = all_policies[1].view(num_actions, -1)\n",
    "            third_policy = all_policies[2].view(num_actions, num_actions, -1)\n",
    "            fourth_policy = all_policies[3].view(num_actions, num_actions, num_actions, -1)\n",
    "\n",
    "            second_layer_probs = first_policy * second_policy\n",
    "            third_layer_probs = second_layer_probs * third_policy\n",
    "            fourth_layer_probs = third_layer_probs * fourth_policy\n",
    "\n",
    "            first = torch.flatten(first_policy).view(num_actions, 1, 1, 1)\n",
    "            second = torch.flatten(second_layer_probs).view(num_actions**2, 1, 1, 1)\n",
    "            third = torch.flatten(third_layer_probs).view(num_actions**3, 1, 1, 1)\n",
    "            fourth = torch.flatten(fourth_layer_probs).view(num_actions**4, 1, 1, 1)\n",
    "\n",
    "            first_loss = (F.mse_loss(decoded_values[1], t[1], reduction='none') * first).sum()\n",
    "            second_loss = (F.mse_loss(decoded_values[2], t[2], reduction='none') * second).sum()\n",
    "            third_loss = (F.mse_loss(decoded_values[3], t[3], reduction='none') * third).sum()\n",
    "            fourth_loss = (F.mse_loss(decoded_values[4], t[4], reduction='none') * fourth).sum()\n",
    "\n",
    "            l2w, l3w, l4w = 1, 1, 1\n",
    "            total_loss += first_loss + second_loss * l2w + third_loss * l3w + fourth_loss * l4w + decode_loss\n",
    "\n",
    "    return total_loss / len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_gradients(model):\n",
    "    gradients = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            gradients.append([name, param.grad.norm().item()])\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Regularization strength (lambda)\n",
    "# lambda_reg = 0 #1e-5\n",
    "\n",
    "# all_gradients = []\n",
    "# raw_losses = []\n",
    "# for epoch in range(10000):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for t in random.sample(train_data, len(train_data)):\n",
    "#         for sample in t:\n",
    "#             optimizer.zero_grad() \n",
    "#             encoding = model.encoder(sample)\n",
    "#             decoding = model.decoder(encoding)\n",
    "            \n",
    "#             # Compute reconstruction loss (MSE)\n",
    "#             reconstruction_loss = F.mse_loss(decoding, sample)\n",
    "            \n",
    "#             # Compute L2 regularization (sum of squared weights)\n",
    "#             l2_reg = 0\n",
    "#             for param in model.parameters():\n",
    "#                 l2_reg += torch.sum(param ** 2)\n",
    "            \n",
    "#             # Combine losses\n",
    "#             loss = reconstruction_loss + lambda_reg * l2_reg\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "        \n",
    "#         all_gradients.append(store_gradients(model))\n",
    "#         raw_losses.append(total_loss)\n",
    "    \n",
    "#     print(f'Epoch {epoch}, Loss: {total_loss}')\n",
    "# #max loss is 0.53 for embed dim 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #freeze encoder and decoder\n",
    "# for param in model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.decoder.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_autoencode ability\n",
    "# with torch.no_grad():\n",
    "#     test_sample = random.choice(valid_data)\n",
    "#     encoding = model.encoder(test_sample[0])\n",
    "#     decoding = model.decoder(encoding)\n",
    "#     print('Original:', test_sample[0].numpy())\n",
    "#     print('Reconstructed:', np.round(decoding.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 1, 1])\n",
      "og\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "testing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m third_layer_probs \u001b[38;5;241m=\u001b[39m second_layer_probs \u001b[38;5;241m*\u001b[39m third_policy\n\u001b[1;32m     46\u001b[0m fourth_layer_probs \u001b[38;5;241m=\u001b[39m third_layer_probs \u001b[38;5;241m*\u001b[39m fourth_policy\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Flatten transition probabilities to then weigh with loss of each predicted state at each layer\u001b[39;00m\n\u001b[1;32m     51\u001b[0m first \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(first_policy)\u001b[38;5;241m.\u001b[39mview(num_actions, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: testing"
     ]
    }
   ],
   "source": [
    "all_gradients = []\n",
    "raw_losses = []\n",
    "model.train()  \n",
    "s1 = num_actions\n",
    "s2 = num_actions**2\n",
    "s3 = num_actions**3\n",
    "s4 = num_actions**4\n",
    "\n",
    "for epoch in range(300):  # epochs\n",
    "# Set the model to training mode\n",
    "    avg_loss = 0\n",
    "    temp_loss = 0\n",
    "    temp_raw_loss = 0\n",
    "    sample_count = 0\n",
    "\n",
    "    avg_decode_loss, avg_first_loss, avg_second_loss, avg_third_loss, avg_fourth_loss = 0, 0, 0, 0, 0\n",
    "\n",
    "    avg_raw_loss = 0\n",
    "    avg_valid_loss =0\n",
    "    raw_gradients = []\n",
    "    for t in random.sample(train_data, len(train_data)):  # sample through all data in random order each epoch\n",
    "        # Get reconstruction loss to help ground abstract state\n",
    "        decoded_values, all_policies = model(t[0])\n",
    "        decode_loss = F.mse_loss(decoded_values[0], t[0], reduction='sum')\n",
    "\n",
    "        # Get transition probabilities for each state\n",
    "        first_policy = all_policies[0]\n",
    "        second_policy = all_policies[1].view(num_actions, -1)\n",
    "        third_policy = all_policies[2].view(num_actions, num_actions, -1)\n",
    "        fourth_policy = all_policies[3].view(num_actions, num_actions, num_actions, -1)\n",
    "\n",
    "        print(first_policy.shape)\n",
    "        print(second_policy.shape)\n",
    "        print(third_policy.shape)\n",
    "        print(fourth_policy.shape)\n",
    "\n",
    "        print('og')\n",
    "        print(all_policies[0].shape)\n",
    "        print(all_policies[1].shape)\n",
    "        print(all_policies[2].shape)\n",
    "        print(all_policies[3].shape)\n",
    "\n",
    "        # These should all add to 1 (in testing there seems to be some small rounding error)\n",
    "        second_layer_probs = first_policy * second_policy\n",
    "        third_layer_probs = second_layer_probs * third_policy\n",
    "        fourth_layer_probs = third_layer_probs * fourth_policy\n",
    "\n",
    "        raise ValueError(\"testing\")\n",
    "\n",
    "        # Flatten transition probabilities to then weigh with loss of each predicted state at each layer\n",
    "        first = torch.flatten(first_policy).view(num_actions, 1, 1, 1)\n",
    "        second = torch.flatten(second_layer_probs).view(s2, 1, 1, 1)\n",
    "        third = torch.flatten(third_layer_probs).view(s3, 1, 1, 1)\n",
    "        fourth = torch.flatten(fourth_layer_probs).view(s4, 1, 1, 1)\n",
    "\n",
    "        first_loss = (F.mse_loss(decoded_values[1], t[1], reduction='none') * first).sum()\n",
    "        second_loss = (F.mse_loss(decoded_values[2], t[2], reduction='none') * second).sum()\n",
    "        third_loss = (F.mse_loss(decoded_values[3], t[3], reduction='none') * third).sum()\n",
    "        fourth_loss = (F.mse_loss(decoded_values[4], t[4], reduction='none') * fourth).sum()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # For experimenting with different weights on different layers\n",
    "        raw_loss = (first_loss + second_loss + third_loss + fourth_loss).detach().item()\n",
    "        raw_losses.append(raw_loss)\n",
    "        l2w, l3w, l4w = 1, 1, 1\n",
    "        total_loss = first_loss + second_loss * l2w + third_loss * l3w + fourth_loss * l4w + decode_loss\n",
    "\n",
    "        # break if total loss is nan\n",
    "        if torch.isnan(total_loss):\n",
    "            raise ValueError(\"NAN LOSS\")\n",
    "\n",
    "\n",
    "        avg_decode_loss += decode_loss.item()\n",
    "        avg_first_loss += first_loss.item()\n",
    "        avg_second_loss += second_loss.item()\n",
    "        avg_third_loss += third_loss.item()\n",
    "        avg_fourth_loss += fourth_loss.item()\n",
    "\n",
    "        temp_loss += total_loss\n",
    "        temp_raw_loss += raw_loss\n",
    "        sample_count += 1\n",
    "        avg_valid_loss += 0 #validate(model, valid_data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        temp_loss.backward()\n",
    "        # Monitor gradients before clipping and stepping\n",
    "        all_gradients.append(store_gradients(model))\n",
    "\n",
    "        # Uncomment if you want to use gradient clipping\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        optimizer.step()\n",
    "        avg_loss += temp_loss.item()\n",
    "        avg_raw_loss += temp_raw_loss\n",
    "        temp_loss = 0\n",
    "        temp_raw_loss = 0\n",
    "\n",
    "    # Freeze encoder and decoder after first epoch\n",
    "    # if epoch == 1:\n",
    "    #     for param in model.encoder.parameters():\n",
    "    #         param.requires_grad = False\n",
    "    #     for param in model.decoder.parameters():\n",
    "    #         param.requires_grad = False\n",
    "    if epoch % 10 == 0: \n",
    "        #print just validation\n",
    "        print(f\"Epoch {epoch + 1}, Validation Loss: {validate(model, valid_data)}\")\n",
    "\n",
    "    avg_decode_loss = avg_decode_loss / len(train_data)\n",
    "    avg_first_loss = avg_first_loss / len(train_data)\n",
    "    avg_second_loss = avg_second_loss / len(train_data)\n",
    "    avg_third_loss = avg_third_loss / len(train_data)\n",
    "    avg_fourth_loss = avg_fourth_loss / len(train_data)\n",
    "\n",
    "\n",
    "    avg_train_loss = avg_loss / len(train_data)\n",
    "    avg_train_raw_loss = avg_raw_loss / len(train_data)\n",
    "    avg_valid_loss = avg_valid_loss / len(valid_data)\n",
    "    # Perform validation\n",
    "    print(f\"Epoch {epoch + 1}, Total Loss: {avg_train_loss}, DLoss: {avg_decode_loss}, A1: {avg_first_loss}, A2: {avg_second_loss}, A3: {avg_third_loss}, A4: {avg_fourth_loss}\")\n",
    "\n",
    "#Max loss with 4 actions, 1.38 after 130 epochs\n",
    "#Max loss with 1 action, 1.5. Bascially the same..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 581, Validation Loss: 0.8256955742835999 Old T function\n",
    "Epoch 581, Train Loss: 0.7633827402764424, Train Raw Loss: 0.7448219670363787, Validation Loss: 0.0\n",
    "Epoch 582, Train Loss: 0.7654743137176742, Train Raw Loss: 0.7468309859618206, Validation Loss: 0.0\n",
    "Epoch 583, Train Loss: 0.7629341331564568, Train Raw Loss: 0.7446876400827684, Validation Loss: 0.0\n",
    "Epoch 584, Train Loss: 0.7623813437213275, Train Raw Loss: 0.7441016497005793, Validation Loss: 0.0\n",
    "Epoch 585, Train Loss: 0.759624786996706, Train Raw Loss: 0.7415352151136506, Validation Loss: 0.0\n",
    "Epoch 586, Train Loss: 0.7635865574025295, Train Raw Loss: 0.7449935508108783, Validation Loss: 0.0\n",
    "Epoch 587, Train Loss: 0.7689634615301408, Train Raw Loss: 0.750305650933561, Validation Loss: 0.0\n",
    "Epoch 588, Train Loss: 0.7626074822196229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64])\n",
      "torch.Size([1, 1, 20, 20])\n",
      "tensor(167)\n",
      "original state\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "action taken: 0\n",
      "tensor([[ 0.,  0., -0.,  0., -0., -0., -0., -0.,  0.,  0.,  0.,  0.],\n",
      "        [-0.,  0., -0., -0., -0.,  0.,  0., -0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., -0.,  0.,  0., -0., -0., -0., -0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., -0.,  0., -0., -0.,  0.,  0.,  0., -0., -0.,  0.,  0.],\n",
      "        [ 0.,  0., -0., -0.,  1.,  0.,  0., -0.,  0., -0., -0.,  0.],\n",
      "        [ 0., -0.,  0.,  0., -0., -0., -0., -0., -0.,  0., -0.,  0.],\n",
      "        [-0., -0.,  0., -0.,  0., -0., -1., -0., -0., -0.,  0.,  0.],\n",
      "        [-0., -0.,  0., -0., -0., -0.,  0., -0., -0.,  0.,  0.,  0.],\n",
      "        [ 0., -0.,  0., -0.,  0., -0.,  0., -0., -0., -0.,  0., -0.],\n",
      "        [ 0., -0.,  0., -0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0., -0., -0.,  0., -0., -0., -0.,  0.,  0., -0., -0.,  0.],\n",
      "        [-0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0., -0.,  0., -0.]],\n",
      "       grad_fn=<RoundBackward0>)\n",
      "action taken: 1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction taken:\u001b[39m\u001b[38;5;124m'\u001b[39m,i)\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mround(\u001b[43mdecoded_transition\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]))\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "test_state = train_data[34][0]\n",
    "encoded_test = model.encoder(test_state)\n",
    "transitioned_test = model.tree_transition(encoded_test)\n",
    "print(transitioned_test.shape)\n",
    "decoded_transition = model.decoder(transitioned_test)\n",
    "print(decoded_transition.shape)\n",
    "print(test_state.argmax())\n",
    "print('original state')\n",
    "print(test_state.squeeze(0).squeeze(0)[4:-4, 4:-4])\n",
    "for i in range(4):\n",
    "    print('action taken:',i)\n",
    "    print(torch.round(decoded_transition[i].squeeze(0).squeeze(0)[4:-4, 4:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_action(start_state, action_index, tf_tensor):\n",
    "    with torch.no_grad():\n",
    "        #encode start_state:\n",
    "        z = model.encoder(start_state)\n",
    "        transitions = (torch.einsum(\"ij,jab->iba\", z, tf_tensor))\n",
    "        decoded_states = model.decoder(transitions)\n",
    "        print(decoded_states.shape)\n",
    "        return torch.round(model.decoder(transitions[action_index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition_fun: 1.94\n",
      "goal: 0.0\n",
      "goal_beta: 0.0\n",
      "decoder.fc.weight: 2.96\n",
      "decoder.fc.bias: 0.08\n",
      "decoder.deconv1.weight: 6.28\n",
      "decoder.deconv1.bias: 0.22\n",
      "decoder.deconv2.weight: 7.74\n",
      "decoder.deconv2.bias: 0.82\n",
      "decoder.final_conv.weight: 7.27\n",
      "decoder.final_conv.bias: 0.0\n",
      "encoder.cnn_encoder.conv1.weight: 1.24\n",
      "encoder.cnn_encoder.conv1.bias: 2.8\n",
      "encoder.cnn_encoder.bn1.weight: 0.26\n",
      "encoder.cnn_encoder.bn1.bias: 0.12\n",
      "encoder.cnn_encoder.conv2.weight: 2.37\n",
      "encoder.cnn_encoder.conv2.bias: 0.32\n",
      "encoder.cnn_encoder.bn2.weight: 0.19\n",
      "encoder.cnn_encoder.bn2.bias: 0.11\n",
      "encoder.cnn_encoder.conv3.weight: 1.55\n",
      "encoder.cnn_encoder.conv3.bias: 0.12\n",
      "encoder.cnn_encoder.bn3.weight: 0.13\n",
      "encoder.cnn_encoder.bn3.bias: 0.04\n",
      "encoder.cnn_encoder.residual_conv.weight: 0.36\n",
      "encoder.cnn_encoder.residual_conv.bias: 0.05\n",
      "encoder.linear.weight: 4.45\n",
      "encoder.linear.bias: 0.04\n"
     ]
    }
   ],
   "source": [
    "#4.46, 5.3 at 2000\n",
    "for name, num in all_gradients[-1]:\n",
    "    print(name +':', round(num, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_path(decoded, all_policies):\n",
    "    best_first_action = all_policies[0].argmax().item()\n",
    "    best_second_action = all_policies[1].view(num_actions,-1)[best_first_action].argmax().item() \n",
    "    best_third_action = all_policies[2].view(num_actions,num_actions,-1)[best_first_action][best_second_action].argmax().item()\n",
    "    best_fourth_action = all_policies[3].view(num_actions,num_actions,num_actions,-1)[best_first_action][best_second_action][best_third_action].argmax().item()\n",
    "    print(best_first_action,best_second_action,best_third_action,best_fourth_action)\n",
    "    first = decoded[0]\n",
    "    second = decoded[1][best_first_action].unsqueeze(0)\n",
    "    third = decoded[2][best_first_action**2 + best_second_action].unsqueeze(0)\n",
    "    fourth = decoded[3][best_first_action**3 + best_second_action**2 + best_third_action].unsqueeze(0)\n",
    "    fifth = decoded[4][best_first_action**4 + best_second_action**3 + best_third_action**2 + best_fourth_action].unsqueeze(0)\n",
    "    return [first,second,third,fourth,fifth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_path_actions(decoded, all_policies):\n",
    "    best_first_action = all_policies[0].argmax().item()\n",
    "    best_second_action = all_policies[1].view(num_actions,-1)[best_first_action].argmax().item() \n",
    "    best_third_action = all_policies[2].view(num_actions,num_actions,-1)[best_first_action][best_second_action].argmax().item()\n",
    "    best_fourth_action = all_policies[3].view(num_actions,num_actions,num_actions,-1)[best_first_action][best_second_action][best_third_action].argmax().item()\n",
    "    return best_first_action,best_second_action,best_third_action,best_fourth_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_data = []\n",
    "for i in valid_data:\n",
    "    d,q = model(i[0])\n",
    "    a_data.append(get_best_path_actions(d,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    0.0\n",
       "2    0.0\n",
       "3    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(a_data).var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "#try backward in succession?\n",
    "a_data = []\n",
    "for i in valid_data:\n",
    "    a,b = model(i[0])\n",
    "    get_best_path(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64])\n",
      "torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "test_trans = torch.rand(num_actions,embedding_dim)\n",
    "enc_state = model.encoder(valid_data[0][0])\n",
    "print(test_trans.shape)\n",
    "print(enc_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = (test_trans + enc_state)\n",
    "t1 = t1.view(num_actions,embedding_dim,-1).shape\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(210)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[0][2].argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, q = model(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0\n",
      "Start\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "\n",
      "\n",
      "Action 1\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "\n",
      "\n",
      "Action 2\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  1.,  0.,  0., -1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "\n",
      "\n",
      "Action 3\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  1.,  0., -1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "\n",
      "\n",
      "Action 4\n",
      "tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def viewer(tensor):\n",
    "    tensor = tensor.squeeze(0).squeeze(0)\n",
    "    max_val_index = tensor.argmax().item()\n",
    "    min_val_index = tensor.argmin().item()\n",
    "    max_val = tensor.max().item()\n",
    "    min_val = tensor.min().item()\n",
    "\n",
    "    new_tensor = torch.zeros_like(tensor).flatten()\n",
    "    new_tensor[max_val_index] = 1\n",
    "    new_tensor[min_val_index] = -1\n",
    "    new_tensor = new_tensor.view(tensor.shape)\n",
    "\n",
    "    zoomed_tensor = new_tensor[5:-5, 5:-5]\n",
    "    return torch.round(zoomed_tensor)\n",
    "\n",
    "def viewer_2(tensor): \n",
    "    tensor = tensor.squeeze(0).squeeze(0) \n",
    "    zoomed_tensor = tensor[5:-5, 5:-5]\n",
    "    return torch.round(zoomed_tensor)\n",
    "\n",
    "def print_movie(tensor_list):\n",
    "    for tensor,name in zip(tensor_list,[\"Start\",\"Action 1\",\"Action 2\",\"Action 3\",\"Action 4\"]):\n",
    "        print(name)\n",
    "        print(viewer(tensor))\n",
    "        print(\"\\n\")\n",
    "\n",
    "print_movie(get_best_path(d,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Weight Sums tensor([ 0.5190,  0.5720,  0.1650,  0.8650,  0.7450,  0.3820,  0.2240,  0.3700,\n",
      "         0.2120,  0.7790,  0.7160,  0.8930,  0.3270,  0.2910,  0.4630,  0.3670,\n",
      "         0.5340,  0.4240,  0.9550,  0.2990,  0.3370,  0.0580,  0.1530,  0.0390,\n",
      "         0.9360,  0.7010,  0.9120,  0.8130,  0.3640,  0.5070,  0.6280,  0.6390,\n",
      "        -0.0670,  0.4870,  0.9580,  1.0000,  0.7500,  0.2280,  0.6930,  0.4580,\n",
      "         0.1240,  0.1720,  0.3240,  0.5160,  0.2940,  0.0600,  0.9600,  0.9100,\n",
      "         0.6570,  0.4000,  0.7830,  0.1180,  0.5930,  0.5110,  0.5780,  1.0540,\n",
      "         0.8860,  0.4110,  0.7130,  0.5170,  0.3490,  0.5970,  0.2290,  0.6070])\n"
     ]
    }
   ],
   "source": [
    "#View Action Weights (This hasn't been informative yet)\n",
    "dec, all_policies = model(train_data[0][0]) \n",
    "# dot = make_dot((dec[0],dec[1],dec[2],dec[3],dec[4],all_policies[0],all_policies[1],all_policies[2],all_policies[3]),params=dict(model.named_parameters()))\n",
    "# dot.render('model', format='png')\n",
    "print(f\"Action Weight Sums { torch.round(model.transition_fun.data,decimals=3).sum(dim=0).sum(dim=0)}\")  #might be summing the wrong way, or just not interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Actions: 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "best_first_action = all_policies[0].argmax()\n",
    "best_second_action = all_policies[1].view(num_actions,-1)[best_first_action].argmax() \n",
    "best_third_action = all_policies[2].view(num_actions,num_actions,-1)[best_first_action][best_second_action].argmax()\n",
    "best_fourth_action = all_policies[3].view(num_actions,num_actions,num_actions,-1)[best_first_action][best_second_action][best_third_action].argmax() \n",
    "# print(torch.round(all_q[0],decimals=3).detach(), f\"Argmax {all_q[0].argmax().item()}\")\n",
    "# print(torch.round(all_q[1],decimals=3).view(4,-1).detach(),f\"Argmax {all_q[1].view(4,-1)[1].argmax().item()}\")\n",
    "# print(torch.round(all_q[2],decimals=3).view(4,4,-1)[0].detach(),f\"Argmax {all_q[2].view(4,4,-1)[1][0].argmax().item()}\")\n",
    "# print(torch.round(all_q[3],decimals=3).view(4,4,4,-1)[0][0].detach(),f\"Argmax {all_q[3].view(4,4,4,-1)[1][1][0].argmax().item()}\")\n",
    "print(f\"Best Actions: {best_first_action.item()} {best_second_action.item()} {best_third_action.item()} {best_fourth_action.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, -1]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m dec, all_policies \u001b[38;5;241m=\u001b[39m model(train_data[i][\u001b[38;5;241m0\u001b[39m]) \n\u001b[1;32m      5\u001b[0m best_first_action \u001b[38;5;241m=\u001b[39m all_policies[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmax()\n\u001b[0;32m----> 6\u001b[0m best_second_action \u001b[38;5;241m=\u001b[39m \u001b[43mall_policies\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[best_first_action]\u001b[38;5;241m.\u001b[39margmax() \n\u001b[1;32m      7\u001b[0m best_third_action \u001b[38;5;241m=\u001b[39m all_policies[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[best_first_action][best_second_action]\u001b[38;5;241m.\u001b[39margmax()\n\u001b[1;32m      8\u001b[0m best_fourth_action \u001b[38;5;241m=\u001b[39m all_policies[\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[best_first_action][best_second_action][best_third_action]\u001b[38;5;241m.\u001b[39margmax() \n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[4, -1]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "#View Action Weights (This hasn't been informative yet)\n",
    "for i in range(500):\n",
    "    dec, all_policies = model(train_data[i][0]) \n",
    "\n",
    "    best_first_action = all_policies[0].argmax()\n",
    "    best_second_action = all_policies[1].view(4,-1)[best_first_action].argmax() \n",
    "    best_third_action = all_policies[2].view(4,4,-1)[best_first_action][best_second_action].argmax()\n",
    "    best_fourth_action = all_policies[3].view(4,4,4,-1)[best_first_action][best_second_action][best_third_action].argmax() \n",
    "\n",
    "    print(f\"Best Actions: {best_first_action.item()} {best_second_action.item()} {best_third_action.item()} {best_fourth_action.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! JSON saved to output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_file = r'/home/mike/Desktop/TreeQN/BIB/TreeQN_Approach/treeQN/final_losses2.txt'\n",
    "output_file = 'output.json'\n",
    "\n",
    "# Initialize a list to store each block of data\n",
    "data = []\n",
    "\n",
    "# Initialize a temporary dictionary to store each set of values\n",
    "current_dict = {}\n",
    "\n",
    "# Read the text file line by line\n",
    "with open(input_file, 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if not line:  # If the line is empty, it's a new block of data\n",
    "            if current_dict:\n",
    "                data.append(current_dict)\n",
    "                current_dict = {}\n",
    "        else:\n",
    "            # Check if the line contains a colon\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "\n",
    "                # Convert numeric values to float or int\n",
    "                try:\n",
    "                    value = float(value) if '.' in value else int(value)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                \n",
    "                # Add the key-value pair to the current dictionary\n",
    "                current_dict[key] = value\n",
    "\n",
    "    # Append the last block of data\n",
    "    if current_dict:\n",
    "        data.append(current_dict)\n",
    "\n",
    "# Write the list of dictionaries to a JSON file\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json.dump(data, json_file, indent=4)\n",
    "\n",
    "print(\"Conversion complete! JSON saved to\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load JSON data (replace 'your_data.json' with your actual JSON file path)\n",
    "with open(r'/home/mike/Desktop/TreeQN/BIB/TreeQN_Approach/Notebooks/image_gridworld/tree_kfold_result.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Convert the JSON data to a DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Learning Rate'] = df['Learning Rate'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12.901, 13.257]    41\n",
       "(12.544, 12.901]    14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Learning Rate'] == 1e-5]['Final Validation Loss'].value_counts(bins=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(columns=['Gradient Clipping','Epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Discount Factor'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = 'Discount Factor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"Final Validation Loss\").head(15).reset_index(drop=True).to_csv(\"Top_15.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
