{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "from treeQN.treeqn_traj_sr import TreeQN\n",
    "import random\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start(size):\n",
    "    goal_point = (size//2, size//2)\n",
    "    start_point = -1\n",
    "    while True:\n",
    "        start_point = (random.randint(0,size), random.randint(0,size))\n",
    "        if goal_point != start_point:\n",
    "            break\n",
    "    return start_point, goal_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_policy(state,goal_point):\n",
    "    goal_x, goal_y = goal_point\n",
    "    x,y = state\n",
    "    x_right = goal_x > x # if goal is right\n",
    "    x_left = goal_x < x # if goal is left\n",
    "    y_up = goal_y > y # if goal is above\n",
    "    y_down = goal_y < y # if goal is below\n",
    "    possible_next_states = []\n",
    "    if x_right:\n",
    "        possible_next_states.append((x+1,y))\n",
    "    if x_left:\n",
    "        possible_next_states.append((x-1,y))\n",
    "    if y_up:\n",
    "        possible_next_states.append((x,y+1))\n",
    "    if y_down:\n",
    "        possible_next_states.append((x,y-1))\n",
    "    if len(possible_next_states) == 0:\n",
    "        return -1\n",
    "    return random.choice(possible_next_states)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_to_tensor(point,goal,size):\n",
    "    x,y = point\n",
    "    x_goal, y_goal = goal\n",
    "    tensor = torch.zeros(size+2,size+2)\n",
    "    scale = 1\n",
    "    tensor[x][y] = 1 * scale\n",
    "    tensor[x+1][y] = 1 * scale\n",
    "    tensor[x][y+1] = 1 * scale\n",
    "    tensor[x+1][y+1] = 1\n",
    "    tensor[x_goal][y_goal] = -1 * scale\n",
    "    tensor[x_goal+1][y_goal] = -1 * scale\n",
    "    tensor[x_goal][y_goal+1] = -1 * scale\n",
    "    tensor[x_goal+1][y_goal+1] = -1 * scale\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(size = 18,start_point = None, goal_point = None):\n",
    "    trajectory = []\n",
    "    if start_point is None:\n",
    "        start, goal = get_start(size)\n",
    "    else:\n",
    "        start, goal = start_point, goal_point\n",
    "    trajectory.append(start)\n",
    "    while start != goal:\n",
    "        start = hard_policy(start,goal)\n",
    "        trajectory.append(start)\n",
    "    if len(trajectory) != 5:\n",
    "        return get_trajectory(size)\n",
    "    return [point_to_tensor(p,goal,size).unsqueeze(0).unsqueeze(0) for p in trajectory]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_starting_points(size = 18):\n",
    "    start_points = []\n",
    "    for i in range(10000):\n",
    "        start_points.append(get_start(18)[0])\n",
    "    start_points = set(start_points)\n",
    "    goal_point = (size//2, size//2)\n",
    "    return start_points, goal_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, goal_point = max_starting_points()\n",
    "start_points = list(s)\n",
    "train_start_points = start_points[:len(start_points)//2]\n",
    "test_start_points = start_points[len(start_points)//2:]\n",
    "\n",
    "train_data = [get_trajectory(18,start_point,goal_point) for start_point in train_start_points]\n",
    "valid_data = [get_trajectory(18,start_point,goal_point) for start_point in test_start_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = torch.zeros(1, 1,20, 20).shape# minimum size #train_data[0][0].shape\n",
    "num_actions = 2\n",
    "tree_depth = 4\n",
    "embedding_dim = 4\n",
    "td_lambda = 1\n",
    "gamma = 1    #0.99\n",
    "model = TreeQN(input_shape=input_shape, num_actions=num_actions, tree_depth=tree_depth, embedding_dim=embedding_dim, td_lambda=td_lambda,gamma=gamma)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "#optimizer = RMSprop(model.parameters(), lr=1e-4,alpha =0.99, eps = 1e-5) | loss from treeqn paper\n",
    "# Collect all encoder and decoder parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, valid_data):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for t in valid_data:\n",
    "            decoded_values, all_policies = model(t[0])\n",
    "            decode_loss = F.mse_loss(decoded_values[0], t[0], reduction='sum')\n",
    "\n",
    "            first_policy = all_policies[0]\n",
    "            second_policy = all_policies[1].view(num_actions, -1)\n",
    "            third_policy = all_policies[2].view(num_actions, num_actions, -1)\n",
    "            fourth_policy = all_policies[3].view(num_actions, num_actions, num_actions, -1)\n",
    "\n",
    "            second_layer_probs = first_policy * second_policy\n",
    "            third_layer_probs = second_layer_probs * third_policy\n",
    "            fourth_layer_probs = third_layer_probs * fourth_policy\n",
    "\n",
    "            first = torch.flatten(first_policy).view(num_actions, 1, 1, 1)\n",
    "            second = torch.flatten(second_layer_probs).view(num_actions**2, 1, 1, 1)\n",
    "            third = torch.flatten(third_layer_probs).view(num_actions**3, 1, 1, 1)\n",
    "            fourth = torch.flatten(fourth_layer_probs).view(num_actions**4, 1, 1, 1)\n",
    "\n",
    "            first_loss = (F.mse_loss(decoded_values[1], t[1], reduction='none') * first).sum()\n",
    "            second_loss = (F.mse_loss(decoded_values[2], t[2], reduction='none') * second).sum()\n",
    "            third_loss = (F.mse_loss(decoded_values[3], t[3], reduction='none') * third).sum()\n",
    "            fourth_loss = (F.mse_loss(decoded_values[4], t[4], reduction='none') * fourth).sum()\n",
    "\n",
    "            l2w, l3w, l4w = 1, 1, 1\n",
    "            total_loss += first_loss + second_loss * l2w + third_loss * l3w + fourth_loss * l4w + decode_loss\n",
    "\n",
    "    return total_loss / len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_gradients(model):\n",
    "    gradients = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            gradients.append([name, param.grad.norm().item()])\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17769/1929968495.py:34: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([2, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  first_loss = (F.mse_loss(decoded_values[1], t[1], reduction='none') * first).sum()\n",
      "/tmp/ipykernel_17769/1929968495.py:35: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([4, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  second_loss = (F.mse_loss(decoded_values[2], t[2], reduction='none') * second).sum()\n",
      "/tmp/ipykernel_17769/1929968495.py:36: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([8, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  third_loss = (F.mse_loss(decoded_values[3], t[3], reduction='none') * third).sum()\n",
      "/tmp/ipykernel_17769/1929968495.py:37: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([16, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  fourth_loss = (F.mse_loss(decoded_values[4], t[4], reduction='none') * fourth).sum()\n",
      "/tmp/ipykernel_17769/1368165830.py:23: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([2, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  first_loss = (F.mse_loss(decoded_values[1], t[1], reduction='none') * first).sum()\n",
      "/tmp/ipykernel_17769/1368165830.py:24: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([4, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  second_loss = (F.mse_loss(decoded_values[2], t[2], reduction='none') * second).sum()\n",
      "/tmp/ipykernel_17769/1368165830.py:25: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([8, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  third_loss = (F.mse_loss(decoded_values[3], t[3], reduction='none') * third).sum()\n",
      "/tmp/ipykernel_17769/1368165830.py:26: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([16, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  fourth_loss = (F.mse_loss(decoded_values[4], t[4], reduction='none') * fourth).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 37.3575232717726, Train Raw Loss: 28.553085062238907, Validation Loss: 27.97074317932129\n",
      "Epoch 2, Train Loss: 20.647692426045737, Train Raw Loss: 15.359329748153687, Validation Loss: 15.841936111450195\n",
      "Epoch 3, Train Loss: 14.232015228271484, Train Raw Loss: 10.077617655860053, Validation Loss: 13.414351463317871\n",
      "Epoch 4, Train Loss: 13.064902273813884, Train Raw Loss: 9.128142494625516, Validation Loss: 12.954946517944336\n",
      "Epoch 5, Train Loss: 12.781253062354194, Train Raw Loss: 8.919904963175457, Validation Loss: 12.785567283630371\n",
      "Epoch 6, Train Loss: 12.660044214460585, Train Raw Loss: 8.837732474009195, Validation Loss: 12.714303016662598\n",
      "Epoch 7, Train Loss: 12.586459716161093, Train Raw Loss: 8.796415419048733, Validation Loss: 12.624241828918457\n",
      "Epoch 8, Train Loss: 12.519942347208659, Train Raw Loss: 8.767472653918796, Validation Loss: 12.570167541503906\n",
      "Epoch 9, Train Loss: 12.446549293729994, Train Raw Loss: 8.743793487548828, Validation Loss: 12.511021614074707\n",
      "Epoch 10, Train Loss: 12.3922875351376, Train Raw Loss: 8.723821380403306, Validation Loss: 12.467841148376465\n",
      "Epoch 11, Train Loss: 12.33978328704834, Train Raw Loss: 8.706027666727701, Validation Loss: 12.388776779174805\n",
      "Epoch 12, Train Loss: 12.274082008997599, Train Raw Loss: 8.680751344892714, Validation Loss: 12.336180686950684\n",
      "Epoch 13, Train Loss: 12.22254540125529, Train Raw Loss: 8.6614527437422, Validation Loss: 12.276162147521973\n",
      "Epoch 14, Train Loss: 12.162597280078463, Train Raw Loss: 8.637280639012655, Validation Loss: 12.208701133728027\n",
      "Epoch 15, Train Loss: 12.10821041531033, Train Raw Loss: 8.615308080779181, Validation Loss: 12.17233657836914\n",
      "Epoch 16, Train Loss: 12.050872770945231, Train Raw Loss: 8.585215690400865, Validation Loss: 12.132604598999023\n",
      "Epoch 17, Train Loss: 11.986730368932088, Train Raw Loss: 8.553980422019958, Validation Loss: 12.073246002197266\n",
      "Epoch 18, Train Loss: 11.91672469774882, Train Raw Loss: 8.518892918692694, Validation Loss: 11.994340896606445\n",
      "Epoch 19, Train Loss: 11.850723367267184, Train Raw Loss: 8.483426364262899, Validation Loss: 11.967416763305664\n",
      "Epoch 20, Train Loss: 11.790363783306546, Train Raw Loss: 8.453454173935784, Validation Loss: 11.90345287322998\n",
      "Epoch 21, Train Loss: 11.728320524427627, Train Raw Loss: 8.417197871208192, Validation Loss: 11.837145805358887\n",
      "Epoch 22, Train Loss: 11.667475287119547, Train Raw Loss: 8.382856138547261, Validation Loss: 11.770889282226562\n",
      "Epoch 23, Train Loss: 11.59415209558275, Train Raw Loss: 8.336595956484476, Validation Loss: 11.729842185974121\n",
      "Epoch 24, Train Loss: 11.534290928310819, Train Raw Loss: 8.301272855864632, Validation Loss: 11.691617965698242\n",
      "Epoch 25, Train Loss: 11.469023015764025, Train Raw Loss: 8.261287628279792, Validation Loss: 11.633828163146973\n",
      "Epoch 26, Train Loss: 11.397126833597818, Train Raw Loss: 8.216445891062419, Validation Loss: 11.577549934387207\n",
      "Epoch 27, Train Loss: 11.314786868625218, Train Raw Loss: 8.16702983909183, Validation Loss: 11.53354263305664\n",
      "Epoch 28, Train Loss: 11.242465469572279, Train Raw Loss: 8.125568678643969, Validation Loss: 11.446012496948242\n",
      "Epoch 29, Train Loss: 11.163095712661743, Train Raw Loss: 8.077679371833801, Validation Loss: 11.377385139465332\n",
      "Epoch 30, Train Loss: 11.0769681930542, Train Raw Loss: 8.01820761097802, Validation Loss: 11.346796035766602\n",
      "Epoch 31, Train Loss: 10.997531345155505, Train Raw Loss: 7.964643218782213, Validation Loss: 11.291776657104492\n",
      "Epoch 32, Train Loss: 10.916410083240933, Train Raw Loss: 7.91164850393931, Validation Loss: 11.243574142456055\n",
      "Epoch 33, Train Loss: 10.830703263812595, Train Raw Loss: 7.847302028867934, Validation Loss: 11.173306465148926\n",
      "Epoch 34, Train Loss: 10.75926805867089, Train Raw Loss: 7.798288485738966, Validation Loss: 11.106727600097656\n",
      "Epoch 35, Train Loss: 10.681501250796847, Train Raw Loss: 7.735818775494893, Validation Loss: 11.049714088439941\n",
      "Epoch 36, Train Loss: 10.611478784349229, Train Raw Loss: 7.679699616962009, Validation Loss: 11.02115249633789\n",
      "Epoch 37, Train Loss: 10.549970489078097, Train Raw Loss: 7.627252258194817, Validation Loss: 10.993085861206055\n",
      "Epoch 38, Train Loss: 10.494615766737196, Train Raw Loss: 7.586583333545261, Validation Loss: 10.916993141174316\n",
      "Epoch 39, Train Loss: 10.416228106286791, Train Raw Loss: 7.522989604208204, Validation Loss: 10.922781944274902\n",
      "Epoch 40, Train Loss: 10.366010051303439, Train Raw Loss: 7.483184107144674, Validation Loss: 10.87318229675293\n",
      "Epoch 41, Train Loss: 10.29411907725864, Train Raw Loss: 7.423417425155639, Validation Loss: 10.846099853515625\n",
      "Epoch 42, Train Loss: 10.253288862440321, Train Raw Loss: 7.390630271699694, Validation Loss: 10.798152923583984\n",
      "Epoch 43, Train Loss: 10.190116108788384, Train Raw Loss: 7.341180262300703, Validation Loss: 10.799845695495605\n",
      "Epoch 44, Train Loss: 10.14272984398736, Train Raw Loss: 7.307161774900225, Validation Loss: 10.744101524353027\n",
      "Epoch 45, Train Loss: 10.111143453915913, Train Raw Loss: 7.277924491299523, Validation Loss: 10.69236946105957\n",
      "Epoch 46, Train Loss: 10.048892161581252, Train Raw Loss: 7.231265604496002, Validation Loss: 10.669617652893066\n",
      "Epoch 47, Train Loss: 10.017335637410481, Train Raw Loss: 7.207096107800802, Validation Loss: 10.661110877990723\n",
      "Epoch 48, Train Loss: 9.97389448483785, Train Raw Loss: 7.168558496899075, Validation Loss: 10.647265434265137\n",
      "Epoch 49, Train Loss: 9.934477191501193, Train Raw Loss: 7.13329635726081, Validation Loss: 10.652399063110352\n",
      "Epoch 50, Train Loss: 9.90455154577891, Train Raw Loss: 7.117396210299598, Validation Loss: 10.609383583068848\n",
      "Epoch 51, Train Loss: 9.876176193025376, Train Raw Loss: 7.089960489008162, Validation Loss: 10.574151039123535\n",
      "Epoch 52, Train Loss: 9.839183346430461, Train Raw Loss: 7.0536323149998985, Validation Loss: 10.553872108459473\n",
      "Epoch 53, Train Loss: 9.790895266003078, Train Raw Loss: 7.0184251930978565, Validation Loss: 10.551338195800781\n",
      "Epoch 54, Train Loss: 9.773012404971652, Train Raw Loss: 7.005894107288785, Validation Loss: 10.535093307495117\n",
      "Epoch 55, Train Loss: 9.740694811609057, Train Raw Loss: 6.979394450452593, Validation Loss: 10.519875526428223\n",
      "Epoch 56, Train Loss: 9.708463591999477, Train Raw Loss: 6.953842194875081, Validation Loss: 10.529131889343262\n",
      "Epoch 57, Train Loss: 9.674802223841349, Train Raw Loss: 6.924637916353014, Validation Loss: 10.479659080505371\n",
      "Epoch 58, Train Loss: 9.657672293980916, Train Raw Loss: 6.913897920979394, Validation Loss: 10.454129219055176\n",
      "Epoch 59, Train Loss: 9.617041807704501, Train Raw Loss: 6.884912604755826, Validation Loss: 10.433772087097168\n",
      "Epoch 60, Train Loss: 9.59874152607388, Train Raw Loss: 6.867083115047879, Validation Loss: 10.405550003051758\n",
      "Epoch 61, Train Loss: 9.552683390511406, Train Raw Loss: 6.824139357937707, Validation Loss: 10.399909019470215\n",
      "Epoch 62, Train Loss: 9.540973096423679, Train Raw Loss: 6.821219905217489, Validation Loss: 10.395607948303223\n",
      "Epoch 63, Train Loss: 9.515245763460795, Train Raw Loss: 6.801954861481985, Validation Loss: 10.396584510803223\n",
      "Epoch 64, Train Loss: 9.478706546624501, Train Raw Loss: 6.774508957068125, Validation Loss: 10.396607398986816\n",
      "Epoch 65, Train Loss: 9.471270871162414, Train Raw Loss: 6.767377250724369, Validation Loss: 10.366384506225586\n",
      "Epoch 66, Train Loss: 9.441465361913044, Train Raw Loss: 6.752372099293603, Validation Loss: 10.307526588439941\n",
      "Epoch 67, Train Loss: 9.410589819484287, Train Raw Loss: 6.722675457265642, Validation Loss: 10.29863452911377\n",
      "Epoch 68, Train Loss: 9.353551334804958, Train Raw Loss: 6.6883094072341915, Validation Loss: 10.207751274108887\n",
      "Epoch 69, Train Loss: 9.310320247544183, Train Raw Loss: 6.659082941214243, Validation Loss: 10.159032821655273\n",
      "Epoch 70, Train Loss: 9.283700671460894, Train Raw Loss: 6.64452572133806, Validation Loss: 10.114047050476074\n",
      "Epoch 71, Train Loss: 9.225152809090085, Train Raw Loss: 6.602671106656392, Validation Loss: 10.104682922363281\n",
      "Epoch 72, Train Loss: 9.192516683207618, Train Raw Loss: 6.574308353000217, Validation Loss: 10.063730239868164\n",
      "Epoch 73, Train Loss: 9.162518841690487, Train Raw Loss: 6.553392493724823, Validation Loss: 10.051445960998535\n",
      "Epoch 74, Train Loss: 9.137767758634356, Train Raw Loss: 6.5335916419823965, Validation Loss: 10.020532608032227\n",
      "Epoch 75, Train Loss: 9.103866990407308, Train Raw Loss: 6.506778620349037, Validation Loss: 10.001038551330566\n",
      "Epoch 76, Train Loss: 9.078892407152388, Train Raw Loss: 6.490545084079106, Validation Loss: 9.975997924804688\n",
      "Epoch 77, Train Loss: 9.065193793508742, Train Raw Loss: 6.473561278316709, Validation Loss: 9.94222354888916\n",
      "Epoch 78, Train Loss: 9.033281570010715, Train Raw Loss: 6.450739683045281, Validation Loss: 9.931838035583496\n",
      "Epoch 79, Train Loss: 9.014771517117818, Train Raw Loss: 6.430897193484836, Validation Loss: 9.913165092468262\n",
      "Epoch 80, Train Loss: 8.987471977869669, Train Raw Loss: 6.420552643140157, Validation Loss: 9.88440990447998\n",
      "Epoch 81, Train Loss: 8.963000479009416, Train Raw Loss: 6.384362156523599, Validation Loss: 9.84693717956543\n",
      "Epoch 82, Train Loss: 8.928782671027713, Train Raw Loss: 6.370864503913456, Validation Loss: 9.849321365356445\n",
      "Epoch 83, Train Loss: 8.925314875443776, Train Raw Loss: 6.358801592058605, Validation Loss: 9.780877113342285\n",
      "Epoch 84, Train Loss: 8.899795191817814, Train Raw Loss: 6.338811400863859, Validation Loss: 9.774563789367676\n",
      "Epoch 85, Train Loss: 8.873079404566024, Train Raw Loss: 6.305199257532755, Validation Loss: 9.784527778625488\n",
      "Epoch 86, Train Loss: 8.844249324003856, Train Raw Loss: 6.299258697695202, Validation Loss: 9.74158763885498\n",
      "Epoch 87, Train Loss: 8.833416004975637, Train Raw Loss: 6.2903612395127615, Validation Loss: 9.709683418273926\n",
      "Epoch 88, Train Loss: 8.803408238622877, Train Raw Loss: 6.261156346400579, Validation Loss: 9.689018249511719\n",
      "Epoch 89, Train Loss: 8.788331372208065, Train Raw Loss: 6.252257076899211, Validation Loss: 9.693825721740723\n",
      "Epoch 90, Train Loss: 8.776770088407728, Train Raw Loss: 6.241138619846768, Validation Loss: 9.687186241149902\n",
      "Epoch 91, Train Loss: 8.751850762632158, Train Raw Loss: 6.218170901139577, Validation Loss: 9.641226768493652\n",
      "Epoch 92, Train Loss: 8.735935495959389, Train Raw Loss: 6.206831206215752, Validation Loss: 9.632509231567383\n",
      "Epoch 93, Train Loss: 8.724062453375922, Train Raw Loss: 6.195572007364697, Validation Loss: 9.596144676208496\n",
      "Epoch 94, Train Loss: 8.69059434334437, Train Raw Loss: 6.16974878973431, Validation Loss: 9.58250904083252\n",
      "Epoch 95, Train Loss: 8.671344627274408, Train Raw Loss: 6.154920081959831, Validation Loss: 9.583723068237305\n",
      "Epoch 96, Train Loss: 8.670788306660123, Train Raw Loss: 6.148551819721858, Validation Loss: 9.570181846618652\n",
      "Epoch 97, Train Loss: 8.63517316977183, Train Raw Loss: 6.126332112153372, Validation Loss: 9.566550254821777\n",
      "Epoch 98, Train Loss: 8.630837547779084, Train Raw Loss: 6.117811869250404, Validation Loss: 9.566363334655762\n",
      "Epoch 99, Train Loss: 8.611455368995667, Train Raw Loss: 6.100598129298952, Validation Loss: 9.523173332214355\n",
      "Epoch 100, Train Loss: 8.594194318188562, Train Raw Loss: 6.091933399438858, Validation Loss: 9.496654510498047\n",
      "Epoch 101, Train Loss: 8.575882324907514, Train Raw Loss: 6.073879282342063, Validation Loss: 9.487299919128418\n",
      "Epoch 102, Train Loss: 8.564365621407827, Train Raw Loss: 6.056983906030655, Validation Loss: 9.473628044128418\n",
      "Epoch 103, Train Loss: 8.542412014802297, Train Raw Loss: 6.043666288587782, Validation Loss: 9.475847244262695\n",
      "Epoch 104, Train Loss: 8.541108649306826, Train Raw Loss: 6.0459212799866995, Validation Loss: 9.416938781738281\n",
      "Epoch 105, Train Loss: 8.5129285203086, Train Raw Loss: 6.019852163394292, Validation Loss: 9.441414833068848\n",
      "Epoch 106, Train Loss: 8.501178578535717, Train Raw Loss: 6.017016353872087, Validation Loss: 9.434478759765625\n",
      "Epoch 107, Train Loss: 8.482667028903961, Train Raw Loss: 5.993862152099609, Validation Loss: 9.392644882202148\n",
      "Epoch 108, Train Loss: 8.469004436333973, Train Raw Loss: 5.990738537576464, Validation Loss: 9.388068199157715\n",
      "Epoch 109, Train Loss: 8.458151739173465, Train Raw Loss: 5.974477191766103, Validation Loss: 9.385733604431152\n",
      "Epoch 110, Train Loss: 8.432378285461002, Train Raw Loss: 5.960911369986004, Validation Loss: 9.355911254882812\n",
      "Epoch 111, Train Loss: 8.413847540484534, Train Raw Loss: 5.9370200667116375, Validation Loss: 9.373978614807129\n",
      "Epoch 112, Train Loss: 8.41528889603085, Train Raw Loss: 5.949876830975215, Validation Loss: 9.325104713439941\n",
      "Epoch 113, Train Loss: 8.383350813388825, Train Raw Loss: 5.914059550232357, Validation Loss: 9.316740036010742\n",
      "Epoch 114, Train Loss: 8.378692481252882, Train Raw Loss: 5.90986812710762, Validation Loss: 9.31115436553955\n",
      "Epoch 115, Train Loss: 8.353615118397606, Train Raw Loss: 5.89913638830185, Validation Loss: 9.293638229370117\n",
      "Epoch 116, Train Loss: 8.339997902181413, Train Raw Loss: 5.886369215779834, Validation Loss: 9.263152122497559\n",
      "Epoch 117, Train Loss: 8.315831327438355, Train Raw Loss: 5.86722283826934, Validation Loss: 9.262112617492676\n",
      "Epoch 118, Train Loss: 8.30775420665741, Train Raw Loss: 5.864048310120901, Validation Loss: 9.240407943725586\n",
      "Epoch 119, Train Loss: 8.295792815420363, Train Raw Loss: 5.8518038239743975, Validation Loss: 9.249662399291992\n",
      "Epoch 120, Train Loss: 8.291674207316504, Train Raw Loss: 5.848627780543433, Validation Loss: 9.202439308166504\n",
      "Epoch 121, Train Loss: 8.274265103869968, Train Raw Loss: 5.835274891720878, Validation Loss: 9.21683406829834\n",
      "Epoch 122, Train Loss: 8.246229602230919, Train Raw Loss: 5.815333344539007, Validation Loss: 9.207571029663086\n",
      "Epoch 123, Train Loss: 8.24677300983005, Train Raw Loss: 5.815295409493976, Validation Loss: 9.185715675354004\n",
      "Epoch 124, Train Loss: 8.218229095141092, Train Raw Loss: 5.801624374257194, Validation Loss: 9.206768035888672\n",
      "Epoch 125, Train Loss: 8.22640742858251, Train Raw Loss: 5.7942708008819155, Validation Loss: 9.139019966125488\n",
      "Epoch 126, Train Loss: 8.19622902340359, Train Raw Loss: 5.772414946556092, Validation Loss: 9.123865127563477\n",
      "Epoch 127, Train Loss: 8.185307742489709, Train Raw Loss: 5.765114882919524, Validation Loss: 9.107330322265625\n",
      "Epoch 128, Train Loss: 8.158549059761896, Train Raw Loss: 5.751483183436924, Validation Loss: 9.120195388793945\n",
      "Epoch 129, Train Loss: 8.155992072158389, Train Raw Loss: 5.7465833657317695, Validation Loss: 9.10627555847168\n",
      "Epoch 130, Train Loss: 8.144778745704228, Train Raw Loss: 5.736748908625708, Validation Loss: 9.09325122833252\n",
      "Epoch 131, Train Loss: 8.124387911955516, Train Raw Loss: 5.715387927823596, Validation Loss: 9.082868576049805\n",
      "Epoch 132, Train Loss: 8.098439563645258, Train Raw Loss: 5.708673236105177, Validation Loss: 9.042343139648438\n",
      "Epoch 133, Train Loss: 8.085866358545092, Train Raw Loss: 5.694584163692262, Validation Loss: 9.035648345947266\n",
      "Epoch 134, Train Loss: 8.082076089911991, Train Raw Loss: 5.690151952372657, Validation Loss: 9.00499439239502\n",
      "Epoch 135, Train Loss: 8.058423422442543, Train Raw Loss: 5.66957181096077, Validation Loss: 9.01576042175293\n",
      "Epoch 136, Train Loss: 8.03490835428238, Train Raw Loss: 5.660425351725684, Validation Loss: 9.017675399780273\n",
      "Epoch 137, Train Loss: 8.016716138521831, Train Raw Loss: 5.641626854075326, Validation Loss: 9.02649974822998\n",
      "Epoch 138, Train Loss: 8.034622253312005, Train Raw Loss: 5.653157758050495, Validation Loss: 8.989444732666016\n",
      "Epoch 139, Train Loss: 8.00904278755188, Train Raw Loss: 5.636957065264384, Validation Loss: 8.935440063476562\n",
      "Epoch 140, Train Loss: 7.9836149083243475, Train Raw Loss: 5.615613633394242, Validation Loss: 8.949499130249023\n",
      "Epoch 141, Train Loss: 7.996696893374125, Train Raw Loss: 5.6227454152372145, Validation Loss: 8.919599533081055\n",
      "Epoch 142, Train Loss: 7.963009680642022, Train Raw Loss: 5.607044783565733, Validation Loss: 8.91751480102539\n",
      "Epoch 143, Train Loss: 7.95578316450119, Train Raw Loss: 5.592717216412226, Validation Loss: 8.90854263305664\n",
      "Epoch 144, Train Loss: 7.934598461786906, Train Raw Loss: 5.5872228920459746, Validation Loss: 8.881710052490234\n",
      "Epoch 145, Train Loss: 7.91891901757982, Train Raw Loss: 5.567299194468392, Validation Loss: 8.896275520324707\n",
      "Epoch 146, Train Loss: 7.903544145160251, Train Raw Loss: 5.556747170951631, Validation Loss: 8.87594223022461\n",
      "Epoch 147, Train Loss: 7.902367271317376, Train Raw Loss: 5.558926668432024, Validation Loss: 8.86929702758789\n",
      "Epoch 148, Train Loss: 7.882299811310238, Train Raw Loss: 5.543171922365825, Validation Loss: 8.836331367492676\n",
      "Epoch 149, Train Loss: 7.861690288119846, Train Raw Loss: 5.529199269082811, Validation Loss: 8.842737197875977\n",
      "Epoch 150, Train Loss: 7.8554776125484045, Train Raw Loss: 5.52552042471038, Validation Loss: 8.815077781677246\n",
      "Epoch 151, Train Loss: 7.834592220518324, Train Raw Loss: 5.5136321908897825, Validation Loss: 8.803167343139648\n",
      "Epoch 152, Train Loss: 7.830755984783172, Train Raw Loss: 5.503358430332607, Validation Loss: 8.801663398742676\n",
      "Epoch 153, Train Loss: 7.811106850041283, Train Raw Loss: 5.48256719244851, Validation Loss: 8.769781112670898\n",
      "Epoch 154, Train Loss: 7.804216063022613, Train Raw Loss: 5.489520246452756, Validation Loss: 8.761311531066895\n",
      "Epoch 155, Train Loss: 7.788662371370528, Train Raw Loss: 5.47655082543691, Validation Loss: 8.786096572875977\n",
      "Epoch 156, Train Loss: 7.789960606892904, Train Raw Loss: 5.474032170905007, Validation Loss: 8.75964641571045\n",
      "Epoch 157, Train Loss: 7.759516015317705, Train Raw Loss: 5.452052620384428, Validation Loss: 8.726998329162598\n",
      "Epoch 158, Train Loss: 7.750651102595859, Train Raw Loss: 5.447903543710709, Validation Loss: 8.723237037658691\n",
      "Epoch 159, Train Loss: 7.732664856645796, Train Raw Loss: 5.433941015270022, Validation Loss: 8.767675399780273\n",
      "Epoch 160, Train Loss: 7.72554100089603, Train Raw Loss: 5.43002961675326, Validation Loss: 8.67249584197998\n",
      "Epoch 161, Train Loss: 7.720719096395705, Train Raw Loss: 5.421047522624334, Validation Loss: 8.665000915527344\n",
      "Epoch 162, Train Loss: 7.708074590894911, Train Raw Loss: 5.412795389360852, Validation Loss: 8.665563583374023\n",
      "Epoch 163, Train Loss: 7.681074767642551, Train Raw Loss: 5.395534070332845, Validation Loss: 8.664115905761719\n",
      "Epoch 164, Train Loss: 7.6857970476150514, Train Raw Loss: 5.39407371017668, Validation Loss: 8.642827987670898\n",
      "Epoch 165, Train Loss: 7.666589930322435, Train Raw Loss: 5.381760491927465, Validation Loss: 8.617796897888184\n",
      "Epoch 166, Train Loss: 7.647566988070806, Train Raw Loss: 5.373703618182076, Validation Loss: 8.637661933898926\n",
      "Epoch 167, Train Loss: 7.640114919344584, Train Raw Loss: 5.365832678476969, Validation Loss: 8.63381576538086\n",
      "Epoch 168, Train Loss: 7.637634736961789, Train Raw Loss: 5.354820934931437, Validation Loss: 8.598074913024902\n",
      "Epoch 169, Train Loss: 7.610047953658634, Train Raw Loss: 5.34806718826294, Validation Loss: 8.596511840820312\n",
      "Epoch 170, Train Loss: 7.597904929849837, Train Raw Loss: 5.335856734381782, Validation Loss: 8.57470703125\n",
      "Epoch 171, Train Loss: 7.601250372992622, Train Raw Loss: 5.335122856828901, Validation Loss: 8.573930740356445\n",
      "Epoch 172, Train Loss: 7.579222251309289, Train Raw Loss: 5.321666786405776, Validation Loss: 8.530879020690918\n",
      "Epoch 173, Train Loss: 7.560744980971019, Train Raw Loss: 5.30941275689337, Validation Loss: 8.580799102783203\n",
      "Epoch 174, Train Loss: 7.549169447686937, Train Raw Loss: 5.300144756502576, Validation Loss: 8.533719062805176\n",
      "Epoch 175, Train Loss: 7.536127091778649, Train Raw Loss: 5.290590782960256, Validation Loss: 8.496857643127441\n",
      "Epoch 176, Train Loss: 7.52739733060201, Train Raw Loss: 5.282198239035076, Validation Loss: 8.482268333435059\n",
      "Epoch 177, Train Loss: 7.528055991729101, Train Raw Loss: 5.28064071337382, Validation Loss: 8.504246711730957\n",
      "Epoch 178, Train Loss: 7.516193049483829, Train Raw Loss: 5.281700440247854, Validation Loss: 8.498679161071777\n",
      "Epoch 179, Train Loss: 7.511838518910938, Train Raw Loss: 5.267370343870587, Validation Loss: 8.477546691894531\n",
      "Epoch 180, Train Loss: 7.505694737699296, Train Raw Loss: 5.266399307383431, Validation Loss: 8.45820140838623\n",
      "Epoch 181, Train Loss: 7.489028678337733, Train Raw Loss: 5.254284221596188, Validation Loss: 8.448375701904297\n",
      "Epoch 182, Train Loss: 7.461061322026783, Train Raw Loss: 5.2374312579631805, Validation Loss: 8.445735931396484\n",
      "Epoch 183, Train Loss: 7.460034426053365, Train Raw Loss: 5.233277773857116, Validation Loss: 8.444068908691406\n",
      "Epoch 184, Train Loss: 7.447193821933534, Train Raw Loss: 5.228844601578182, Validation Loss: 8.43492317199707\n",
      "Epoch 185, Train Loss: 7.435297287172741, Train Raw Loss: 5.214204923311869, Validation Loss: 8.414511680603027\n",
      "Epoch 186, Train Loss: 7.417829220162497, Train Raw Loss: 5.207476819886102, Validation Loss: 8.423102378845215\n",
      "Epoch 187, Train Loss: 7.419107480181588, Train Raw Loss: 5.208493615521325, Validation Loss: 8.397156715393066\n",
      "Epoch 188, Train Loss: 7.404353773593902, Train Raw Loss: 5.1976487225956385, Validation Loss: 8.37785816192627\n",
      "Epoch 189, Train Loss: 7.381745848390791, Train Raw Loss: 5.185751353700955, Validation Loss: 8.379652976989746\n",
      "Epoch 190, Train Loss: 7.390693172481325, Train Raw Loss: 5.185533224874073, Validation Loss: 8.380552291870117\n",
      "Epoch 191, Train Loss: 7.382317590713501, Train Raw Loss: 5.183181851440006, Validation Loss: 8.345542907714844\n",
      "Epoch 192, Train Loss: 7.354038591517343, Train Raw Loss: 5.162779508034388, Validation Loss: 8.366601943969727\n",
      "Epoch 193, Train Loss: 7.366610418425666, Train Raw Loss: 5.170378699567583, Validation Loss: 8.34334945678711\n",
      "Epoch 194, Train Loss: 7.335709632767571, Train Raw Loss: 5.152695518069797, Validation Loss: 8.358031272888184\n",
      "Epoch 195, Train Loss: 7.337953225109312, Train Raw Loss: 5.153790380557378, Validation Loss: 8.329172134399414\n",
      "Epoch 196, Train Loss: 7.337916638453802, Train Raw Loss: 5.154711349474059, Validation Loss: 8.325438499450684\n",
      "Epoch 197, Train Loss: 7.3076517959435785, Train Raw Loss: 5.132803629835447, Validation Loss: 8.303594589233398\n",
      "Epoch 198, Train Loss: 7.294330626063877, Train Raw Loss: 5.124970119860437, Validation Loss: 8.315913200378418\n",
      "Epoch 199, Train Loss: 7.300659428040187, Train Raw Loss: 5.1285971817043094, Validation Loss: 8.292404174804688\n",
      "Epoch 200, Train Loss: 7.290750990973579, Train Raw Loss: 5.117107440365685, Validation Loss: 8.273459434509277\n",
      "Epoch 201, Train Loss: 7.280154275231891, Train Raw Loss: 5.117549190256331, Validation Loss: 8.281326293945312\n",
      "Epoch 202, Train Loss: 7.272377857234743, Train Raw Loss: 5.114033582144313, Validation Loss: 8.23936653137207\n",
      "Epoch 203, Train Loss: 7.249134671025806, Train Raw Loss: 5.10152468085289, Validation Loss: 8.252676010131836\n",
      "Epoch 204, Train Loss: 7.248909956879086, Train Raw Loss: 5.096364692515797, Validation Loss: 8.230384826660156\n",
      "Epoch 205, Train Loss: 7.238246276643541, Train Raw Loss: 5.087216257717874, Validation Loss: 8.222574234008789\n",
      "Epoch 206, Train Loss: 7.229495306809743, Train Raw Loss: 5.0852601435449385, Validation Loss: 8.258399963378906\n",
      "Epoch 207, Train Loss: 7.210235248009364, Train Raw Loss: 5.071914721528689, Validation Loss: 8.193694114685059\n",
      "Epoch 208, Train Loss: 7.214732101228502, Train Raw Loss: 5.066833654377196, Validation Loss: 8.190305709838867\n",
      "Epoch 209, Train Loss: 7.191893464326858, Train Raw Loss: 5.053056614266501, Validation Loss: 8.196184158325195\n",
      "Epoch 210, Train Loss: 7.205980935361651, Train Raw Loss: 5.067980106009378, Validation Loss: 8.201858520507812\n",
      "Epoch 211, Train Loss: 7.182158603933122, Train Raw Loss: 5.049939920173751, Validation Loss: 8.204021453857422\n",
      "Epoch 212, Train Loss: 7.181516902314292, Train Raw Loss: 5.058822623557514, Validation Loss: 8.18077564239502\n",
      "Epoch 213, Train Loss: 7.170003273752001, Train Raw Loss: 5.050764973627197, Validation Loss: 8.183484077453613\n",
      "Epoch 214, Train Loss: 7.154432340463003, Train Raw Loss: 5.029349972804387, Validation Loss: 8.200547218322754\n",
      "Epoch 215, Train Loss: 7.149881387419171, Train Raw Loss: 5.0310182905859415, Validation Loss: 8.154169082641602\n",
      "Epoch 216, Train Loss: 7.149107058842977, Train Raw Loss: 5.028730011317465, Validation Loss: 8.128583908081055\n",
      "Epoch 217, Train Loss: 7.115612083011204, Train Raw Loss: 5.010792756080628, Validation Loss: 8.127313613891602\n",
      "Epoch 218, Train Loss: 7.120432856347826, Train Raw Loss: 5.005817185507881, Validation Loss: 8.119111061096191\n",
      "Epoch 219, Train Loss: 7.125903805759218, Train Raw Loss: 5.015698213378588, Validation Loss: 8.093860626220703\n",
      "Epoch 220, Train Loss: 7.0865536385112335, Train Raw Loss: 4.99723851316505, Validation Loss: 8.082263946533203\n",
      "Epoch 221, Train Loss: 7.094969228241179, Train Raw Loss: 4.998459895783, Validation Loss: 8.0782470703125\n",
      "Epoch 222, Train Loss: 7.079539735449685, Train Raw Loss: 4.989421424600813, Validation Loss: 8.069527626037598\n",
      "Epoch 223, Train Loss: 7.070705462164349, Train Raw Loss: 4.97864148815473, Validation Loss: 8.067444801330566\n",
      "Epoch 224, Train Loss: 7.065404330359565, Train Raw Loss: 4.980820565091239, Validation Loss: 8.058469772338867\n",
      "Epoch 225, Train Loss: 7.057351421647602, Train Raw Loss: 4.974529322319561, Validation Loss: 8.05390453338623\n",
      "Epoch 226, Train Loss: 7.052218475606707, Train Raw Loss: 4.970306301448081, Validation Loss: 8.045291900634766\n",
      "Epoch 227, Train Loss: 7.028992992639542, Train Raw Loss: 4.9530819488896265, Validation Loss: 8.05547046661377\n",
      "Epoch 228, Train Loss: 7.032650907834371, Train Raw Loss: 4.960508905847868, Validation Loss: 8.012029647827148\n",
      "Epoch 229, Train Loss: 7.035848898357815, Train Raw Loss: 4.955162857638465, Validation Loss: 8.042495727539062\n",
      "Epoch 230, Train Loss: 7.020279319418801, Train Raw Loss: 4.944962122374111, Validation Loss: 8.00655460357666\n",
      "Epoch 231, Train Loss: 7.004190488656362, Train Raw Loss: 4.940135419037607, Validation Loss: 7.974879264831543\n",
      "Epoch 232, Train Loss: 7.005939559141795, Train Raw Loss: 4.942768689990044, Validation Loss: 7.989616394042969\n",
      "Epoch 233, Train Loss: 6.993453945053949, Train Raw Loss: 4.934058011240429, Validation Loss: 7.985168933868408\n",
      "Epoch 234, Train Loss: 6.970408435000314, Train Raw Loss: 4.917879076798757, Validation Loss: 8.023646354675293\n",
      "Epoch 235, Train Loss: 6.975504652659098, Train Raw Loss: 4.923595543702444, Validation Loss: 7.964718341827393\n",
      "Epoch 236, Train Loss: 6.965283727645874, Train Raw Loss: 4.9208775728940966, Validation Loss: 7.9522576332092285\n",
      "Epoch 237, Train Loss: 6.940686262978447, Train Raw Loss: 4.897976718677414, Validation Loss: 7.963503837585449\n",
      "Epoch 238, Train Loss: 6.943639585044649, Train Raw Loss: 4.890746695134375, Validation Loss: 7.965221405029297\n",
      "Epoch 239, Train Loss: 6.942845864428414, Train Raw Loss: 4.905027040508059, Validation Loss: 7.948462009429932\n",
      "Epoch 240, Train Loss: 6.931348530451457, Train Raw Loss: 4.892729800608423, Validation Loss: 7.987009525299072\n",
      "Epoch 241, Train Loss: 6.911314313279258, Train Raw Loss: 4.88741279774242, Validation Loss: 7.930330276489258\n",
      "Epoch 242, Train Loss: 6.9072884023189545, Train Raw Loss: 4.87958962255054, Validation Loss: 7.937112808227539\n",
      "Epoch 243, Train Loss: 6.897289680110084, Train Raw Loss: 4.8696727481153275, Validation Loss: 7.910222053527832\n",
      "Epoch 244, Train Loss: 6.904758385154936, Train Raw Loss: 4.878033142288526, Validation Loss: 7.89495849609375\n",
      "Epoch 245, Train Loss: 6.887109712759654, Train Raw Loss: 4.868368800481161, Validation Loss: 7.907044887542725\n",
      "Epoch 246, Train Loss: 6.869627361165152, Train Raw Loss: 4.854953951305813, Validation Loss: 7.880102634429932\n",
      "Epoch 247, Train Loss: 6.861100843879911, Train Raw Loss: 4.850609222054482, Validation Loss: 7.874642372131348\n",
      "Epoch 248, Train Loss: 6.858392486307356, Train Raw Loss: 4.854392501049571, Validation Loss: 7.851705551147461\n",
      "Epoch 249, Train Loss: 6.835265024503072, Train Raw Loss: 4.8399409502744675, Validation Loss: 7.862166404724121\n",
      "Epoch 250, Train Loss: 6.8513041979736755, Train Raw Loss: 4.840056666400698, Validation Loss: 7.844938278198242\n",
      "Epoch 251, Train Loss: 6.822227247556051, Train Raw Loss: 4.824568581249979, Validation Loss: 7.849747180938721\n",
      "Epoch 252, Train Loss: 6.804195925262239, Train Raw Loss: 4.818202653858397, Validation Loss: 7.873226642608643\n",
      "Epoch 253, Train Loss: 6.827785255511602, Train Raw Loss: 4.828137032190958, Validation Loss: 7.826681137084961\n",
      "Epoch 254, Train Loss: 6.801224397950702, Train Raw Loss: 4.807006587584813, Validation Loss: 7.814804553985596\n",
      "Epoch 255, Train Loss: 6.782377701997757, Train Raw Loss: 4.795742215050591, Validation Loss: 7.798854351043701\n",
      "Epoch 256, Train Loss: 6.773702693647809, Train Raw Loss: 4.795153859588835, Validation Loss: 7.777541160583496\n",
      "Epoch 257, Train Loss: 6.778546676370833, Train Raw Loss: 4.802817485729853, Validation Loss: 7.797770977020264\n",
      "Epoch 258, Train Loss: 6.77476644648446, Train Raw Loss: 4.799504691693518, Validation Loss: 7.760592460632324\n",
      "Epoch 259, Train Loss: 6.744218693176905, Train Raw Loss: 4.778805934058296, Validation Loss: 7.792640686035156\n",
      "Epoch 260, Train Loss: 6.757444570461909, Train Raw Loss: 4.786336200435956, Validation Loss: 7.769525051116943\n",
      "Epoch 261, Train Loss: 6.759333549605476, Train Raw Loss: 4.788105962342686, Validation Loss: 7.76960563659668\n",
      "Epoch 262, Train Loss: 6.7377354231145645, Train Raw Loss: 4.7778981645902, Validation Loss: 7.788265705108643\n",
      "Epoch 263, Train Loss: 6.736320247252782, Train Raw Loss: 4.770993493000666, Validation Loss: 7.743319988250732\n",
      "Epoch 264, Train Loss: 6.716514897346497, Train Raw Loss: 4.761651496423616, Validation Loss: 7.739606857299805\n",
      "Epoch 265, Train Loss: 6.714106457100974, Train Raw Loss: 4.767575457692146, Validation Loss: 7.7479071617126465\n",
      "Epoch 266, Train Loss: 6.697516361210082, Train Raw Loss: 4.753137704398897, Validation Loss: 7.735177993774414\n",
      "Epoch 267, Train Loss: 6.711523411009047, Train Raw Loss: 4.756242765651809, Validation Loss: 7.703378677368164\n",
      "Epoch 268, Train Loss: 6.696484189563328, Train Raw Loss: 4.755066458053059, Validation Loss: 7.69453239440918\n",
      "Epoch 269, Train Loss: 6.66058751543363, Train Raw Loss: 4.7248293618361155, Validation Loss: 7.725425720214844\n",
      "Epoch 270, Train Loss: 6.679141116142273, Train Raw Loss: 4.739317240317662, Validation Loss: 7.677060127258301\n",
      "Epoch 271, Train Loss: 6.662146005365583, Train Raw Loss: 4.729800814059046, Validation Loss: 7.651344299316406\n",
      "Epoch 272, Train Loss: 6.642928666538662, Train Raw Loss: 4.7178127275572885, Validation Loss: 7.667293071746826\n",
      "Epoch 273, Train Loss: 6.648112887806363, Train Raw Loss: 4.722037066684829, Validation Loss: 7.631819725036621\n",
      "Epoch 274, Train Loss: 6.636704292562273, Train Raw Loss: 4.716229729851087, Validation Loss: 7.691726207733154\n",
      "Epoch 275, Train Loss: 6.64286077287462, Train Raw Loss: 4.7211540132761005, Validation Loss: 7.622690677642822\n",
      "Epoch 276, Train Loss: 6.618987327151828, Train Raw Loss: 4.702409459153811, Validation Loss: 7.6267409324646\n",
      "Epoch 277, Train Loss: 6.604708341095183, Train Raw Loss: 4.691738142900997, Validation Loss: 7.648199558258057\n",
      "Epoch 278, Train Loss: 6.607402547862795, Train Raw Loss: 4.693737283349037, Validation Loss: 7.618144512176514\n",
      "Epoch 279, Train Loss: 6.600911578204896, Train Raw Loss: 4.697364372677273, Validation Loss: 7.610800743103027\n",
      "Epoch 280, Train Loss: 6.597641967402565, Train Raw Loss: 4.6928112288316095, Validation Loss: 7.598621368408203\n",
      "Epoch 281, Train Loss: 6.567747581005096, Train Raw Loss: 4.675101680556933, Validation Loss: 7.575316905975342\n",
      "Epoch 282, Train Loss: 6.563176285558277, Train Raw Loss: 4.6667223181989455, Validation Loss: 7.611954689025879\n",
      "Epoch 283, Train Loss: 6.5593506203757395, Train Raw Loss: 4.666288960642285, Validation Loss: 7.569742679595947\n",
      "Epoch 284, Train Loss: 6.564137476682663, Train Raw Loss: 4.669803290565809, Validation Loss: 7.561089515686035\n",
      "Epoch 285, Train Loss: 6.536464199754927, Train Raw Loss: 4.645448395278719, Validation Loss: 7.559977054595947\n",
      "Epoch 286, Train Loss: 6.556646760967043, Train Raw Loss: 4.660443618893623, Validation Loss: 7.542304039001465\n",
      "Epoch 287, Train Loss: 6.537555021047592, Train Raw Loss: 4.647701127330462, Validation Loss: 7.527857303619385\n",
      "Epoch 288, Train Loss: 6.517736871374978, Train Raw Loss: 4.6425455967585245, Validation Loss: 7.52058219909668\n",
      "Epoch 289, Train Loss: 6.503720137145784, Train Raw Loss: 4.630286943581369, Validation Loss: 7.548628807067871\n",
      "Epoch 290, Train Loss: 6.504564126332601, Train Raw Loss: 4.630926233861182, Validation Loss: 7.5569634437561035\n",
      "Epoch 291, Train Loss: 6.507207927438948, Train Raw Loss: 4.629295273953014, Validation Loss: 7.519671440124512\n",
      "Epoch 292, Train Loss: 6.489742443958918, Train Raw Loss: 4.621360107925203, Validation Loss: 7.506351947784424\n",
      "Epoch 293, Train Loss: 6.476076383723153, Train Raw Loss: 4.607323204477628, Validation Loss: 7.507993698120117\n",
      "Epoch 294, Train Loss: 6.4729889406098255, Train Raw Loss: 4.606879351205296, Validation Loss: 7.506881237030029\n",
      "Epoch 295, Train Loss: 6.459923008415434, Train Raw Loss: 4.607258705629243, Validation Loss: 7.521492958068848\n",
      "Epoch 296, Train Loss: 6.48234031730228, Train Raw Loss: 4.612928326924642, Validation Loss: 7.4824347496032715\n",
      "Epoch 297, Train Loss: 6.452386399110158, Train Raw Loss: 4.596407972110642, Validation Loss: 7.500824451446533\n",
      "Epoch 298, Train Loss: 6.466844760047065, Train Raw Loss: 4.6080388877126905, Validation Loss: 7.46263313293457\n",
      "Epoch 299, Train Loss: 6.453459911876255, Train Raw Loss: 4.60307993458377, Validation Loss: 7.467426300048828\n",
      "Epoch 300, Train Loss: 6.430098425017463, Train Raw Loss: 4.581409124864472, Validation Loss: 7.433169841766357\n",
      "Epoch 301, Train Loss: 6.4121371534135605, Train Raw Loss: 4.566153263714578, Validation Loss: 7.471160888671875\n",
      "Epoch 302, Train Loss: 6.43091704249382, Train Raw Loss: 4.585023244884279, Validation Loss: 7.43891716003418\n",
      "Epoch 303, Train Loss: 6.4190880520476234, Train Raw Loss: 4.578289254506429, Validation Loss: 7.437652587890625\n",
      "Epoch 304, Train Loss: 6.403281825118595, Train Raw Loss: 4.564700218041738, Validation Loss: 7.439936637878418\n",
      "Epoch 305, Train Loss: 6.406276377704408, Train Raw Loss: 4.566057680381669, Validation Loss: 7.418434143066406\n",
      "Epoch 306, Train Loss: 6.404188839594523, Train Raw Loss: 4.566721520821253, Validation Loss: 7.391524314880371\n",
      "Epoch 307, Train Loss: 6.392568943566746, Train Raw Loss: 4.567675777276357, Validation Loss: 7.402319431304932\n",
      "Epoch 308, Train Loss: 6.38735674586561, Train Raw Loss: 4.553355557057593, Validation Loss: 7.398627281188965\n",
      "Epoch 309, Train Loss: 6.365136971407466, Train Raw Loss: 4.541761733425988, Validation Loss: 7.375930309295654\n",
      "Epoch 310, Train Loss: 6.365570370356242, Train Raw Loss: 4.547108308639792, Validation Loss: 7.4003987312316895\n",
      "Epoch 311, Train Loss: 6.371342298057344, Train Raw Loss: 4.549736057387458, Validation Loss: 7.398008346557617\n",
      "Epoch 312, Train Loss: 6.374209521545304, Train Raw Loss: 4.551729025774532, Validation Loss: 7.360950946807861\n",
      "Epoch 313, Train Loss: 6.343028854661518, Train Raw Loss: 4.525836590429147, Validation Loss: 7.364788055419922\n",
      "Epoch 314, Train Loss: 6.353311155902015, Train Raw Loss: 4.541377076837752, Validation Loss: 7.3553080558776855\n",
      "Epoch 315, Train Loss: 6.332044531901677, Train Raw Loss: 4.524115606480175, Validation Loss: 7.336387634277344\n",
      "Epoch 316, Train Loss: 6.334459510114458, Train Raw Loss: 4.522759634256363, Validation Loss: 7.347132682800293\n",
      "Epoch 317, Train Loss: 6.324899430738555, Train Raw Loss: 4.525038740038871, Validation Loss: 7.348586082458496\n",
      "Epoch 318, Train Loss: 6.333122864034441, Train Raw Loss: 4.527475374937057, Validation Loss: 7.320313930511475\n",
      "Epoch 319, Train Loss: 6.310725905829006, Train Raw Loss: 4.510817526777585, Validation Loss: 7.334682941436768\n",
      "Epoch 320, Train Loss: 6.29855508406957, Train Raw Loss: 4.507602569792006, Validation Loss: 7.3174967765808105\n",
      "Epoch 321, Train Loss: 6.327625703149372, Train Raw Loss: 4.529963791535961, Validation Loss: 7.278285503387451\n",
      "Epoch 322, Train Loss: 6.305375234948264, Train Raw Loss: 4.513302285638121, Validation Loss: 7.277688980102539\n",
      "Epoch 323, Train Loss: 6.287654985321892, Train Raw Loss: 4.496652082767752, Validation Loss: 7.300991535186768\n",
      "Epoch 324, Train Loss: 6.297009558478991, Train Raw Loss: 4.50822045703729, Validation Loss: 7.298158168792725\n",
      "Epoch 325, Train Loss: 6.291437585486306, Train Raw Loss: 4.503071644074387, Validation Loss: 7.286369323730469\n",
      "Epoch 326, Train Loss: 6.280269472135438, Train Raw Loss: 4.495635945763853, Validation Loss: 7.297228813171387\n",
      "Epoch 327, Train Loss: 6.273418843415048, Train Raw Loss: 4.4932420336537895, Validation Loss: 7.280095100402832\n",
      "Epoch 328, Train Loss: 6.265850178731812, Train Raw Loss: 4.485404810143842, Validation Loss: 7.275722026824951\n",
      "Epoch 329, Train Loss: 6.27581551041868, Train Raw Loss: 4.48605570776595, Validation Loss: 7.2829461097717285\n",
      "Epoch 330, Train Loss: 6.258178787430127, Train Raw Loss: 4.491756245493889, Validation Loss: 7.266217231750488\n",
      "Epoch 331, Train Loss: 6.269045070476002, Train Raw Loss: 4.488204043110212, Validation Loss: 7.257808208465576\n",
      "Epoch 332, Train Loss: 6.2584068453974195, Train Raw Loss: 4.486400528417693, Validation Loss: 7.259452819824219\n",
      "Epoch 333, Train Loss: 6.24338660604424, Train Raw Loss: 4.472582109769186, Validation Loss: 7.253271579742432\n",
      "Epoch 334, Train Loss: 6.2408023138841, Train Raw Loss: 4.47072502705786, Validation Loss: 7.245547294616699\n",
      "Epoch 335, Train Loss: 6.247767726249165, Train Raw Loss: 4.476633195082346, Validation Loss: 7.206343650817871\n",
      "Epoch 336, Train Loss: 6.235851581560241, Train Raw Loss: 4.472249385880099, Validation Loss: 7.2130208015441895\n",
      "Epoch 337, Train Loss: 6.22705091436704, Train Raw Loss: 4.465888526539008, Validation Loss: 7.189700603485107\n",
      "Epoch 338, Train Loss: 6.2078853872087265, Train Raw Loss: 4.445735547112094, Validation Loss: 7.221742630004883\n",
      "Epoch 339, Train Loss: 6.204908360375298, Train Raw Loss: 4.453723787433571, Validation Loss: 7.205630302429199\n",
      "Epoch 340, Train Loss: 6.203659622867902, Train Raw Loss: 4.453062107165654, Validation Loss: 7.195288181304932\n",
      "Epoch 341, Train Loss: 6.197020767463578, Train Raw Loss: 4.447424983978271, Validation Loss: 7.224722862243652\n",
      "Epoch 342, Train Loss: 6.201458729969131, Train Raw Loss: 4.44606570535236, Validation Loss: 7.203242301940918\n",
      "Epoch 343, Train Loss: 6.201190322637558, Train Raw Loss: 4.45022043502993, Validation Loss: 7.18610143661499\n",
      "Epoch 344, Train Loss: 6.176093989610672, Train Raw Loss: 4.436201255520185, Validation Loss: 7.177250862121582\n",
      "Epoch 345, Train Loss: 6.185001697142919, Train Raw Loss: 4.439909315771526, Validation Loss: 7.156947612762451\n",
      "Epoch 346, Train Loss: 6.178057824571927, Train Raw Loss: 4.437361109422313, Validation Loss: 7.1651787757873535\n",
      "Epoch 347, Train Loss: 6.188575995299551, Train Raw Loss: 4.440496999356482, Validation Loss: 7.134490489959717\n",
      "Epoch 348, Train Loss: 6.15464314421018, Train Raw Loss: 4.417599946757158, Validation Loss: 7.196568489074707\n",
      "Epoch 349, Train Loss: 6.170005476474762, Train Raw Loss: 4.431916463209523, Validation Loss: 7.121583938598633\n",
      "Epoch 350, Train Loss: 6.148006395830048, Train Raw Loss: 4.4128330894642405, Validation Loss: 7.140096664428711\n",
      "Epoch 351, Train Loss: 6.156419574552112, Train Raw Loss: 4.4250851324862905, Validation Loss: 7.144620895385742\n",
      "Epoch 352, Train Loss: 6.153216134839588, Train Raw Loss: 4.421738673084312, Validation Loss: 7.130398750305176\n",
      "Epoch 353, Train Loss: 6.150996247927348, Train Raw Loss: 4.422896450261275, Validation Loss: 7.109577655792236\n",
      "Epoch 354, Train Loss: 6.1437973260879515, Train Raw Loss: 4.411160351170434, Validation Loss: 7.1125617027282715\n",
      "Epoch 355, Train Loss: 6.135042944219378, Train Raw Loss: 4.4066124113069645, Validation Loss: 7.096268653869629\n",
      "Epoch 356, Train Loss: 6.135298895835876, Train Raw Loss: 4.417026335994403, Validation Loss: 7.098811626434326\n",
      "Epoch 357, Train Loss: 6.101376476221614, Train Raw Loss: 4.3812191819151245, Validation Loss: 7.1342854499816895\n",
      "Epoch 358, Train Loss: 6.129709773593479, Train Raw Loss: 4.4069165610604815, Validation Loss: 7.087442398071289\n",
      "Epoch 359, Train Loss: 6.123669092522727, Train Raw Loss: 4.401733836200502, Validation Loss: 7.069159030914307\n",
      "Epoch 360, Train Loss: 6.099102583527565, Train Raw Loss: 4.391083823972278, Validation Loss: 7.092419624328613\n",
      "Epoch 361, Train Loss: 6.111840566330486, Train Raw Loss: 4.396551915175385, Validation Loss: 7.082371234893799\n",
      "Epoch 362, Train Loss: 6.0924160023530325, Train Raw Loss: 4.38327690710624, Validation Loss: 7.07538366317749\n",
      "Epoch 363, Train Loss: 6.094915447963609, Train Raw Loss: 4.3891692731115555, Validation Loss: 7.0694451332092285\n",
      "Epoch 364, Train Loss: 6.084331780672073, Train Raw Loss: 4.379974199665917, Validation Loss: 7.081066131591797\n",
      "Epoch 365, Train Loss: 6.083509302470419, Train Raw Loss: 4.376938617891735, Validation Loss: 7.072941303253174\n",
      "Epoch 366, Train Loss: 6.082619566056463, Train Raw Loss: 4.378563159207503, Validation Loss: 7.0752339363098145\n",
      "Epoch 367, Train Loss: 6.077866460879644, Train Raw Loss: 4.373763239880403, Validation Loss: 7.063903331756592\n",
      "Epoch 368, Train Loss: 6.067089599370957, Train Raw Loss: 4.3702569443318575, Validation Loss: 7.06219482421875\n",
      "Epoch 369, Train Loss: 6.0676256987783646, Train Raw Loss: 4.3760712171594305, Validation Loss: 7.059659004211426\n",
      "Epoch 370, Train Loss: 6.06088823709223, Train Raw Loss: 4.367021898428599, Validation Loss: 7.054952621459961\n",
      "Epoch 371, Train Loss: 6.06243851714664, Train Raw Loss: 4.367784880432818, Validation Loss: 7.034958362579346\n",
      "Epoch 372, Train Loss: 6.055004129807155, Train Raw Loss: 4.365443966620498, Validation Loss: 7.0161452293396\n",
      "Epoch 373, Train Loss: 6.040715599060059, Train Raw Loss: 4.357085095842679, Validation Loss: 7.008058071136475\n",
      "Epoch 374, Train Loss: 6.04571963581774, Train Raw Loss: 4.359604642788569, Validation Loss: 6.994830131530762\n",
      "Epoch 375, Train Loss: 6.028141585323546, Train Raw Loss: 4.348149102760686, Validation Loss: 7.0242438316345215\n",
      "Epoch 376, Train Loss: 6.038183354338011, Train Raw Loss: 4.350197700162728, Validation Loss: 7.014907360076904\n",
      "Epoch 377, Train Loss: 6.03445515897539, Train Raw Loss: 4.355849387910631, Validation Loss: 7.026919364929199\n",
      "Epoch 378, Train Loss: 6.034490883350372, Train Raw Loss: 4.35493935280376, Validation Loss: 6.970476150512695\n",
      "Epoch 379, Train Loss: 6.009416956702868, Train Raw Loss: 4.333940097689629, Validation Loss: 6.999814033508301\n",
      "Epoch 380, Train Loss: 6.004891500208113, Train Raw Loss: 4.335974059336715, Validation Loss: 7.029290199279785\n",
      "Epoch 381, Train Loss: 6.026055417789353, Train Raw Loss: 4.347418348822329, Validation Loss: 7.005356311798096\n",
      "Epoch 382, Train Loss: 6.021720523966684, Train Raw Loss: 4.346296198831665, Validation Loss: 6.985910892486572\n",
      "Epoch 383, Train Loss: 6.008427893122037, Train Raw Loss: 4.334381016592185, Validation Loss: 6.98439884185791\n",
      "Epoch 384, Train Loss: 6.007099444667499, Train Raw Loss: 4.340966194868088, Validation Loss: 6.966441631317139\n",
      "Epoch 385, Train Loss: 5.979768804046842, Train Raw Loss: 4.316208497848776, Validation Loss: 6.981166362762451\n",
      "Epoch 386, Train Loss: 5.98635717233022, Train Raw Loss: 4.319731103380521, Validation Loss: 6.972792625427246\n",
      "Epoch 387, Train Loss: 5.996168474687471, Train Raw Loss: 4.334603715439638, Validation Loss: 6.937734603881836\n",
      "Epoch 388, Train Loss: 5.953925701644685, Train Raw Loss: 4.306892814570003, Validation Loss: 6.963142395019531\n",
      "Epoch 389, Train Loss: 5.971013196971682, Train Raw Loss: 4.3174437746405605, Validation Loss: 6.995738983154297\n",
      "Epoch 390, Train Loss: 5.973226541943021, Train Raw Loss: 4.31688201510244, Validation Loss: 6.908361911773682\n",
      "Epoch 391, Train Loss: 5.9480136775308186, Train Raw Loss: 4.3023049849602915, Validation Loss: 6.974263668060303\n",
      "Epoch 392, Train Loss: 5.968512323167589, Train Raw Loss: 4.315016545852026, Validation Loss: 6.949737071990967\n",
      "Epoch 393, Train Loss: 5.960949861341053, Train Raw Loss: 4.314219027592076, Validation Loss: 6.89437198638916\n",
      "Epoch 394, Train Loss: 5.9310629503594505, Train Raw Loss: 4.291909544997745, Validation Loss: 6.924365043640137\n",
      "Epoch 395, Train Loss: 5.945967784523964, Train Raw Loss: 4.305418369174004, Validation Loss: 6.937909126281738\n",
      "Epoch 396, Train Loss: 5.932199004623625, Train Raw Loss: 4.297196060915788, Validation Loss: 6.929064750671387\n",
      "Epoch 397, Train Loss: 5.928077015280723, Train Raw Loss: 4.296271664566464, Validation Loss: 6.905301094055176\n",
      "Epoch 398, Train Loss: 5.923418299357096, Train Raw Loss: 4.294773859282334, Validation Loss: 6.895164489746094\n",
      "Epoch 399, Train Loss: 5.917990983857049, Train Raw Loss: 4.287502007186413, Validation Loss: 6.889307498931885\n",
      "Epoch 400, Train Loss: 5.9293434431155525, Train Raw Loss: 4.29250338309341, Validation Loss: 6.921955585479736\n",
      "Epoch 401, Train Loss: 5.909267271227307, Train Raw Loss: 4.286724403997263, Validation Loss: 6.907267093658447\n",
      "Epoch 402, Train Loss: 5.915470387538274, Train Raw Loss: 4.285788643525707, Validation Loss: 6.90863037109375\n",
      "Epoch 403, Train Loss: 5.919275397062302, Train Raw Loss: 4.295309144755205, Validation Loss: 6.868796348571777\n",
      "Epoch 404, Train Loss: 5.923385919796096, Train Raw Loss: 4.291223692231708, Validation Loss: 6.864879608154297\n",
      "Epoch 405, Train Loss: 5.897627086440722, Train Raw Loss: 4.273669814566771, Validation Loss: 6.8857808113098145\n",
      "Epoch 406, Train Loss: 5.907376282744938, Train Raw Loss: 4.287119579811891, Validation Loss: 6.858262062072754\n",
      "Epoch 407, Train Loss: 5.88094987836149, Train Raw Loss: 4.264773947331641, Validation Loss: 6.888397216796875\n",
      "Epoch 408, Train Loss: 5.887165653043323, Train Raw Loss: 4.2739309731456965, Validation Loss: 6.87091588973999\n",
      "Epoch 409, Train Loss: 5.890397278136677, Train Raw Loss: 4.274162315660053, Validation Loss: 6.852673530578613\n",
      "Epoch 410, Train Loss: 5.876625351773368, Train Raw Loss: 4.262997341487143, Validation Loss: 6.855040073394775\n",
      "Epoch 411, Train Loss: 5.870939556757609, Train Raw Loss: 4.259811098211341, Validation Loss: 6.849933624267578\n",
      "Epoch 412, Train Loss: 5.870885522166888, Train Raw Loss: 4.257946642902162, Validation Loss: 6.869626998901367\n",
      "Epoch 413, Train Loss: 5.875802479022079, Train Raw Loss: 4.273437982300917, Validation Loss: 6.846124172210693\n",
      "Epoch 414, Train Loss: 5.867033725645807, Train Raw Loss: 4.2670480358931755, Validation Loss: 6.860231399536133\n",
      "Epoch 415, Train Loss: 5.865954600108994, Train Raw Loss: 4.265639572342237, Validation Loss: 6.852383136749268\n",
      "Epoch 416, Train Loss: 5.842635060018964, Train Raw Loss: 4.2488746366567085, Validation Loss: 6.8473944664001465\n",
      "Epoch 417, Train Loss: 5.851850985487302, Train Raw Loss: 4.247809953987598, Validation Loss: 6.840489864349365\n",
      "Epoch 418, Train Loss: 5.846274323264757, Train Raw Loss: 4.254185186823209, Validation Loss: 6.824009895324707\n",
      "Epoch 419, Train Loss: 5.838773465156555, Train Raw Loss: 4.249689693748951, Validation Loss: 6.821976661682129\n",
      "Epoch 420, Train Loss: 5.830780547857285, Train Raw Loss: 4.244384329517683, Validation Loss: 6.833288669586182\n",
      "Epoch 421, Train Loss: 5.833613726331128, Train Raw Loss: 4.243250681956609, Validation Loss: 6.830945014953613\n",
      "Epoch 422, Train Loss: 5.831031051609251, Train Raw Loss: 4.244880566828781, Validation Loss: 6.805691242218018\n",
      "Epoch 423, Train Loss: 5.829258125358158, Train Raw Loss: 4.2361959798468485, Validation Loss: 6.808524131774902\n",
      "Epoch 424, Train Loss: 5.8281177714467045, Train Raw Loss: 4.2435946461227205, Validation Loss: 6.805410385131836\n",
      "Epoch 425, Train Loss: 5.826341876718733, Train Raw Loss: 4.239215170343717, Validation Loss: 6.80421257019043\n",
      "Epoch 426, Train Loss: 5.827246271404955, Train Raw Loss: 4.236427577171061, Validation Loss: 6.77956485748291\n",
      "Epoch 427, Train Loss: 5.810060395797094, Train Raw Loss: 4.22958034409417, Validation Loss: 6.791869640350342\n",
      "Epoch 428, Train Loss: 5.80470816426807, Train Raw Loss: 4.226117139392429, Validation Loss: 6.770854473114014\n",
      "Epoch 429, Train Loss: 5.806075278917948, Train Raw Loss: 4.233841557469633, Validation Loss: 6.772674083709717\n",
      "Epoch 430, Train Loss: 5.799612839188841, Train Raw Loss: 4.223898827532927, Validation Loss: 6.7596869468688965\n",
      "Epoch 431, Train Loss: 5.797524405850305, Train Raw Loss: 4.222165924807389, Validation Loss: 6.75017786026001\n",
      "Epoch 432, Train Loss: 5.791495317882962, Train Raw Loss: 4.22307896580961, Validation Loss: 6.755023002624512\n",
      "Epoch 433, Train Loss: 5.787839332885213, Train Raw Loss: 4.217367625567648, Validation Loss: 6.749881267547607\n",
      "Epoch 434, Train Loss: 5.764336949918005, Train Raw Loss: 4.208550907505883, Validation Loss: 6.800366401672363\n",
      "Epoch 435, Train Loss: 5.782846596671475, Train Raw Loss: 4.220027254356278, Validation Loss: 6.749063968658447\n",
      "Epoch 436, Train Loss: 5.7889021254248085, Train Raw Loss: 4.224545687105921, Validation Loss: 6.751800537109375\n",
      "Epoch 437, Train Loss: 5.7801729758580525, Train Raw Loss: 4.218882724311617, Validation Loss: 6.749190807342529\n",
      "Epoch 438, Train Loss: 5.76900079217222, Train Raw Loss: 4.21475559870402, Validation Loss: 6.742733955383301\n",
      "Epoch 439, Train Loss: 5.7721159040927885, Train Raw Loss: 4.213755484918753, Validation Loss: 6.7496514320373535\n",
      "Epoch 440, Train Loss: 5.762030551003085, Train Raw Loss: 4.206994590825505, Validation Loss: 6.7265753746032715\n",
      "Epoch 441, Train Loss: 5.7443705017368, Train Raw Loss: 4.19763217618068, Validation Loss: 6.748771667480469\n",
      "Epoch 442, Train Loss: 5.764705646865898, Train Raw Loss: 4.211290139291021, Validation Loss: 6.751280307769775\n",
      "Epoch 443, Train Loss: 5.7572109058499334, Train Raw Loss: 4.204065054986212, Validation Loss: 6.728453159332275\n",
      "Epoch 444, Train Loss: 5.760061387386587, Train Raw Loss: 4.209302097724544, Validation Loss: 6.698888301849365\n",
      "Epoch 445, Train Loss: 5.742222179638015, Train Raw Loss: 4.191933618899848, Validation Loss: 6.711191654205322\n",
      "Epoch 446, Train Loss: 5.738681540389856, Train Raw Loss: 4.193859949707985, Validation Loss: 6.707378387451172\n",
      "Epoch 447, Train Loss: 5.747688427070776, Train Raw Loss: 4.201919538941648, Validation Loss: 6.73057746887207\n",
      "Epoch 448, Train Loss: 5.730003791881932, Train Raw Loss: 4.187061257991526, Validation Loss: 6.725001811981201\n",
      "Epoch 449, Train Loss: 5.73662903027402, Train Raw Loss: 4.197008059836096, Validation Loss: 6.738561153411865\n",
      "Epoch 450, Train Loss: 5.718130776120557, Train Raw Loss: 4.188445429421133, Validation Loss: 6.747170448303223\n",
      "Epoch 451, Train Loss: 5.747907563381725, Train Raw Loss: 4.204579391413265, Validation Loss: 6.696254253387451\n",
      "Epoch 452, Train Loss: 5.7177201145225105, Train Raw Loss: 4.188226909604338, Validation Loss: 6.709654331207275\n",
      "Epoch 453, Train Loss: 5.717883543670178, Train Raw Loss: 4.188497679183881, Validation Loss: 6.710812568664551\n",
      "Epoch 454, Train Loss: 5.709855691260762, Train Raw Loss: 4.179248794913292, Validation Loss: 6.683368682861328\n",
      "Epoch 455, Train Loss: 5.719376685387559, Train Raw Loss: 4.1934884662429495, Validation Loss: 6.702670097351074\n",
      "Epoch 456, Train Loss: 5.716150337457657, Train Raw Loss: 4.1859042978949015, Validation Loss: 6.665127754211426\n",
      "Epoch 457, Train Loss: 5.7223945064677135, Train Raw Loss: 4.192143374350336, Validation Loss: 6.697011470794678\n",
      "Epoch 458, Train Loss: 5.683850396838453, Train Raw Loss: 4.162109762926897, Validation Loss: 6.719968795776367\n",
      "Epoch 459, Train Loss: 5.694455267488957, Train Raw Loss: 4.179292943328619, Validation Loss: 6.697493553161621\n",
      "Epoch 460, Train Loss: 5.705834570858213, Train Raw Loss: 4.185024355351925, Validation Loss: 6.706568717956543\n",
      "Epoch 461, Train Loss: 5.714949703878826, Train Raw Loss: 4.183333139535454, Validation Loss: 6.720901966094971\n",
      "Epoch 462, Train Loss: 5.680761056476169, Train Raw Loss: 4.176809776657158, Validation Loss: 6.6695380210876465\n",
      "Epoch 463, Train Loss: 5.69581156687604, Train Raw Loss: 4.179522248523103, Validation Loss: 6.660728454589844\n",
      "Epoch 464, Train Loss: 5.667464018861453, Train Raw Loss: 4.1593959164288306, Validation Loss: 6.6847710609436035\n",
      "Epoch 465, Train Loss: 5.685527342557907, Train Raw Loss: 4.171914376070102, Validation Loss: 6.624000549316406\n",
      "Epoch 466, Train Loss: 5.672876648439301, Train Raw Loss: 4.163734261526002, Validation Loss: 6.661203384399414\n",
      "Epoch 467, Train Loss: 5.679103521505992, Train Raw Loss: 4.173361908975575, Validation Loss: 6.624041557312012\n",
      "Epoch 468, Train Loss: 5.6601804021332, Train Raw Loss: 4.162207196984026, Validation Loss: 6.650392532348633\n",
      "Epoch 469, Train Loss: 5.661203134556612, Train Raw Loss: 4.153571322643095, Validation Loss: 6.651129722595215\n",
      "Epoch 470, Train Loss: 5.648728077775902, Train Raw Loss: 4.1519913436637985, Validation Loss: 6.640110492706299\n",
      "Epoch 471, Train Loss: 5.670599319040775, Train Raw Loss: 4.165476794540882, Validation Loss: 6.618710517883301\n",
      "Epoch 472, Train Loss: 5.65460309965743, Train Raw Loss: 4.157180985063315, Validation Loss: 6.656721115112305\n",
      "Epoch 473, Train Loss: 5.65406470729245, Train Raw Loss: 4.158507622033357, Validation Loss: 6.6128010749816895\n",
      "Epoch 474, Train Loss: 5.6439328491687775, Train Raw Loss: 4.1507625391085945, Validation Loss: 6.618467807769775\n",
      "Epoch 475, Train Loss: 5.663822792801592, Train Raw Loss: 4.165491782128811, Validation Loss: 6.621954917907715\n",
      "Epoch 476, Train Loss: 5.641523386041324, Train Raw Loss: 4.151024450278944, Validation Loss: 6.627038478851318\n",
      "Epoch 477, Train Loss: 5.650431393252479, Train Raw Loss: 4.161823156227668, Validation Loss: 6.611069679260254\n",
      "Epoch 478, Train Loss: 5.632520616882378, Train Raw Loss: 4.148009848015176, Validation Loss: 6.6243720054626465\n",
      "Epoch 479, Train Loss: 5.638247376183669, Train Raw Loss: 4.149369301895301, Validation Loss: 6.583160877227783\n",
      "Epoch 480, Train Loss: 5.626680678460333, Train Raw Loss: 4.1465257025427285, Validation Loss: 6.589532375335693\n",
      "Epoch 481, Train Loss: 5.608158276644018, Train Raw Loss: 4.139137036022213, Validation Loss: 6.635684490203857\n",
      "Epoch 482, Train Loss: 5.635788244340155, Train Raw Loss: 4.149680187387599, Validation Loss: 6.57469367980957\n",
      "Epoch 483, Train Loss: 5.599592130548424, Train Raw Loss: 4.1354868590831755, Validation Loss: 6.59268856048584\n",
      "Epoch 484, Train Loss: 5.619936678144667, Train Raw Loss: 4.143882163117329, Validation Loss: 6.568299770355225\n",
      "Epoch 485, Train Loss: 5.613419131603506, Train Raw Loss: 4.138870673461093, Validation Loss: 6.567713737487793\n",
      "Epoch 486, Train Loss: 5.608722216222021, Train Raw Loss: 4.1366011892755825, Validation Loss: 6.558438301086426\n",
      "Epoch 487, Train Loss: 5.603673866225614, Train Raw Loss: 4.132881227715147, Validation Loss: 6.536820411682129\n",
      "Epoch 488, Train Loss: 5.595063470966286, Train Raw Loss: 4.131353377633625, Validation Loss: 6.565536022186279\n",
      "Epoch 489, Train Loss: 5.612544038891793, Train Raw Loss: 4.139811678230762, Validation Loss: 6.590543746948242\n",
      "Epoch 490, Train Loss: 5.588332044912709, Train Raw Loss: 4.12737434167001, Validation Loss: 6.586526870727539\n",
      "Epoch 491, Train Loss: 5.587608355449306, Train Raw Loss: 4.126213406274716, Validation Loss: 6.62671422958374\n",
      "Epoch 492, Train Loss: 5.600914122329818, Train Raw Loss: 4.1375833324260185, Validation Loss: 6.56165885925293\n",
      "Epoch 493, Train Loss: 5.567342026862833, Train Raw Loss: 4.116061561885807, Validation Loss: 6.567859649658203\n",
      "Epoch 494, Train Loss: 5.583489323490196, Train Raw Loss: 4.128408954706457, Validation Loss: 6.55584716796875\n",
      "Epoch 495, Train Loss: 5.5604490947392256, Train Raw Loss: 4.105076114502218, Validation Loss: 6.582161903381348\n",
      "Epoch 496, Train Loss: 5.582427036762238, Train Raw Loss: 4.12807867212428, Validation Loss: 6.586010932922363\n",
      "Epoch 497, Train Loss: 5.582579240037335, Train Raw Loss: 4.13267556892501, Validation Loss: 6.5228142738342285\n",
      "Epoch 498, Train Loss: 5.560315445231067, Train Raw Loss: 4.108755160702599, Validation Loss: 6.534004211425781\n",
      "Epoch 499, Train Loss: 5.568432945013046, Train Raw Loss: 4.1210926075776415, Validation Loss: 6.511998176574707\n",
      "Epoch 500, Train Loss: 5.556825226048629, Train Raw Loss: 4.108448100421164, Validation Loss: 6.542377471923828\n",
      "Epoch 501, Train Loss: 5.5478654757142065, Train Raw Loss: 4.110299933122264, Validation Loss: 6.560611248016357\n",
      "Epoch 502, Train Loss: 5.556510846316814, Train Raw Loss: 4.11373432237241, Validation Loss: 6.5304341316223145\n",
      "Epoch 503, Train Loss: 5.541122330394056, Train Raw Loss: 4.10254184719589, Validation Loss: 6.531108379364014\n",
      "Epoch 504, Train Loss: 5.548908404343658, Train Raw Loss: 4.104218205726809, Validation Loss: 6.523762226104736\n",
      "Epoch 505, Train Loss: 5.542380959126684, Train Raw Loss: 4.1106061531437765, Validation Loss: 6.550358295440674\n",
      "Epoch 506, Train Loss: 5.548660677174727, Train Raw Loss: 4.110954586582052, Validation Loss: 6.507748603820801\n",
      "Epoch 507, Train Loss: 5.540534789032407, Train Raw Loss: 4.1053950002623925, Validation Loss: 6.540786266326904\n",
      "Epoch 508, Train Loss: 5.525460877186722, Train Raw Loss: 4.0966235075559885, Validation Loss: 6.5276923179626465\n",
      "Epoch 509, Train Loss: 5.541499261558056, Train Raw Loss: 4.104540371066994, Validation Loss: 6.495615482330322\n",
      "Epoch 510, Train Loss: 5.531673147943285, Train Raw Loss: 4.095908484028445, Validation Loss: 6.517684459686279\n",
      "Epoch 511, Train Loss: 5.516083917518457, Train Raw Loss: 4.0943186942901875, Validation Loss: 6.519222736358643\n",
      "Epoch 512, Train Loss: 5.518916425936752, Train Raw Loss: 4.0974664164086185, Validation Loss: 6.512502670288086\n",
      "Epoch 513, Train Loss: 5.508888363672627, Train Raw Loss: 4.08689507883456, Validation Loss: 6.523836135864258\n",
      "Epoch 514, Train Loss: 5.5271849845846495, Train Raw Loss: 4.102784852352407, Validation Loss: 6.455067157745361\n",
      "Epoch 515, Train Loss: 5.516129723522398, Train Raw Loss: 4.099368477778302, Validation Loss: 6.46420955657959\n",
      "Epoch 516, Train Loss: 5.497602748043008, Train Raw Loss: 4.083088701963424, Validation Loss: 6.496905326843262\n",
      "Epoch 517, Train Loss: 5.505036191973422, Train Raw Loss: 4.092992124789291, Validation Loss: 6.445770263671875\n",
      "Epoch 518, Train Loss: 5.496060878866249, Train Raw Loss: 4.086419958538479, Validation Loss: 6.46376895904541\n",
      "Epoch 519, Train Loss: 5.4878981000847284, Train Raw Loss: 4.076740981886784, Validation Loss: 6.471576690673828\n",
      "Epoch 520, Train Loss: 5.49421645651261, Train Raw Loss: 4.08266776493854, Validation Loss: 6.461025714874268\n",
      "Epoch 521, Train Loss: 5.476464475691318, Train Raw Loss: 4.073966339313322, Validation Loss: 6.47643518447876\n",
      "Epoch 522, Train Loss: 5.490908468597465, Train Raw Loss: 4.0851873497996065, Validation Loss: 6.457638740539551\n",
      "Epoch 523, Train Loss: 5.481649286879433, Train Raw Loss: 4.0790348937941925, Validation Loss: 6.452488899230957\n",
      "Epoch 524, Train Loss: 5.482357250485156, Train Raw Loss: 4.081013702683979, Validation Loss: 6.469631671905518\n",
      "Epoch 525, Train Loss: 5.474051727520095, Train Raw Loss: 4.072433141784535, Validation Loss: 6.468386650085449\n",
      "Epoch 526, Train Loss: 5.471154161128733, Train Raw Loss: 4.073147556516859, Validation Loss: 6.448521614074707\n",
      "Epoch 527, Train Loss: 5.471463243166606, Train Raw Loss: 4.072782775014639, Validation Loss: 6.489345073699951\n",
      "Epoch 528, Train Loss: 5.472744132412805, Train Raw Loss: 4.070421178804503, Validation Loss: 6.468217849731445\n",
      "Epoch 529, Train Loss: 5.4678092087308565, Train Raw Loss: 4.0734264990521805, Validation Loss: 6.4513936042785645\n",
      "Epoch 530, Train Loss: 5.4628845062520766, Train Raw Loss: 4.066476841767629, Validation Loss: 6.447220802307129\n",
      "Epoch 531, Train Loss: 5.468451960219277, Train Raw Loss: 4.071640206293927, Validation Loss: 6.471714973449707\n",
      "Epoch 532, Train Loss: 5.456636228991879, Train Raw Loss: 4.067533759607209, Validation Loss: 6.515296936035156\n",
      "Epoch 533, Train Loss: 5.46168616645866, Train Raw Loss: 4.070745701342821, Validation Loss: 6.465844631195068\n",
      "Epoch 534, Train Loss: 5.4697297697265945, Train Raw Loss: 4.079613854818874, Validation Loss: 6.4748148918151855\n",
      "Epoch 535, Train Loss: 5.446993490225739, Train Raw Loss: 4.0599594775173395, Validation Loss: 6.458970069885254\n",
      "Epoch 536, Train Loss: 5.448957061933147, Train Raw Loss: 4.065926975756883, Validation Loss: 6.455241680145264\n",
      "Epoch 537, Train Loss: 5.450103197826279, Train Raw Loss: 4.06102245218224, Validation Loss: 6.433917999267578\n",
      "Epoch 538, Train Loss: 5.433359669811196, Train Raw Loss: 4.044644815640317, Validation Loss: 6.476258754730225\n",
      "Epoch 539, Train Loss: 5.436052381495634, Train Raw Loss: 4.058555992444356, Validation Loss: 6.419001579284668\n",
      "Epoch 540, Train Loss: 5.435949869453907, Train Raw Loss: 4.057801422393984, Validation Loss: 6.443436622619629\n",
      "Epoch 541, Train Loss: 5.429937475919724, Train Raw Loss: 4.056688755419519, Validation Loss: 6.427021026611328\n",
      "Epoch 542, Train Loss: 5.444982009960546, Train Raw Loss: 4.064785086860259, Validation Loss: 6.4172892570495605\n",
      "Epoch 543, Train Loss: 5.409184579716788, Train Raw Loss: 4.044539605660571, Validation Loss: 6.408782482147217\n",
      "Epoch 544, Train Loss: 5.433101082675987, Train Raw Loss: 4.060772955997122, Validation Loss: 6.45306396484375\n",
      "Epoch 545, Train Loss: 5.411466940244039, Train Raw Loss: 4.041136921114392, Validation Loss: 6.430104732513428\n",
      "Epoch 546, Train Loss: 5.421462213827504, Train Raw Loss: 4.049642941272921, Validation Loss: 6.4145426750183105\n",
      "Epoch 547, Train Loss: 5.413468596835931, Train Raw Loss: 4.045696350021495, Validation Loss: 6.422099590301514\n",
      "Epoch 548, Train Loss: 5.404935074183676, Train Raw Loss: 4.043380786312951, Validation Loss: 6.422566890716553\n",
      "Epoch 549, Train Loss: 5.421116629905171, Train Raw Loss: 4.058049165374703, Validation Loss: 6.416723728179932\n",
      "Epoch 550, Train Loss: 5.418521682255798, Train Raw Loss: 4.048605771362782, Validation Loss: 6.39122200012207\n",
      "Epoch 551, Train Loss: 5.413308872613642, Train Raw Loss: 4.045710913257467, Validation Loss: 6.429947853088379\n",
      "Epoch 552, Train Loss: 5.416634001334509, Train Raw Loss: 4.05080176666379, Validation Loss: 6.3950066566467285\n",
      "Epoch 553, Train Loss: 5.417964192231496, Train Raw Loss: 4.05001454245713, Validation Loss: 6.418191909790039\n",
      "Epoch 554, Train Loss: 5.4009795975353985, Train Raw Loss: 4.043284000042412, Validation Loss: 6.387571334838867\n",
      "Epoch 555, Train Loss: 5.402729258934657, Train Raw Loss: 4.043894686549902, Validation Loss: 6.383888244628906\n",
      "Epoch 556, Train Loss: 5.394513720936246, Train Raw Loss: 4.034659563998381, Validation Loss: 6.390885353088379\n",
      "Epoch 557, Train Loss: 5.3962232983774605, Train Raw Loss: 4.044374466439089, Validation Loss: 6.388339519500732\n",
      "Epoch 558, Train Loss: 5.3892646764715515, Train Raw Loss: 4.0387006405327055, Validation Loss: 6.377683639526367\n",
      "Epoch 559, Train Loss: 5.390744333631463, Train Raw Loss: 4.041043103983005, Validation Loss: 6.3844170570373535\n",
      "Epoch 560, Train Loss: 5.398599956598547, Train Raw Loss: 4.045690845118629, Validation Loss: 6.363081932067871\n",
      "Epoch 561, Train Loss: 5.370998532407814, Train Raw Loss: 4.0264964818126625, Validation Loss: 6.349729537963867\n",
      "Epoch 562, Train Loss: 5.36993984149562, Train Raw Loss: 4.021397293276257, Validation Loss: 6.377499580383301\n",
      "Epoch 563, Train Loss: 5.384239729411072, Train Raw Loss: 4.036997094833189, Validation Loss: 6.380539417266846\n",
      "Epoch 564, Train Loss: 5.374411346018315, Train Raw Loss: 4.030274476276504, Validation Loss: 6.373325824737549\n",
      "Epoch 565, Train Loss: 5.373921108576987, Train Raw Loss: 4.031921104258961, Validation Loss: 6.352249622344971\n",
      "Epoch 566, Train Loss: 5.3683297046356735, Train Raw Loss: 4.030138577106926, Validation Loss: 6.378365516662598\n",
      "Epoch 567, Train Loss: 5.390506406293975, Train Raw Loss: 4.041096663888958, Validation Loss: 6.364683151245117\n",
      "Epoch 568, Train Loss: 5.373224250806702, Train Raw Loss: 4.032373602771097, Validation Loss: 6.3569416999816895\n",
      "Epoch 569, Train Loss: 5.359593976371818, Train Raw Loss: 4.025392500725057, Validation Loss: 6.346560955047607\n",
      "Epoch 570, Train Loss: 5.365512598223156, Train Raw Loss: 4.022565439509021, Validation Loss: 6.3397908210754395\n",
      "Epoch 571, Train Loss: 5.37210419078668, Train Raw Loss: 4.02733037662175, Validation Loss: 6.330494403839111\n",
      "Epoch 572, Train Loss: 5.3548568252060145, Train Raw Loss: 4.02068010651403, Validation Loss: 6.360710144042969\n",
      "Epoch 573, Train Loss: 5.353362782630655, Train Raw Loss: 4.025614875306686, Validation Loss: 6.335683345794678\n",
      "Epoch 574, Train Loss: 5.34931734767225, Train Raw Loss: 4.022108197129435, Validation Loss: 6.381464958190918\n",
      "Epoch 575, Train Loss: 5.3613201151291525, Train Raw Loss: 4.023115406350957, Validation Loss: 6.368316173553467\n",
      "Epoch 576, Train Loss: 5.358083611395624, Train Raw Loss: 4.032069442090061, Validation Loss: 6.373300552368164\n",
      "Epoch 577, Train Loss: 5.350470855500963, Train Raw Loss: 4.022805555164814, Validation Loss: 6.348088264465332\n",
      "Epoch 578, Train Loss: 5.341591090460619, Train Raw Loss: 4.0229364431566665, Validation Loss: 6.342688083648682\n",
      "Epoch 579, Train Loss: 5.348111969398127, Train Raw Loss: 4.021400198174847, Validation Loss: 6.33342170715332\n",
      "Epoch 580, Train Loss: 5.351310361259514, Train Raw Loss: 4.026108519981305, Validation Loss: 6.325336456298828\n",
      "Epoch 581, Train Loss: 5.339979497260518, Train Raw Loss: 4.0188547476298275, Validation Loss: 6.334151268005371\n",
      "Epoch 582, Train Loss: 5.326589930554231, Train Raw Loss: 4.003090031196674, Validation Loss: 6.357619762420654\n",
      "Epoch 583, Train Loss: 5.373881752623452, Train Raw Loss: 4.0424571333660015, Validation Loss: 6.330569744110107\n",
      "Epoch 584, Train Loss: 5.328794404864311, Train Raw Loss: 4.012226450443268, Validation Loss: 6.302290916442871\n",
      "Epoch 585, Train Loss: 5.3344999449120625, Train Raw Loss: 4.0156889527208275, Validation Loss: 6.303505897521973\n",
      "Epoch 586, Train Loss: 5.330092919949029, Train Raw Loss: 4.012334017703931, Validation Loss: 6.306468963623047\n",
      "Epoch 587, Train Loss: 5.330639241635799, Train Raw Loss: 4.012768079754379, Validation Loss: 6.2994914054870605\n",
      "Epoch 588, Train Loss: 5.323026519020399, Train Raw Loss: 4.009530438151624, Validation Loss: 6.318579196929932\n",
      "Epoch 589, Train Loss: 5.328776081403096, Train Raw Loss: 4.014036635392242, Validation Loss: 6.280265808105469\n",
      "Epoch 590, Train Loss: 5.321443711304003, Train Raw Loss: 4.00225012211336, Validation Loss: 6.375284671783447\n",
      "Epoch 591, Train Loss: 5.315379466944271, Train Raw Loss: 4.005364131679138, Validation Loss: 6.334358215332031\n",
      "Epoch 592, Train Loss: 5.323197310831811, Train Raw Loss: 4.009614741388295, Validation Loss: 6.304110527038574\n",
      "Epoch 593, Train Loss: 5.322946417331695, Train Raw Loss: 4.015370405299796, Validation Loss: 6.3005051612854\n",
      "Epoch 594, Train Loss: 5.327944383687443, Train Raw Loss: 4.019476028449006, Validation Loss: 6.292830944061279\n",
      "Epoch 595, Train Loss: 5.298264550003741, Train Raw Loss: 3.9907937126855058, Validation Loss: 6.301922798156738\n",
      "Epoch 596, Train Loss: 5.312129152235058, Train Raw Loss: 4.006981379290422, Validation Loss: 6.310378074645996\n",
      "Epoch 597, Train Loss: 5.302270307805803, Train Raw Loss: 3.995100108699666, Validation Loss: 6.272375583648682\n",
      "Epoch 598, Train Loss: 5.281101684272289, Train Raw Loss: 3.9826852317485546, Validation Loss: 6.334143161773682\n",
      "Epoch 599, Train Loss: 5.316476697888639, Train Raw Loss: 4.011385986788405, Validation Loss: 6.277917385101318\n",
      "Epoch 600, Train Loss: 5.290062655508518, Train Raw Loss: 3.9963770701653427, Validation Loss: 6.292142868041992\n",
      "Epoch 601, Train Loss: 5.286844095091025, Train Raw Loss: 3.9915250222716065, Validation Loss: 6.319419860839844\n",
      "Epoch 602, Train Loss: 5.313074506819248, Train Raw Loss: 4.007961691502068, Validation Loss: 6.281682014465332\n",
      "Epoch 603, Train Loss: 5.289916716350449, Train Raw Loss: 3.99527487407128, Validation Loss: 6.305202007293701\n",
      "Epoch 604, Train Loss: 5.295943555235863, Train Raw Loss: 4.00339655412568, Validation Loss: 6.317901134490967\n",
      "Epoch 605, Train Loss: 5.3002045908735855, Train Raw Loss: 4.0048185157279175, Validation Loss: 6.329818248748779\n",
      "Epoch 606, Train Loss: 5.30021888597144, Train Raw Loss: 4.004300571978092, Validation Loss: 6.295848369598389\n",
      "Epoch 607, Train Loss: 5.283376650843356, Train Raw Loss: 3.995057327217526, Validation Loss: 6.292235851287842\n",
      "Epoch 608, Train Loss: 5.287462586992317, Train Raw Loss: 3.995860733091831, Validation Loss: 6.267292499542236\n",
      "Epoch 609, Train Loss: 5.276625980271233, Train Raw Loss: 3.990245005157259, Validation Loss: 6.315572738647461\n",
      "Epoch 610, Train Loss: 5.2940540076129965, Train Raw Loss: 4.0049607560038565, Validation Loss: 6.241072654724121\n",
      "Epoch 611, Train Loss: 5.271148742735386, Train Raw Loss: 3.9892295458250575, Validation Loss: 6.249819755554199\n",
      "Epoch 612, Train Loss: 5.2788881588313314, Train Raw Loss: 3.992640239497026, Validation Loss: 6.245086669921875\n",
      "Epoch 613, Train Loss: 5.281268303675784, Train Raw Loss: 3.998222919719087, Validation Loss: 6.271503925323486\n",
      "Epoch 614, Train Loss: 5.266622712380356, Train Raw Loss: 3.9859653289947246, Validation Loss: 6.256043910980225\n",
      "Epoch 615, Train Loss: 5.256325177020496, Train Raw Loss: 3.982055103033781, Validation Loss: 6.278672218322754\n",
      "Epoch 616, Train Loss: 5.273196941117446, Train Raw Loss: 3.9905096554093893, Validation Loss: 6.27778434753418\n",
      "Epoch 617, Train Loss: 5.261075117356247, Train Raw Loss: 3.985647344009744, Validation Loss: 6.289299011230469\n",
      "Epoch 618, Train Loss: 5.268383064534929, Train Raw Loss: 3.9936719512773884, Validation Loss: 6.2699809074401855\n",
      "Epoch 619, Train Loss: 5.252475097775459, Train Raw Loss: 3.9794965462552176, Validation Loss: 6.281914710998535\n",
      "Epoch 620, Train Loss: 5.263661682109038, Train Raw Loss: 3.9893759368194472, Validation Loss: 6.256068706512451\n",
      "Epoch 621, Train Loss: 5.25393003175656, Train Raw Loss: 3.981076578878694, Validation Loss: 6.268702030181885\n",
      "Epoch 622, Train Loss: 5.246596236858103, Train Raw Loss: 3.9772455842130716, Validation Loss: 6.278789043426514\n",
      "Epoch 623, Train Loss: 5.263794212705559, Train Raw Loss: 3.9853768905003863, Validation Loss: 6.237959861755371\n",
      "Epoch 624, Train Loss: 5.257679380807612, Train Raw Loss: 3.987875171833568, Validation Loss: 6.24229621887207\n",
      "Epoch 625, Train Loss: 5.249186530046993, Train Raw Loss: 3.975934092120992, Validation Loss: 6.247220039367676\n",
      "Epoch 626, Train Loss: 5.245219321714507, Train Raw Loss: 3.9806538002358542, Validation Loss: 6.299441337585449\n",
      "Epoch 627, Train Loss: 5.245208806627327, Train Raw Loss: 3.97438293620944, Validation Loss: 6.248876094818115\n",
      "Epoch 628, Train Loss: 5.245565219720205, Train Raw Loss: 3.980767496758037, Validation Loss: 6.252243518829346\n",
      "Epoch 629, Train Loss: 5.254850540558497, Train Raw Loss: 3.982883210149076, Validation Loss: 6.2440266609191895\n",
      "Epoch 630, Train Loss: 5.235436214175489, Train Raw Loss: 3.9737069830298424, Validation Loss: 6.248039245605469\n",
      "Epoch 631, Train Loss: 5.225505968762769, Train Raw Loss: 3.96552326488826, Validation Loss: 6.245402812957764\n",
      "Epoch 632, Train Loss: 5.250502564178573, Train Raw Loss: 3.9824262601633866, Validation Loss: 6.247718811035156\n",
      "Epoch 633, Train Loss: 5.230942386554347, Train Raw Loss: 3.9732599832117557, Validation Loss: 6.259860515594482\n",
      "Epoch 634, Train Loss: 5.236621119081974, Train Raw Loss: 3.9752099294629364, Validation Loss: 6.243948936462402\n",
      "Epoch 635, Train Loss: 5.240520187218984, Train Raw Loss: 3.9784556311865646, Validation Loss: 6.208216190338135\n",
      "Epoch 636, Train Loss: 5.220360448294215, Train Raw Loss: 3.9687747318711546, Validation Loss: 6.252921104431152\n",
      "Epoch 637, Train Loss: 5.241671178738276, Train Raw Loss: 3.9805511539181073, Validation Loss: 6.233338832855225\n",
      "Epoch 638, Train Loss: 5.2215995240542625, Train Raw Loss: 3.9671584661636086, Validation Loss: 6.210841655731201\n",
      "Epoch 639, Train Loss: 5.2313400020202, Train Raw Loss: 3.9742821434305773, Validation Loss: 6.194870948791504\n",
      "Epoch 640, Train Loss: 5.224668019347721, Train Raw Loss: 3.9754339255392552, Validation Loss: 6.24514627456665\n",
      "Epoch 641, Train Loss: 5.226576460566785, Train Raw Loss: 3.971655935380194, Validation Loss: 6.202951431274414\n",
      "Epoch 642, Train Loss: 5.228692221558756, Train Raw Loss: 3.9694961646364795, Validation Loss: 6.200676918029785\n",
      "Epoch 643, Train Loss: 5.207102656861147, Train Raw Loss: 3.9641465839412477, Validation Loss: 6.254638195037842\n",
      "Epoch 644, Train Loss: 5.237733320477936, Train Raw Loss: 3.9838977903955515, Validation Loss: 6.203999042510986\n",
      "Epoch 645, Train Loss: 5.211267052425279, Train Raw Loss: 3.9591108123461405, Validation Loss: 6.192534923553467\n",
      "Epoch 646, Train Loss: 5.208538925978872, Train Raw Loss: 3.9601468838751317, Validation Loss: 6.235257148742676\n",
      "Epoch 647, Train Loss: 5.21272796317935, Train Raw Loss: 3.9683313091595966, Validation Loss: 6.248898506164551\n",
      "Epoch 648, Train Loss: 5.234605022105906, Train Raw Loss: 3.9760970436036587, Validation Loss: 6.255887985229492\n",
      "Epoch 649, Train Loss: 5.221117213368416, Train Raw Loss: 3.973092541181379, Validation Loss: 6.215765953063965\n",
      "Epoch 650, Train Loss: 5.211973876754443, Train Raw Loss: 3.967625738018089, Validation Loss: 6.212620258331299\n",
      "Epoch 651, Train Loss: 5.219252928098043, Train Raw Loss: 3.966063977364037, Validation Loss: 6.202488422393799\n",
      "Epoch 652, Train Loss: 5.197149100734128, Train Raw Loss: 3.9564306936330267, Validation Loss: 6.194921016693115\n",
      "Epoch 653, Train Loss: 5.203831191774872, Train Raw Loss: 3.96060859022869, Validation Loss: 6.198530197143555\n",
      "Epoch 654, Train Loss: 5.195458810610903, Train Raw Loss: 3.952709074235625, Validation Loss: 6.200247764587402\n",
      "Epoch 655, Train Loss: 5.2195507753226495, Train Raw Loss: 3.975618227157328, Validation Loss: 6.195466995239258\n",
      "Epoch 656, Train Loss: 5.204114996973011, Train Raw Loss: 3.9570236687858897, Validation Loss: 6.189509868621826\n",
      "Epoch 657, Train Loss: 5.212234156164858, Train Raw Loss: 3.968129669295417, Validation Loss: 6.153479099273682\n",
      "Epoch 658, Train Loss: 5.198337286462387, Train Raw Loss: 3.959341302431292, Validation Loss: 6.225743293762207\n",
      "Epoch 659, Train Loss: 5.21391334897942, Train Raw Loss: 3.9653352655470373, Validation Loss: 6.178755760192871\n",
      "Epoch 660, Train Loss: 5.192203568418821, Train Raw Loss: 3.9521158047020437, Validation Loss: 6.206869125366211\n",
      "Epoch 661, Train Loss: 5.2201443987588085, Train Raw Loss: 3.9732557183338537, Validation Loss: 6.195714473724365\n",
      "Epoch 662, Train Loss: 5.1931056400140125, Train Raw Loss: 3.958135890877909, Validation Loss: 6.204989910125732\n",
      "Epoch 663, Train Loss: 5.198607922014263, Train Raw Loss: 3.9631694353289073, Validation Loss: 6.169659614562988\n",
      "Epoch 664, Train Loss: 5.1956819307472975, Train Raw Loss: 3.9627412238882647, Validation Loss: 6.195825099945068\n",
      "Epoch 665, Train Loss: 5.191738817509678, Train Raw Loss: 3.953718111415704, Validation Loss: 6.206429481506348\n",
      "Epoch 666, Train Loss: 5.187705906563335, Train Raw Loss: 3.9548379396398863, Validation Loss: 6.177043914794922\n",
      "Epoch 667, Train Loss: 5.177545899649461, Train Raw Loss: 3.9453393313619824, Validation Loss: 6.208669185638428\n",
      "Epoch 668, Train Loss: 5.180578901618719, Train Raw Loss: 3.9492231506440376, Validation Loss: 6.21051025390625\n",
      "Epoch 669, Train Loss: 5.18721036877897, Train Raw Loss: 3.958037090301514, Validation Loss: 6.237242221832275\n",
      "Epoch 670, Train Loss: 5.184538464910454, Train Raw Loss: 3.951557087898254, Validation Loss: 6.193410873413086\n",
      "Epoch 671, Train Loss: 5.17744091881646, Train Raw Loss: 3.944798824770583, Validation Loss: 6.212221145629883\n",
      "Epoch 672, Train Loss: 5.18310197715958, Train Raw Loss: 3.954594783650504, Validation Loss: 6.192228317260742\n",
      "Epoch 673, Train Loss: 5.167437830732928, Train Raw Loss: 3.9419406712883047, Validation Loss: 6.187304496765137\n",
      "Epoch 674, Train Loss: 5.188258925577005, Train Raw Loss: 3.9591231063008308, Validation Loss: 6.207580089569092\n",
      "Epoch 675, Train Loss: 5.192944488260481, Train Raw Loss: 3.9638411045902306, Validation Loss: 6.18579626083374\n",
      "Epoch 676, Train Loss: 5.169151648879051, Train Raw Loss: 3.942366277674834, Validation Loss: 6.203832149505615\n",
      "Epoch 677, Train Loss: 5.181189790119728, Train Raw Loss: 3.956333511322737, Validation Loss: 6.219261169433594\n",
      "Epoch 678, Train Loss: 5.193487810095151, Train Raw Loss: 3.963798188004229, Validation Loss: 6.168881416320801\n",
      "Epoch 679, Train Loss: 5.170058114412758, Train Raw Loss: 3.9469953063461514, Validation Loss: 6.180338382720947\n",
      "Epoch 680, Train Loss: 5.175207474331061, Train Raw Loss: 3.9510809885130986, Validation Loss: 6.195091247558594\n",
      "Epoch 681, Train Loss: 5.167466908610529, Train Raw Loss: 3.9432076384623844, Validation Loss: 6.184576511383057\n",
      "Epoch 682, Train Loss: 5.166793849898709, Train Raw Loss: 3.948368149250746, Validation Loss: 6.19306755065918\n",
      "Epoch 683, Train Loss: 5.16996347780029, Train Raw Loss: 3.9485606643060844, Validation Loss: 6.143344402313232\n",
      "Epoch 684, Train Loss: 5.163470073209869, Train Raw Loss: 3.947686072770092, Validation Loss: 6.207674026489258\n",
      "Epoch 685, Train Loss: 5.160390090280109, Train Raw Loss: 3.9399889865683186, Validation Loss: 6.16519021987915\n",
      "Epoch 686, Train Loss: 5.172583131161001, Train Raw Loss: 3.9549520382450685, Validation Loss: 6.150218963623047\n",
      "Epoch 687, Train Loss: 5.148789716760318, Train Raw Loss: 3.9385807716184194, Validation Loss: 6.18969202041626\n",
      "Epoch 688, Train Loss: 5.164832868923743, Train Raw Loss: 3.945343786974748, Validation Loss: 6.163455486297607\n",
      "Epoch 689, Train Loss: 5.1661304199033315, Train Raw Loss: 3.948964402079582, Validation Loss: 6.154791831970215\n",
      "Epoch 690, Train Loss: 5.145830991036362, Train Raw Loss: 3.933139946228928, Validation Loss: 6.168681621551514\n",
      "Epoch 691, Train Loss: 5.163529114259614, Train Raw Loss: 3.9528903582857713, Validation Loss: 6.157261371612549\n",
      "Epoch 692, Train Loss: 5.157366586310996, Train Raw Loss: 3.9457627582053343, Validation Loss: 6.133368492126465\n",
      "Epoch 693, Train Loss: 5.152671122799317, Train Raw Loss: 3.9401902239355775, Validation Loss: 6.138277053833008\n",
      "Epoch 694, Train Loss: 5.145763354914056, Train Raw Loss: 3.940071564416091, Validation Loss: 6.170888423919678\n",
      "Epoch 695, Train Loss: 5.149944639537069, Train Raw Loss: 3.9379728875226445, Validation Loss: 6.16106653213501\n",
      "Epoch 696, Train Loss: 5.157138195551104, Train Raw Loss: 3.9391195411483446, Validation Loss: 6.161309242248535\n",
      "Epoch 697, Train Loss: 5.1376060709357265, Train Raw Loss: 3.927026751803027, Validation Loss: 6.1514506340026855\n",
      "Epoch 698, Train Loss: 5.142285131083595, Train Raw Loss: 3.939945863766803, Validation Loss: 6.193410873413086\n",
      "Epoch 699, Train Loss: 5.142381670408779, Train Raw Loss: 3.936800030536122, Validation Loss: 6.1482367515563965\n",
      "Epoch 700, Train Loss: 5.138591994924678, Train Raw Loss: 3.930175482067797, Validation Loss: 6.155043601989746\n",
      "Epoch 701, Train Loss: 5.1597093286613624, Train Raw Loss: 3.9496993319027953, Validation Loss: 6.132151126861572\n",
      "Epoch 702, Train Loss: 5.144868524124225, Train Raw Loss: 3.939433505634467, Validation Loss: 6.110981464385986\n",
      "Epoch 703, Train Loss: 5.138115377144681, Train Raw Loss: 3.940767151779599, Validation Loss: 6.145753860473633\n",
      "Epoch 704, Train Loss: 5.140538171927134, Train Raw Loss: 3.933609433886078, Validation Loss: 6.14354944229126\n",
      "Epoch 705, Train Loss: 5.140212720135848, Train Raw Loss: 3.9415243844191235, Validation Loss: 6.142189025878906\n",
      "Epoch 706, Train Loss: 5.141288252588775, Train Raw Loss: 3.9394384240938556, Validation Loss: 6.158665657043457\n",
      "Epoch 707, Train Loss: 5.141312405880955, Train Raw Loss: 3.9381914224889543, Validation Loss: 6.124878406524658\n",
      "Epoch 708, Train Loss: 5.12934441599581, Train Raw Loss: 3.929925515419907, Validation Loss: 6.140191555023193\n",
      "Epoch 709, Train Loss: 5.130525807456838, Train Raw Loss: 3.9283711820840836, Validation Loss: 6.153764724731445\n",
      "Epoch 710, Train Loss: 5.144219535672002, Train Raw Loss: 3.9402112416095205, Validation Loss: 6.158103942871094\n",
      "Epoch 711, Train Loss: 5.137462345427937, Train Raw Loss: 3.9394778285589482, Validation Loss: 6.137746334075928\n",
      "Epoch 712, Train Loss: 5.126128794749578, Train Raw Loss: 3.9276093480487666, Validation Loss: 6.138174057006836\n",
      "Epoch 713, Train Loss: 5.130907701287005, Train Raw Loss: 3.9342185490661197, Validation Loss: 6.129668712615967\n",
      "Epoch 714, Train Loss: 5.125722676184442, Train Raw Loss: 3.934522483249505, Validation Loss: 6.162772178649902\n",
      "Epoch 715, Train Loss: 5.13149532419112, Train Raw Loss: 3.929977584299114, Validation Loss: 6.1298604011535645\n",
      "Epoch 716, Train Loss: 5.103661710355017, Train Raw Loss: 3.9164667069911956, Validation Loss: 6.184674263000488\n",
      "Epoch 717, Train Loss: 5.1304884064528675, Train Raw Loss: 3.9344894614484573, Validation Loss: 6.157252311706543\n",
      "Epoch 718, Train Loss: 5.116746660653088, Train Raw Loss: 3.9142236651645765, Validation Loss: 6.1381096839904785\n",
      "Epoch 719, Train Loss: 5.126234495557017, Train Raw Loss: 3.9290252686374716, Validation Loss: 6.143048286437988\n",
      "Epoch 720, Train Loss: 5.112915343874031, Train Raw Loss: 3.921536914838685, Validation Loss: 6.127613067626953\n",
      "Epoch 721, Train Loss: 5.133117734889189, Train Raw Loss: 3.941501676953501, Validation Loss: 6.150301933288574\n",
      "Epoch 722, Train Loss: 5.1169298552804525, Train Raw Loss: 3.925140704959631, Validation Loss: 6.144156455993652\n",
      "Epoch 723, Train Loss: 5.1233677295347055, Train Raw Loss: 3.931444422652324, Validation Loss: 6.129045486450195\n",
      "Epoch 724, Train Loss: 5.132211304455995, Train Raw Loss: 3.9315317063695856, Validation Loss: 6.094403266906738\n",
      "Epoch 725, Train Loss: 5.115264941255251, Train Raw Loss: 3.927737285031213, Validation Loss: 6.129866600036621\n",
      "Epoch 726, Train Loss: 5.1114557945893875, Train Raw Loss: 3.923272736204995, Validation Loss: 6.151741027832031\n",
      "Epoch 727, Train Loss: 5.1098764512274, Train Raw Loss: 3.923791032284498, Validation Loss: 6.123313903808594\n",
      "Epoch 728, Train Loss: 5.10193037705289, Train Raw Loss: 3.919751931395796, Validation Loss: 6.159564018249512\n",
      "Epoch 729, Train Loss: 5.122599902500709, Train Raw Loss: 3.933520770735211, Validation Loss: 6.094685077667236\n",
      "Epoch 730, Train Loss: 5.11188189494941, Train Raw Loss: 3.9247556590371664, Validation Loss: 6.122960567474365\n",
      "Epoch 731, Train Loss: 5.105138705256913, Train Raw Loss: 3.9202804141574434, Validation Loss: 6.11029577255249\n",
      "Epoch 732, Train Loss: 5.10578968293137, Train Raw Loss: 3.9206190363400513, Validation Loss: 6.115855693817139\n",
      "Epoch 733, Train Loss: 5.103832139902645, Train Raw Loss: 3.923442378391822, Validation Loss: 6.120788097381592\n",
      "Epoch 734, Train Loss: 5.113640089001921, Train Raw Loss: 3.927534023506774, Validation Loss: 6.089604377746582\n",
      "Epoch 735, Train Loss: 5.102009177539084, Train Raw Loss: 3.920980907231569, Validation Loss: 6.129641532897949\n",
      "Epoch 736, Train Loss: 5.098522799296512, Train Raw Loss: 3.9171888295974995, Validation Loss: 6.129066467285156\n",
      "Epoch 737, Train Loss: 5.108469673660067, Train Raw Loss: 3.92807385093636, Validation Loss: 6.103672504425049\n",
      "Epoch 738, Train Loss: 5.106486549890704, Train Raw Loss: 3.9195956598553394, Validation Loss: 6.091830730438232\n",
      "Epoch 739, Train Loss: 5.100898086445199, Train Raw Loss: 3.9158036278353796, Validation Loss: 6.093344688415527\n",
      "Epoch 740, Train Loss: 5.1185995776620175, Train Raw Loss: 3.9385528499881426, Validation Loss: 6.109394550323486\n",
      "Epoch 741, Train Loss: 5.089561252130403, Train Raw Loss: 3.911740230851703, Validation Loss: 6.1249613761901855\n",
      "Epoch 742, Train Loss: 5.096793887101942, Train Raw Loss: 3.91663304840525, Validation Loss: 6.101272106170654\n",
      "Epoch 743, Train Loss: 5.087344211008814, Train Raw Loss: 3.9086661165787113, Validation Loss: 6.114522933959961\n",
      "Epoch 744, Train Loss: 5.098370892554522, Train Raw Loss: 3.9172735318541525, Validation Loss: 6.131444454193115\n",
      "Epoch 745, Train Loss: 5.1063698710666765, Train Raw Loss: 3.9274841140541765, Validation Loss: 6.113846778869629\n",
      "Epoch 746, Train Loss: 5.08923065935572, Train Raw Loss: 3.910311849663655, Validation Loss: 6.089660167694092\n",
      "Epoch 747, Train Loss: 5.095733371211423, Train Raw Loss: 3.917145026723544, Validation Loss: 6.124218940734863\n",
      "Epoch 748, Train Loss: 5.097332181533178, Train Raw Loss: 3.914281705352995, Validation Loss: 6.106053352355957\n",
      "Epoch 749, Train Loss: 5.090658230500089, Train Raw Loss: 3.9178045680953395, Validation Loss: 6.151412010192871\n",
      "Epoch 750, Train Loss: 5.105442729261187, Train Raw Loss: 3.929054872526063, Validation Loss: 6.140175819396973\n",
      "Epoch 751, Train Loss: 5.106325884246164, Train Raw Loss: 3.929175414890051, Validation Loss: 6.134441375732422\n",
      "Epoch 752, Train Loss: 5.083601986368497, Train Raw Loss: 3.9074141282174324, Validation Loss: 6.11962890625\n",
      "Epoch 753, Train Loss: 5.098177082836628, Train Raw Loss: 3.9204593726330335, Validation Loss: 6.122582912445068\n",
      "Epoch 754, Train Loss: 5.084804629534483, Train Raw Loss: 3.9156931217345927, Validation Loss: 6.11788272857666\n",
      "Epoch 755, Train Loss: 5.097515176319414, Train Raw Loss: 3.9187953895992704, Validation Loss: 6.0990824699401855\n",
      "Epoch 756, Train Loss: 5.086179272333781, Train Raw Loss: 3.912753676291969, Validation Loss: 6.115443229675293\n",
      "Epoch 757, Train Loss: 5.090130185667012, Train Raw Loss: 3.9194301219450103, Validation Loss: 6.103650093078613\n",
      "Epoch 758, Train Loss: 5.080813579923577, Train Raw Loss: 3.910271807346079, Validation Loss: 6.085959434509277\n",
      "Epoch 759, Train Loss: 5.087869908826219, Train Raw Loss: 3.918019648641348, Validation Loss: 6.1040520668029785\n",
      "Epoch 760, Train Loss: 5.087820846256283, Train Raw Loss: 3.9198847021493646, Validation Loss: 6.121151924133301\n",
      "Epoch 761, Train Loss: 5.088211632023255, Train Raw Loss: 3.915337788975901, Validation Loss: 6.100348472595215\n",
      "Epoch 762, Train Loss: 5.081975971659024, Train Raw Loss: 3.913835470130046, Validation Loss: 6.091527462005615\n",
      "Epoch 763, Train Loss: 5.083826571785742, Train Raw Loss: 3.9160705601175625, Validation Loss: 6.084636688232422\n",
      "Epoch 764, Train Loss: 5.071432832794057, Train Raw Loss: 3.9037878034015496, Validation Loss: 6.082587242126465\n",
      "Epoch 765, Train Loss: 5.0914728114174475, Train Raw Loss: 3.919364427278439, Validation Loss: 6.083438396453857\n",
      "Epoch 766, Train Loss: 5.085380077775982, Train Raw Loss: 3.9208263015581504, Validation Loss: 6.068204879760742\n",
      "Epoch 767, Train Loss: 5.07906356089645, Train Raw Loss: 3.909771797309319, Validation Loss: 6.084281921386719\n",
      "Epoch 768, Train Loss: 5.079478261454238, Train Raw Loss: 3.9134164687660005, Validation Loss: 6.0603156089782715\n",
      "Epoch 769, Train Loss: 5.061989358647002, Train Raw Loss: 3.902840905967686, Validation Loss: 6.080681800842285\n",
      "Epoch 770, Train Loss: 5.065350585513645, Train Raw Loss: 3.8987950527833566, Validation Loss: 6.089037895202637\n",
      "Epoch 771, Train Loss: 5.0632985139058695, Train Raw Loss: 3.9007248358594047, Validation Loss: 6.08070182800293\n",
      "Epoch 772, Train Loss: 5.071322829524676, Train Raw Loss: 3.908559337506692, Validation Loss: 6.07522439956665\n",
      "Epoch 773, Train Loss: 5.085573738978969, Train Raw Loss: 3.9195558017326726, Validation Loss: 6.071971416473389\n",
      "Epoch 774, Train Loss: 5.083813831210136, Train Raw Loss: 3.9194676031668982, Validation Loss: 6.109905242919922\n",
      "Epoch 775, Train Loss: 5.065603398448891, Train Raw Loss: 3.902010820971595, Validation Loss: 6.079120635986328\n",
      "Epoch 776, Train Loss: 5.065248283412721, Train Raw Loss: 3.9079055230236714, Validation Loss: 6.090801239013672\n",
      "Epoch 777, Train Loss: 5.070624530563752, Train Raw Loss: 3.9095401532120175, Validation Loss: 6.0932841300964355\n",
      "Epoch 778, Train Loss: 5.060184964289268, Train Raw Loss: 3.900491365086701, Validation Loss: 6.0846476554870605\n",
      "Epoch 779, Train Loss: 5.067075574563609, Train Raw Loss: 3.9059876565304066, Validation Loss: 6.093412399291992\n",
      "Epoch 780, Train Loss: 5.06207310176558, Train Raw Loss: 3.9023546383612686, Validation Loss: 6.071112632751465\n",
      "Epoch 781, Train Loss: 5.067330943213569, Train Raw Loss: 3.9032739617758327, Validation Loss: 6.050387859344482\n",
      "Epoch 782, Train Loss: 5.049887510803011, Train Raw Loss: 3.8883396171861224, Validation Loss: 6.062036037445068\n",
      "Epoch 783, Train Loss: 5.04427625818385, Train Raw Loss: 3.893236736704906, Validation Loss: 6.06706428527832\n",
      "Epoch 784, Train Loss: 5.061064318650299, Train Raw Loss: 3.9048363670706747, Validation Loss: 6.07318115234375\n",
      "Epoch 785, Train Loss: 5.071138587511248, Train Raw Loss: 3.913381928453843, Validation Loss: 6.057859897613525\n",
      "Epoch 786, Train Loss: 5.0508910920057035, Train Raw Loss: 3.8965479411184787, Validation Loss: 6.057309150695801\n",
      "Epoch 787, Train Loss: 5.040623887214395, Train Raw Loss: 3.8867626114851896, Validation Loss: 6.063781261444092\n",
      "Epoch 788, Train Loss: 5.040186676051881, Train Raw Loss: 3.8912215162068606, Validation Loss: 6.083445072174072\n",
      "Epoch 789, Train Loss: 5.047128244489431, Train Raw Loss: 3.8976251476340824, Validation Loss: 6.107132911682129\n",
      "Epoch 790, Train Loss: 5.039603598250283, Train Raw Loss: 3.8924751069810655, Validation Loss: 6.112453937530518\n",
      "Epoch 791, Train Loss: 5.0626387722790245, Train Raw Loss: 3.912619675613112, Validation Loss: 6.081228256225586\n",
      "Epoch 792, Train Loss: 5.039847106403775, Train Raw Loss: 3.894436541572213, Validation Loss: 6.059956073760986\n",
      "Epoch 793, Train Loss: 5.045379417058494, Train Raw Loss: 3.895197131112218, Validation Loss: 6.057340621948242\n",
      "Epoch 794, Train Loss: 5.040889102717241, Train Raw Loss: 3.891359048171176, Validation Loss: 6.058125019073486\n",
      "Epoch 795, Train Loss: 5.037137924134731, Train Raw Loss: 3.893018831105696, Validation Loss: 6.051711082458496\n",
      "Epoch 796, Train Loss: 5.0379670341809595, Train Raw Loss: 3.8900875804324944, Validation Loss: 6.080937385559082\n",
      "Epoch 797, Train Loss: 5.041785948475202, Train Raw Loss: 3.8928808339767986, Validation Loss: 6.086949825286865\n",
      "Epoch 798, Train Loss: 5.055118342323436, Train Raw Loss: 3.9023996913598644, Validation Loss: 6.0617756843566895\n",
      "Epoch 799, Train Loss: 5.041500684784518, Train Raw Loss: 3.8970334665642845, Validation Loss: 6.077254295349121\n",
      "Epoch 800, Train Loss: 5.05133837353852, Train Raw Loss: 3.8966090223855443, Validation Loss: 6.056192874908447\n",
      "Epoch 801, Train Loss: 5.052350189122889, Train Raw Loss: 3.9058115486469536, Validation Loss: 6.039332866668701\n",
      "Epoch 802, Train Loss: 5.041592204405202, Train Raw Loss: 3.894124963424272, Validation Loss: 6.07811164855957\n",
      "Epoch 803, Train Loss: 5.042271760271655, Train Raw Loss: 3.8960503851787913, Validation Loss: 6.112913608551025\n",
      "Epoch 804, Train Loss: 5.064210514558686, Train Raw Loss: 3.9098998460504744, Validation Loss: 6.063292980194092\n",
      "Epoch 805, Train Loss: 5.035818334917227, Train Raw Loss: 3.888408929647671, Validation Loss: 6.04741907119751\n",
      "Epoch 806, Train Loss: 5.031804907073577, Train Raw Loss: 3.88804217800498, Validation Loss: 6.029616832733154\n",
      "Epoch 807, Train Loss: 5.038953450984425, Train Raw Loss: 3.8957221591224274, Validation Loss: 6.083369731903076\n",
      "Epoch 808, Train Loss: 5.060304256031911, Train Raw Loss: 3.910317519803842, Validation Loss: 6.081690311431885\n",
      "Epoch 809, Train Loss: 5.041576917386717, Train Raw Loss: 3.8957557206352553, Validation Loss: 6.071496486663818\n",
      "Epoch 810, Train Loss: 5.050186802032921, Train Raw Loss: 3.899103788452016, Validation Loss: 6.062805652618408\n",
      "Epoch 811, Train Loss: 5.037741082989507, Train Raw Loss: 3.889883209599389, Validation Loss: 6.030611991882324\n",
      "Epoch 812, Train Loss: 5.035063360548682, Train Raw Loss: 3.889678096605672, Validation Loss: 6.022601127624512\n",
      "Epoch 813, Train Loss: 5.013767552458578, Train Raw Loss: 3.8744915131893425, Validation Loss: 6.062016010284424\n",
      "Epoch 814, Train Loss: 5.031123778803481, Train Raw Loss: 3.8916536858926216, Validation Loss: 6.075070381164551\n",
      "Epoch 815, Train Loss: 5.029202000796795, Train Raw Loss: 3.888223355677393, Validation Loss: 6.057835578918457\n",
      "Epoch 816, Train Loss: 5.0292434281773035, Train Raw Loss: 3.89078019571801, Validation Loss: 6.041730880737305\n",
      "Epoch 817, Train Loss: 5.029207097407844, Train Raw Loss: 3.8861159392529063, Validation Loss: 6.064631462097168\n",
      "Epoch 818, Train Loss: 5.0417327431341015, Train Raw Loss: 3.8912417391108143, Validation Loss: 6.036398887634277\n",
      "Epoch 819, Train Loss: 5.040262420641051, Train Raw Loss: 3.8970106607509982, Validation Loss: 6.04568338394165\n",
      "Epoch 820, Train Loss: 5.028076024684641, Train Raw Loss: 3.8856468707323075, Validation Loss: 6.04463005065918\n",
      "Epoch 821, Train Loss: 5.018297038806809, Train Raw Loss: 3.8823779080890946, Validation Loss: 6.0488715171813965\n",
      "Epoch 822, Train Loss: 5.023216562055879, Train Raw Loss: 3.885420308717423, Validation Loss: 6.03040885925293\n",
      "Epoch 823, Train Loss: 5.026551615695158, Train Raw Loss: 3.88687772643235, Validation Loss: 6.060285568237305\n",
      "Epoch 824, Train Loss: 5.030993764268027, Train Raw Loss: 3.8903100688838297, Validation Loss: 6.042273044586182\n",
      "Epoch 825, Train Loss: 5.027085312290324, Train Raw Loss: 3.8872426960617306, Validation Loss: 6.042877197265625\n",
      "Epoch 826, Train Loss: 5.023770548237694, Train Raw Loss: 3.8864875103036565, Validation Loss: 6.043793201446533\n",
      "Epoch 827, Train Loss: 5.01871059363087, Train Raw Loss: 3.8808373922275172, Validation Loss: 6.059581279754639\n",
      "Epoch 828, Train Loss: 5.023215356055233, Train Raw Loss: 3.8830162529316214, Validation Loss: 6.033153057098389\n",
      "Epoch 829, Train Loss: 5.020527109669315, Train Raw Loss: 3.883771662124329, Validation Loss: 6.044071674346924\n",
      "Epoch 830, Train Loss: 5.013442253073056, Train Raw Loss: 3.879912940536936, Validation Loss: 6.0710930824279785\n",
      "Epoch 831, Train Loss: 5.0110029693279, Train Raw Loss: 3.8765167094353172, Validation Loss: 6.0352864265441895\n",
      "Epoch 832, Train Loss: 5.024913084258636, Train Raw Loss: 3.891899182357722, Validation Loss: 6.047644138336182\n",
      "Epoch 833, Train Loss: 5.023515459398428, Train Raw Loss: 3.8874605096048778, Validation Loss: 6.047726631164551\n",
      "Epoch 834, Train Loss: 5.011551334957281, Train Raw Loss: 3.879208368808031, Validation Loss: 6.060373783111572\n",
      "Epoch 835, Train Loss: 5.01458546626899, Train Raw Loss: 3.8795677750060955, Validation Loss: 6.038520812988281\n",
      "Epoch 836, Train Loss: 5.016502181109455, Train Raw Loss: 3.8832414930479393, Validation Loss: 6.059466361999512\n",
      "Epoch 837, Train Loss: 5.014041643258598, Train Raw Loss: 3.8772470369935035, Validation Loss: 6.057352066040039\n",
      "Epoch 838, Train Loss: 5.033223156051504, Train Raw Loss: 3.896529808516304, Validation Loss: 6.028450012207031\n",
      "Epoch 839, Train Loss: 5.023574414932066, Train Raw Loss: 3.883404852160149, Validation Loss: 6.031607627868652\n",
      "Epoch 840, Train Loss: 4.997906116313405, Train Raw Loss: 3.865389606108268, Validation Loss: 6.048634052276611\n",
      "Epoch 841, Train Loss: 5.019171250197623, Train Raw Loss: 3.888494072606166, Validation Loss: 6.033941745758057\n",
      "Epoch 842, Train Loss: 5.009908824496799, Train Raw Loss: 3.8712342063171996, Validation Loss: 6.038132190704346\n",
      "Epoch 843, Train Loss: 5.004348021994034, Train Raw Loss: 3.8753314320825867, Validation Loss: 6.019421577453613\n",
      "Epoch 844, Train Loss: 5.009285455693801, Train Raw Loss: 3.8759774477531512, Validation Loss: 6.002076625823975\n",
      "Epoch 845, Train Loss: 4.997424163172642, Train Raw Loss: 3.8748797704362206, Validation Loss: 6.027638912200928\n",
      "Epoch 846, Train Loss: 5.012762696378761, Train Raw Loss: 3.881995877251029, Validation Loss: 6.033127307891846\n",
      "Epoch 847, Train Loss: 5.006832566526201, Train Raw Loss: 3.8778524506423206, Validation Loss: 6.036632061004639\n",
      "Epoch 848, Train Loss: 5.013097904125849, Train Raw Loss: 3.8833120686726437, Validation Loss: 6.013408660888672\n",
      "Epoch 849, Train Loss: 5.0025836200349865, Train Raw Loss: 3.8795796641045146, Validation Loss: 6.052102088928223\n",
      "Epoch 850, Train Loss: 5.00634595155716, Train Raw Loss: 3.8744436534328592, Validation Loss: 5.99383020401001\n",
      "Epoch 851, Train Loss: 5.002244636002514, Train Raw Loss: 3.870084892089168, Validation Loss: 6.002288818359375\n",
      "Epoch 852, Train Loss: 4.993023868070709, Train Raw Loss: 3.8699925480617416, Validation Loss: 6.0516276359558105\n",
      "Epoch 853, Train Loss: 4.995355516092645, Train Raw Loss: 3.8677005813767513, Validation Loss: 6.024866580963135\n",
      "Epoch 854, Train Loss: 4.991919951554801, Train Raw Loss: 3.871051319812735, Validation Loss: 6.062309265136719\n",
      "Epoch 855, Train Loss: 4.998562485145198, Train Raw Loss: 3.8715852022998862, Validation Loss: 6.064971446990967\n",
      "Epoch 856, Train Loss: 5.022064224878947, Train Raw Loss: 3.8918744094669817, Validation Loss: 6.030157566070557\n",
      "Epoch 857, Train Loss: 5.004063350045019, Train Raw Loss: 3.8754166905250815, Validation Loss: 6.0108962059021\n",
      "Epoch 858, Train Loss: 4.999303001082605, Train Raw Loss: 3.8746847813328107, Validation Loss: 6.004303455352783\n",
      "Epoch 859, Train Loss: 4.984503158595827, Train Raw Loss: 3.864003781312042, Validation Loss: 6.046316146850586\n",
      "Epoch 860, Train Loss: 4.980907041662269, Train Raw Loss: 3.858820435280601, Validation Loss: 6.022313117980957\n",
      "Epoch 861, Train Loss: 4.997057364963823, Train Raw Loss: 3.8747940541141563, Validation Loss: 6.022687911987305\n",
      "Epoch 862, Train Loss: 4.994042835384607, Train Raw Loss: 3.8738610453075832, Validation Loss: 6.037308216094971\n",
      "Epoch 863, Train Loss: 4.999270637912883, Train Raw Loss: 3.8758002368526325, Validation Loss: 6.012416362762451\n",
      "Epoch 864, Train Loss: 4.9936035272975765, Train Raw Loss: 3.869730702539285, Validation Loss: 6.007372856140137\n",
      "Epoch 865, Train Loss: 4.965220672057734, Train Raw Loss: 3.847652985403935, Validation Loss: 6.034172058105469\n",
      "Epoch 866, Train Loss: 4.998926088462273, Train Raw Loss: 3.8751542827321424, Validation Loss: 6.033722400665283\n",
      "Epoch 867, Train Loss: 4.992365417546696, Train Raw Loss: 3.8702715256975755, Validation Loss: 5.9891037940979\n",
      "Epoch 868, Train Loss: 4.989436304734813, Train Raw Loss: 3.871281953942445, Validation Loss: 5.987002372741699\n",
      "Epoch 869, Train Loss: 4.982085067696041, Train Raw Loss: 3.8618431319379143, Validation Loss: 6.0051445960998535\n",
      "Epoch 870, Train Loss: 4.987643866489331, Train Raw Loss: 3.8687365966124667, Validation Loss: 6.016834735870361\n",
      "Epoch 871, Train Loss: 4.983966613312562, Train Raw Loss: 3.8624924735890493, Validation Loss: 6.020034313201904\n",
      "Epoch 872, Train Loss: 4.9952551055285666, Train Raw Loss: 3.8740794581671554, Validation Loss: 6.003503322601318\n",
      "Epoch 873, Train Loss: 4.976250053942204, Train Raw Loss: 3.856732229143381, Validation Loss: 6.028544902801514\n",
      "Epoch 874, Train Loss: 4.9968377736707525, Train Raw Loss: 3.874730085126228, Validation Loss: 6.0203857421875\n",
      "Epoch 875, Train Loss: 4.986827631377512, Train Raw Loss: 3.8674940026882623, Validation Loss: 6.0052666664123535\n",
      "Epoch 876, Train Loss: 4.998834813800123, Train Raw Loss: 3.881756568534507, Validation Loss: 6.016100883483887\n",
      "Epoch 877, Train Loss: 4.985993505186505, Train Raw Loss: 3.8671967874384587, Validation Loss: 6.043557167053223\n",
      "Epoch 878, Train Loss: 4.991596591720978, Train Raw Loss: 3.8669673671325047, Validation Loss: 6.029414653778076\n",
      "Epoch 879, Train Loss: 4.996141973551777, Train Raw Loss: 3.8777179634819428, Validation Loss: 6.025233745574951\n",
      "Epoch 880, Train Loss: 4.976832805656724, Train Raw Loss: 3.853583007968134, Validation Loss: 6.031899452209473\n",
      "Epoch 881, Train Loss: 4.984425525036123, Train Raw Loss: 3.8631580046067637, Validation Loss: 6.025984764099121\n",
      "Epoch 882, Train Loss: 4.9984442575938175, Train Raw Loss: 3.876799331481258, Validation Loss: 6.006687641143799\n",
      "Epoch 883, Train Loss: 4.984745324403048, Train Raw Loss: 3.866814540864693, Validation Loss: 5.96626091003418\n",
      "Epoch 884, Train Loss: 4.966554311414559, Train Raw Loss: 3.8494859193762143, Validation Loss: 6.017038345336914\n",
      "Epoch 885, Train Loss: 4.979334699859222, Train Raw Loss: 3.8648983145753544, Validation Loss: 6.006775379180908\n",
      "Epoch 886, Train Loss: 4.973410767234034, Train Raw Loss: 3.851443221916755, Validation Loss: 5.965916156768799\n",
      "Epoch 887, Train Loss: 4.967604408827093, Train Raw Loss: 3.85529276492695, Validation Loss: 6.034974575042725\n",
      "Epoch 888, Train Loss: 4.966407261540493, Train Raw Loss: 3.8498559184786347, Validation Loss: 6.026541709899902\n",
      "Epoch 889, Train Loss: 4.974582721706894, Train Raw Loss: 3.8545109735180936, Validation Loss: 6.011239051818848\n",
      "Epoch 890, Train Loss: 4.973744743317366, Train Raw Loss: 3.85952483481831, Validation Loss: 5.9846014976501465\n",
      "Epoch 891, Train Loss: 4.974439828511741, Train Raw Loss: 3.8575528455691206, Validation Loss: 5.973435401916504\n",
      "Epoch 892, Train Loss: 4.95514847561717, Train Raw Loss: 3.8485511928796767, Validation Loss: 6.0089240074157715\n",
      "Epoch 893, Train Loss: 4.978136377367709, Train Raw Loss: 3.8624674525939757, Validation Loss: 6.002923488616943\n",
      "Epoch 894, Train Loss: 4.9866510291066435, Train Raw Loss: 3.871245321300295, Validation Loss: 6.005218029022217\n",
      "Epoch 895, Train Loss: 4.970069536649518, Train Raw Loss: 3.8544014477481445, Validation Loss: 6.033617973327637\n",
      "Epoch 896, Train Loss: 4.969953028774924, Train Raw Loss: 3.8578306645154954, Validation Loss: 6.009420394897461\n",
      "Epoch 897, Train Loss: 4.9673219164212545, Train Raw Loss: 3.8538341412941617, Validation Loss: 5.9846110343933105\n",
      "Epoch 898, Train Loss: 4.961241819957892, Train Raw Loss: 3.849265225521392, Validation Loss: 5.9918742179870605\n",
      "Epoch 899, Train Loss: 4.971268782267968, Train Raw Loss: 3.856613630014989, Validation Loss: 5.9930033683776855\n",
      "Epoch 900, Train Loss: 4.966365385966169, Train Raw Loss: 3.850316635519266, Validation Loss: 6.015265464782715\n",
      "Epoch 901, Train Loss: 4.962767311102814, Train Raw Loss: 3.853557490019335, Validation Loss: 6.026243209838867\n",
      "Epoch 902, Train Loss: 4.9734727217919295, Train Raw Loss: 3.8580273093034823, Validation Loss: 6.000427722930908\n",
      "Epoch 903, Train Loss: 4.967728706283702, Train Raw Loss: 3.85650470248527, Validation Loss: 6.002521991729736\n",
      "Epoch 904, Train Loss: 4.970273000995318, Train Raw Loss: 3.8556957718812757, Validation Loss: 5.97576904296875\n",
      "Epoch 905, Train Loss: 4.965935372892353, Train Raw Loss: 3.857101181977325, Validation Loss: 5.982535362243652\n",
      "Epoch 906, Train Loss: 4.936287203513913, Train Raw Loss: 3.832693730915586, Validation Loss: 6.008784294128418\n",
      "Epoch 907, Train Loss: 4.963898753705952, Train Raw Loss: 3.8560913166652124, Validation Loss: 6.033812999725342\n",
      "Epoch 908, Train Loss: 4.9609218834175, Train Raw Loss: 3.8504628838764297, Validation Loss: 6.025917053222656\n",
      "Epoch 909, Train Loss: 4.971901724156406, Train Raw Loss: 3.860438921385341, Validation Loss: 5.99202823638916\n",
      "Epoch 910, Train Loss: 4.959646342198054, Train Raw Loss: 3.853250521669785, Validation Loss: 6.015004634857178\n",
      "Epoch 911, Train Loss: 4.962673953341113, Train Raw Loss: 3.853344667206208, Validation Loss: 5.991714954376221\n",
      "Epoch 912, Train Loss: 4.96989452002777, Train Raw Loss: 3.8616748781667813, Validation Loss: 5.999106407165527\n",
      "Epoch 913, Train Loss: 4.9773758540550865, Train Raw Loss: 3.8653375984066063, Validation Loss: 5.977245330810547\n",
      "Epoch 914, Train Loss: 4.951342651661899, Train Raw Loss: 3.8482753881977665, Validation Loss: 6.0286126136779785\n",
      "Epoch 915, Train Loss: 4.972578505343861, Train Raw Loss: 3.8611614580369658, Validation Loss: 6.008700847625732\n",
      "Epoch 916, Train Loss: 4.951811743941572, Train Raw Loss: 3.842407264974382, Validation Loss: 5.998016834259033\n",
      "Epoch 917, Train Loss: 4.946319022526343, Train Raw Loss: 3.839549685145418, Validation Loss: 6.013130187988281\n",
      "Epoch 918, Train Loss: 4.96330764260557, Train Raw Loss: 3.8470011451592048, Validation Loss: 5.976894855499268\n",
      "Epoch 919, Train Loss: 4.959147211164236, Train Raw Loss: 3.8518042029605972, Validation Loss: 6.005051136016846\n",
      "Epoch 920, Train Loss: 4.94814330736796, Train Raw Loss: 3.8452872744450968, Validation Loss: 5.988874435424805\n",
      "Epoch 921, Train Loss: 4.951640862723192, Train Raw Loss: 3.839978305209014, Validation Loss: 5.987852573394775\n",
      "Epoch 922, Train Loss: 4.951566723982493, Train Raw Loss: 3.849237004501952, Validation Loss: 6.007227897644043\n",
      "Epoch 923, Train Loss: 4.958811976512274, Train Raw Loss: 3.851768731077512, Validation Loss: 5.96820068359375\n",
      "Epoch 924, Train Loss: 4.952926296657986, Train Raw Loss: 3.847627733482255, Validation Loss: 5.977935791015625\n",
      "Epoch 925, Train Loss: 4.944707809223069, Train Raw Loss: 3.847177863576346, Validation Loss: 5.965146064758301\n",
      "Epoch 926, Train Loss: 4.950330997837915, Train Raw Loss: 3.8485666658729314, Validation Loss: 6.001071453094482\n",
      "Epoch 927, Train Loss: 4.940811321967178, Train Raw Loss: 3.845434116944671, Validation Loss: 5.950773239135742\n",
      "Epoch 928, Train Loss: 4.950999094628625, Train Raw Loss: 3.854551129953729, Validation Loss: 5.946174621582031\n",
      "Epoch 929, Train Loss: 4.938450374040339, Train Raw Loss: 3.8446601397875284, Validation Loss: 5.994632244110107\n",
      "Epoch 930, Train Loss: 4.9426671286424, Train Raw Loss: 3.8476645042084985, Validation Loss: 5.9365410804748535\n",
      "Epoch 931, Train Loss: 4.935343665298489, Train Raw Loss: 3.8450835891895823, Validation Loss: 5.966167449951172\n",
      "Epoch 932, Train Loss: 4.9298779566254884, Train Raw Loss: 3.840879420563579, Validation Loss: 5.9489359855651855\n",
      "Epoch 933, Train Loss: 4.948971010082298, Train Raw Loss: 3.8595175898737377, Validation Loss: 5.95138692855835\n",
      "Epoch 934, Train Loss: 4.941459571818511, Train Raw Loss: 3.8505185807744664, Validation Loss: 5.9517340660095215\n",
      "Epoch 935, Train Loss: 4.929924954556757, Train Raw Loss: 3.843987537879083, Validation Loss: 5.935505390167236\n",
      "Epoch 936, Train Loss: 4.926771777702703, Train Raw Loss: 3.8391390816618998, Validation Loss: 5.9363274574279785\n",
      "Epoch 937, Train Loss: 4.939606805476878, Train Raw Loss: 3.8543375573638414, Validation Loss: 5.934366703033447\n",
      "Epoch 938, Train Loss: 4.936730023142364, Train Raw Loss: 3.856676718965173, Validation Loss: 5.937953472137451\n",
      "Epoch 939, Train Loss: 4.921535458829668, Train Raw Loss: 3.8452532845652767, Validation Loss: 5.967315196990967\n",
      "Epoch 940, Train Loss: 4.925052117059628, Train Raw Loss: 3.8463037270638676, Validation Loss: 5.916228771209717\n",
      "Epoch 941, Train Loss: 4.928169136080477, Train Raw Loss: 3.853207870034708, Validation Loss: 5.926717758178711\n",
      "Epoch 942, Train Loss: 4.921837866637442, Train Raw Loss: 3.8459988560941483, Validation Loss: 5.9298176765441895\n",
      "Epoch 943, Train Loss: 4.9205956739683945, Train Raw Loss: 3.843667046311829, Validation Loss: 5.936680316925049\n",
      "Epoch 944, Train Loss: 4.9199805220796, Train Raw Loss: 3.8491511659075814, Validation Loss: 5.906779766082764\n",
      "Epoch 945, Train Loss: 4.924242391602861, Train Raw Loss: 3.8490707898719445, Validation Loss: 5.918721675872803\n",
      "Epoch 946, Train Loss: 4.934995346350802, Train Raw Loss: 3.859321797556347, Validation Loss: 5.8957953453063965\n",
      "Epoch 947, Train Loss: 4.913252614769671, Train Raw Loss: 3.8437020952916807, Validation Loss: 5.878371238708496\n",
      "Epoch 948, Train Loss: 4.895601625740528, Train Raw Loss: 3.8309223078605203, Validation Loss: 5.897249221801758\n",
      "Epoch 949, Train Loss: 4.898692334857252, Train Raw Loss: 3.8329323140283424, Validation Loss: 5.901214599609375\n",
      "Epoch 950, Train Loss: 4.911105378468831, Train Raw Loss: 3.8495240353047846, Validation Loss: 5.884788513183594\n",
      "Epoch 951, Train Loss: 4.904838890830676, Train Raw Loss: 3.84026600126591, Validation Loss: 5.884093761444092\n",
      "Epoch 952, Train Loss: 4.9064038294884895, Train Raw Loss: 3.8455332291209037, Validation Loss: 5.907779216766357\n",
      "Epoch 953, Train Loss: 4.918781574153238, Train Raw Loss: 3.851727917790413, Validation Loss: 5.8667120933532715\n",
      "Epoch 954, Train Loss: 4.917027209202448, Train Raw Loss: 3.8544436376541853, Validation Loss: 5.8466572761535645\n",
      "Epoch 955, Train Loss: 4.893771749486526, Train Raw Loss: 3.8398885894566774, Validation Loss: 5.897792339324951\n",
      "Epoch 956, Train Loss: 4.913800362497568, Train Raw Loss: 3.8488848599294823, Validation Loss: 5.892942428588867\n",
      "Epoch 957, Train Loss: 4.909166649811798, Train Raw Loss: 3.8531169238603775, Validation Loss: 5.890676498413086\n",
      "Epoch 958, Train Loss: 4.906039946609074, Train Raw Loss: 3.8464049502793287, Validation Loss: 5.942098617553711\n",
      "Epoch 959, Train Loss: 4.912390825069613, Train Raw Loss: 3.8535814724862574, Validation Loss: 5.867627143859863\n",
      "Epoch 960, Train Loss: 4.9009917823804745, Train Raw Loss: 3.8482613549464277, Validation Loss: 5.876791954040527\n",
      "Epoch 961, Train Loss: 4.8957835746308165, Train Raw Loss: 3.845003693799178, Validation Loss: 5.891140937805176\n",
      "Epoch 962, Train Loss: 4.897169895056221, Train Raw Loss: 3.844645210603873, Validation Loss: 5.881791114807129\n",
      "Epoch 963, Train Loss: 4.9002414368920855, Train Raw Loss: 3.848692189405362, Validation Loss: 5.855031490325928\n",
      "Epoch 964, Train Loss: 4.880367916491297, Train Raw Loss: 3.8293443233602575, Validation Loss: 5.870828628540039\n",
      "Epoch 965, Train Loss: 4.887356387409899, Train Raw Loss: 3.8405168682750728, Validation Loss: 5.8696770668029785\n",
      "Epoch 966, Train Loss: 4.88840551947554, Train Raw Loss: 3.84506259991063, Validation Loss: 5.857705116271973\n",
      "Epoch 967, Train Loss: 4.887562533468008, Train Raw Loss: 3.841744679916236, Validation Loss: 5.8503193855285645\n",
      "Epoch 968, Train Loss: 4.879627377291521, Train Raw Loss: 3.8391840029507875, Validation Loss: 5.862303256988525\n",
      "Epoch 969, Train Loss: 4.886939204649793, Train Raw Loss: 3.8431934817383686, Validation Loss: 5.869019031524658\n",
      "Epoch 970, Train Loss: 4.89038513575991, Train Raw Loss: 3.8495108620574077, Validation Loss: 5.85419225692749\n",
      "Epoch 971, Train Loss: 4.868922929051849, Train Raw Loss: 3.8338366013020275, Validation Loss: 5.868032455444336\n",
      "Epoch 972, Train Loss: 4.877553208834595, Train Raw Loss: 3.8402265756494467, Validation Loss: 5.913050651550293\n",
      "Epoch 973, Train Loss: 4.870939664873812, Train Raw Loss: 3.8361870453589493, Validation Loss: 5.892940044403076\n",
      "Epoch 974, Train Loss: 4.88207372451822, Train Raw Loss: 3.8468715728984937, Validation Loss: 5.87012243270874\n",
      "Epoch 975, Train Loss: 4.867525062047773, Train Raw Loss: 3.835345793929365, Validation Loss: 5.843279838562012\n",
      "Epoch 976, Train Loss: 4.878536228090525, Train Raw Loss: 3.8477150891804035, Validation Loss: 5.855118751525879\n",
      "Epoch 977, Train Loss: 4.868158524317874, Train Raw Loss: 3.838952960777614, Validation Loss: 5.859784126281738\n",
      "Epoch 978, Train Loss: 4.865337505936623, Train Raw Loss: 3.8326259466509023, Validation Loss: 5.860727787017822\n",
      "Epoch 979, Train Loss: 4.862418249911732, Train Raw Loss: 3.8325864357252915, Validation Loss: 5.826424598693848\n",
      "Epoch 980, Train Loss: 4.874117570122083, Train Raw Loss: 3.846391318158971, Validation Loss: 5.83236026763916\n",
      "Epoch 981, Train Loss: 4.872390627943807, Train Raw Loss: 3.8486908355934752, Validation Loss: 5.82352352142334\n",
      "Epoch 982, Train Loss: 4.859460208647781, Train Raw Loss: 3.8331433922880227, Validation Loss: 5.8824076652526855\n",
      "Epoch 983, Train Loss: 4.862427905864186, Train Raw Loss: 3.840319748305612, Validation Loss: 5.8363518714904785\n",
      "Epoch 984, Train Loss: 4.861018382675118, Train Raw Loss: 3.838628358973397, Validation Loss: 5.840610980987549\n",
      "Epoch 985, Train Loss: 4.858783184240262, Train Raw Loss: 3.8390124501039584, Validation Loss: 5.819209098815918\n",
      "Epoch 986, Train Loss: 4.865114114433527, Train Raw Loss: 3.8452961104197634, Validation Loss: 5.817922115325928\n",
      "Epoch 987, Train Loss: 4.859496642814742, Train Raw Loss: 3.8397916406806973, Validation Loss: 5.823520183563232\n",
      "Epoch 988, Train Loss: 4.847902413126495, Train Raw Loss: 3.832115833544069, Validation Loss: 5.861278057098389\n",
      "Epoch 989, Train Loss: 4.858300613529153, Train Raw Loss: 3.8381956360406346, Validation Loss: 5.818520545959473\n",
      "Epoch 990, Train Loss: 4.8608130841619435, Train Raw Loss: 3.842468672287133, Validation Loss: 5.826737403869629\n",
      "Epoch 991, Train Loss: 4.854322383138869, Train Raw Loss: 3.8373891493926444, Validation Loss: 5.815246105194092\n",
      "Epoch 992, Train Loss: 4.859973776423269, Train Raw Loss: 3.8474165512455833, Validation Loss: 5.823792934417725\n",
      "Epoch 993, Train Loss: 4.853855282730526, Train Raw Loss: 3.8411191765632893, Validation Loss: 5.8228230476379395\n",
      "Epoch 994, Train Loss: 4.846929123914904, Train Raw Loss: 3.8247483977427086, Validation Loss: 5.830415725708008\n",
      "Epoch 995, Train Loss: 4.859229669885503, Train Raw Loss: 3.845299127449592, Validation Loss: 5.803975582122803\n",
      "Epoch 996, Train Loss: 4.843112164984147, Train Raw Loss: 3.837590683913893, Validation Loss: 5.841247081756592\n",
      "Epoch 997, Train Loss: 4.867208996829059, Train Raw Loss: 3.8454365123063328, Validation Loss: 5.817317008972168\n",
      "Epoch 998, Train Loss: 4.854008573045333, Train Raw Loss: 3.840980683018764, Validation Loss: 5.819766521453857\n",
      "Epoch 999, Train Loss: 4.855282329022884, Train Raw Loss: 3.84417749510871, Validation Loss: 5.798488140106201\n",
      "Epoch 1000, Train Loss: 4.83764297341307, Train Raw Loss: 3.8247878315962023, Validation Loss: 5.823797702789307\n",
      "Epoch 1001, Train Loss: 4.842908318009641, Train Raw Loss: 3.8360231169395975, Validation Loss: 5.833057880401611\n",
      "Epoch 1002, Train Loss: 4.8547454371220535, Train Raw Loss: 3.8447251120789185, Validation Loss: 5.7889275550842285\n",
      "Epoch 1003, Train Loss: 4.839774494204256, Train Raw Loss: 3.830175915526019, Validation Loss: 5.781697750091553\n",
      "Epoch 1004, Train Loss: 4.823369854191939, Train Raw Loss: 3.8210312979088887, Validation Loss: 5.794871807098389\n",
      "Epoch 1005, Train Loss: 4.841327664090527, Train Raw Loss: 3.8354918839202985, Validation Loss: 5.810230255126953\n",
      "Epoch 1006, Train Loss: 4.835677793704801, Train Raw Loss: 3.829844302808245, Validation Loss: 5.795892238616943\n",
      "Epoch 1007, Train Loss: 4.851965287741687, Train Raw Loss: 3.8484807953652407, Validation Loss: 5.7632832527160645\n",
      "Epoch 1008, Train Loss: 4.833041057570113, Train Raw Loss: 3.8338966806315713, Validation Loss: 5.797537803649902\n",
      "Epoch 1009, Train Loss: 4.822515659530958, Train Raw Loss: 3.824539622788628, Validation Loss: 5.804507732391357\n",
      "Epoch 1010, Train Loss: 4.849326026522451, Train Raw Loss: 3.8448210647536647, Validation Loss: 5.787345886230469\n",
      "Epoch 1011, Train Loss: 4.8409027780923575, Train Raw Loss: 3.841341634136107, Validation Loss: 5.768452167510986\n",
      "Epoch 1012, Train Loss: 4.832267142832279, Train Raw Loss: 3.8369653315593797, Validation Loss: 5.802433490753174\n",
      "Epoch 1013, Train Loss: 4.840663357989656, Train Raw Loss: 3.839947899306814, Validation Loss: 5.799055099487305\n",
      "Epoch 1014, Train Loss: 4.837745751440525, Train Raw Loss: 3.8379510223037667, Validation Loss: 5.762552738189697\n",
      "Epoch 1015, Train Loss: 4.845765467153655, Train Raw Loss: 3.842451685460077, Validation Loss: 5.7715163230896\n",
      "Epoch 1016, Train Loss: 4.82063882475098, Train Raw Loss: 3.8266936495900152, Validation Loss: 5.804485321044922\n",
      "Epoch 1017, Train Loss: 4.827015454404884, Train Raw Loss: 3.8289734875162442, Validation Loss: 5.796335697174072\n",
      "Epoch 1018, Train Loss: 4.833979316221344, Train Raw Loss: 3.8342290814965962, Validation Loss: 5.814136505126953\n",
      "Epoch 1019, Train Loss: 4.8340378713276655, Train Raw Loss: 3.8381338904301328, Validation Loss: 5.78255558013916\n",
      "Epoch 1020, Train Loss: 4.820692641950316, Train Raw Loss: 3.8295638403544823, Validation Loss: 5.8127121925354\n",
      "Epoch 1021, Train Loss: 4.840540279282464, Train Raw Loss: 3.8405670958260694, Validation Loss: 5.780492305755615\n",
      "Epoch 1022, Train Loss: 4.8335479878717, Train Raw Loss: 3.8327641284714145, Validation Loss: 5.79111385345459\n",
      "Epoch 1023, Train Loss: 4.816004592842526, Train Raw Loss: 3.824612563558751, Validation Loss: 5.760906219482422\n",
      "Epoch 1024, Train Loss: 4.830311029156049, Train Raw Loss: 3.836376061456071, Validation Loss: 5.784027576446533\n",
      "Epoch 1025, Train Loss: 4.832357977082332, Train Raw Loss: 3.840489579447442, Validation Loss: 5.795806407928467\n",
      "Epoch 1026, Train Loss: 4.845456850280365, Train Raw Loss: 3.8480642336524196, Validation Loss: 5.783261299133301\n",
      "Epoch 1027, Train Loss: 4.81267207124167, Train Raw Loss: 3.823020334252053, Validation Loss: 5.786495685577393\n",
      "Epoch 1028, Train Loss: 4.818728243642383, Train Raw Loss: 3.8284833495815596, Validation Loss: 5.7712788581848145\n",
      "Epoch 1029, Train Loss: 4.82767419434256, Train Raw Loss: 3.834316801859273, Validation Loss: 5.757021903991699\n",
      "Epoch 1030, Train Loss: 4.825678126679526, Train Raw Loss: 3.8337363405774036, Validation Loss: 5.797000408172607\n",
      "Epoch 1031, Train Loss: 4.823909869790077, Train Raw Loss: 3.833425370686584, Validation Loss: 5.789387226104736\n",
      "Epoch 1032, Train Loss: 4.8246598970558905, Train Raw Loss: 3.8355100450002486, Validation Loss: 5.76103401184082\n",
      "Epoch 1033, Train Loss: 4.814747915251387, Train Raw Loss: 3.831466997952925, Validation Loss: 5.748952388763428\n",
      "Epoch 1034, Train Loss: 4.816266146798928, Train Raw Loss: 3.8271152577052514, Validation Loss: 5.770077705383301\n",
      "Epoch 1035, Train Loss: 4.8127542343404555, Train Raw Loss: 3.826391901820898, Validation Loss: 5.770257472991943\n",
      "Epoch 1036, Train Loss: 4.825334662033452, Train Raw Loss: 3.8396652149243486, Validation Loss: 5.780145168304443\n",
      "Epoch 1037, Train Loss: 4.830693911347124, Train Raw Loss: 3.8457354932609533, Validation Loss: 5.757713317871094\n",
      "Epoch 1038, Train Loss: 4.82410190610422, Train Raw Loss: 3.83620031538109, Validation Loss: 5.749691963195801\n",
      "Epoch 1039, Train Loss: 4.812717697934972, Train Raw Loss: 3.8284093336098723, Validation Loss: 5.783992290496826\n",
      "Epoch 1040, Train Loss: 4.821711045586401, Train Raw Loss: 3.8338588427338336, Validation Loss: 5.755585193634033\n",
      "Epoch 1041, Train Loss: 4.812908372696903, Train Raw Loss: 3.8280512830449474, Validation Loss: 5.745486736297607\n",
      "Epoch 1042, Train Loss: 4.809656138304207, Train Raw Loss: 3.832928903649251, Validation Loss: 5.796651363372803\n",
      "Epoch 1043, Train Loss: 4.831672005189789, Train Raw Loss: 3.845094862497515, Validation Loss: 5.774696350097656\n",
      "Epoch 1044, Train Loss: 4.812316439714697, Train Raw Loss: 3.8313954287519056, Validation Loss: 5.776966094970703\n",
      "Epoch 1045, Train Loss: 4.835902718289031, Train Raw Loss: 3.849548992059297, Validation Loss: 5.740711212158203\n",
      "Epoch 1046, Train Loss: 4.82468996602628, Train Raw Loss: 3.8369503972431023, Validation Loss: 5.748850345611572\n",
      "Epoch 1047, Train Loss: 4.801686120116049, Train Raw Loss: 3.819249758703841, Validation Loss: 5.773547172546387\n",
      "Epoch 1048, Train Loss: 4.80097471666005, Train Raw Loss: 3.823149495902989, Validation Loss: 5.765970706939697\n",
      "Epoch 1049, Train Loss: 4.816346473826302, Train Raw Loss: 3.833518781968289, Validation Loss: 5.741865158081055\n",
      "Epoch 1050, Train Loss: 4.8133171628746725, Train Raw Loss: 3.8314530932654938, Validation Loss: 5.748377323150635\n",
      "Epoch 1051, Train Loss: 4.808868099335167, Train Raw Loss: 3.827874315240317, Validation Loss: 5.741322994232178\n",
      "Epoch 1052, Train Loss: 4.806313486976756, Train Raw Loss: 3.830036748242047, Validation Loss: 5.7504096031188965\n",
      "Epoch 1053, Train Loss: 4.8219644783271685, Train Raw Loss: 3.8387838129368093, Validation Loss: 5.750043869018555\n",
      "Epoch 1054, Train Loss: 4.807288336339924, Train Raw Loss: 3.82613758051561, Validation Loss: 5.733476638793945\n",
      "Epoch 1055, Train Loss: 4.804433535287777, Train Raw Loss: 3.8263262368738653, Validation Loss: 5.777127265930176\n",
      "Epoch 1056, Train Loss: 4.80421232432127, Train Raw Loss: 3.8272097959286637, Validation Loss: 5.755096912384033\n",
      "Epoch 1057, Train Loss: 4.818376290301482, Train Raw Loss: 3.8443183150556353, Validation Loss: 5.774774074554443\n",
      "Epoch 1058, Train Loss: 4.808928653928969, Train Raw Loss: 3.830465019908216, Validation Loss: 5.757659435272217\n",
      "Epoch 1059, Train Loss: 4.799476710375813, Train Raw Loss: 3.8269049139072497, Validation Loss: 5.733710289001465\n",
      "Epoch 1060, Train Loss: 4.79894481855962, Train Raw Loss: 3.823748684467541, Validation Loss: 5.752880096435547\n",
      "Epoch 1061, Train Loss: 4.795001731233464, Train Raw Loss: 3.8209258589893578, Validation Loss: 5.711048126220703\n",
      "Epoch 1062, Train Loss: 4.786979187362724, Train Raw Loss: 3.8165056202560663, Validation Loss: 5.7323079109191895\n",
      "Epoch 1063, Train Loss: 4.798245137929916, Train Raw Loss: 3.8237209955437317, Validation Loss: 5.770209312438965\n",
      "Epoch 1064, Train Loss: 4.810227680951357, Train Raw Loss: 3.8378201603889464, Validation Loss: 5.743656635284424\n",
      "Epoch 1065, Train Loss: 4.791484165688356, Train Raw Loss: 3.821460546594527, Validation Loss: 5.745859146118164\n",
      "Epoch 1066, Train Loss: 4.810585749397675, Train Raw Loss: 3.8388465384228363, Validation Loss: 5.765187740325928\n",
      "Epoch 1067, Train Loss: 4.796445312019851, Train Raw Loss: 3.824682404887345, Validation Loss: 5.746993541717529\n",
      "Epoch 1068, Train Loss: 4.8065596868594485, Train Raw Loss: 3.8384751763194798, Validation Loss: 5.766243934631348\n",
      "Epoch 1069, Train Loss: 4.807507685157987, Train Raw Loss: 3.8367212439990706, Validation Loss: 5.749136924743652\n",
      "Epoch 1070, Train Loss: 4.80698229091035, Train Raw Loss: 3.8401001847452587, Validation Loss: 5.740015506744385\n",
      "Epoch 1071, Train Loss: 4.810462424407403, Train Raw Loss: 3.8442691676732568, Validation Loss: 5.767168998718262\n",
      "Epoch 1072, Train Loss: 4.799494968603055, Train Raw Loss: 3.82989087626338, Validation Loss: 5.731701850891113\n",
      "Epoch 1073, Train Loss: 4.793881325258149, Train Raw Loss: 3.8275610217203697, Validation Loss: 5.702456951141357\n",
      "Epoch 1074, Train Loss: 4.789402127100361, Train Raw Loss: 3.820817039368881, Validation Loss: 5.716244697570801\n",
      "Epoch 1075, Train Loss: 4.792554952700933, Train Raw Loss: 3.8252709166457257, Validation Loss: 5.711880683898926\n",
      "Epoch 1076, Train Loss: 4.768974141528209, Train Raw Loss: 3.8063539020717143, Validation Loss: 5.802865028381348\n",
      "Epoch 1077, Train Loss: 4.804900970227188, Train Raw Loss: 3.836359440121386, Validation Loss: 5.77717399597168\n",
      "Epoch 1078, Train Loss: 4.807626160896486, Train Raw Loss: 3.8391865972429513, Validation Loss: 5.742823600769043\n",
      "Epoch 1079, Train Loss: 4.806694878472222, Train Raw Loss: 3.839912388349573, Validation Loss: 5.7240214347839355\n",
      "Epoch 1080, Train Loss: 4.779737734877401, Train Raw Loss: 3.822460188344121, Validation Loss: 5.75678825378418\n",
      "Epoch 1081, Train Loss: 4.813986401342683, Train Raw Loss: 3.838393233136998, Validation Loss: 5.68864631652832\n",
      "Epoch 1082, Train Loss: 4.775464421841833, Train Raw Loss: 3.8117606174614695, Validation Loss: 5.74279260635376\n",
      "Epoch 1083, Train Loss: 4.785863124827544, Train Raw Loss: 3.8248025241825316, Validation Loss: 5.734654426574707\n",
      "Epoch 1084, Train Loss: 4.788837148249149, Train Raw Loss: 3.8269668005820776, Validation Loss: 5.722212791442871\n",
      "Epoch 1085, Train Loss: 4.768728202581405, Train Raw Loss: 3.811262701575955, Validation Loss: 5.728103160858154\n",
      "Epoch 1086, Train Loss: 4.7920336248974005, Train Raw Loss: 3.8330207317239706, Validation Loss: 5.71434211730957\n",
      "Epoch 1087, Train Loss: 4.797450722588433, Train Raw Loss: 3.8372292332351208, Validation Loss: 5.719707489013672\n",
      "Epoch 1088, Train Loss: 4.794198844581842, Train Raw Loss: 3.8341415885421966, Validation Loss: 5.695785999298096\n",
      "Epoch 1089, Train Loss: 4.790876022726297, Train Raw Loss: 3.831163284058372, Validation Loss: 5.753216743469238\n",
      "Epoch 1090, Train Loss: 4.782790775514311, Train Raw Loss: 3.821832498204377, Validation Loss: 5.734808921813965\n",
      "Epoch 1091, Train Loss: 4.793334177633127, Train Raw Loss: 3.835254118591547, Validation Loss: 5.73210334777832\n",
      "Epoch 1092, Train Loss: 4.787664451615678, Train Raw Loss: 3.827388836857345, Validation Loss: 5.725980758666992\n",
      "Epoch 1093, Train Loss: 4.788682063834535, Train Raw Loss: 3.832039643286003, Validation Loss: 5.714691162109375\n",
      "Epoch 1094, Train Loss: 4.785788500308991, Train Raw Loss: 3.8286169403129153, Validation Loss: 5.739947319030762\n",
      "Epoch 1095, Train Loss: 4.801194558872117, Train Raw Loss: 3.835341422011455, Validation Loss: 5.683390140533447\n",
      "Epoch 1096, Train Loss: 4.779439303527275, Train Raw Loss: 3.820125888246629, Validation Loss: 5.717856407165527\n",
      "Epoch 1097, Train Loss: 4.787463038166364, Train Raw Loss: 3.8298156773050627, Validation Loss: 5.703073978424072\n",
      "Epoch 1098, Train Loss: 4.785922427061531, Train Raw Loss: 3.829018895328045, Validation Loss: 5.7136359214782715\n",
      "Epoch 1099, Train Loss: 4.7801394398841595, Train Raw Loss: 3.8286583902521265, Validation Loss: 5.708523750305176\n",
      "Epoch 1100, Train Loss: 4.775618068791098, Train Raw Loss: 3.819068834723698, Validation Loss: 5.70874547958374\n",
      "Epoch 1101, Train Loss: 4.7800564676523205, Train Raw Loss: 3.8266190701474745, Validation Loss: 5.717670917510986\n",
      "Epoch 1102, Train Loss: 4.786919955495331, Train Raw Loss: 3.8319836248126293, Validation Loss: 5.712626934051514\n",
      "Epoch 1103, Train Loss: 4.765979748798741, Train Raw Loss: 3.8129833868808216, Validation Loss: 5.7264580726623535\n",
      "Epoch 1104, Train Loss: 4.783161318881644, Train Raw Loss: 3.826999798334307, Validation Loss: 5.666159152984619\n",
      "Epoch 1105, Train Loss: 4.762283906671736, Train Raw Loss: 3.815258324311839, Validation Loss: 5.731089115142822\n",
      "Epoch 1106, Train Loss: 4.77700241903464, Train Raw Loss: 3.828714576032427, Validation Loss: 5.682523727416992\n",
      "Epoch 1107, Train Loss: 4.77207886742221, Train Raw Loss: 3.8214001830253337, Validation Loss: 5.727593421936035\n",
      "Epoch 1108, Train Loss: 4.774988700863388, Train Raw Loss: 3.8272281364434297, Validation Loss: 5.697231769561768\n",
      "Epoch 1109, Train Loss: 4.7787753120064735, Train Raw Loss: 3.825865789420075, Validation Loss: 5.692429065704346\n",
      "Epoch 1110, Train Loss: 4.759282786895832, Train Raw Loss: 3.8152105490780537, Validation Loss: 5.723006725311279\n",
      "Epoch 1111, Train Loss: 4.775790460573303, Train Raw Loss: 3.827780751677023, Validation Loss: 5.70351505279541\n",
      "Epoch 1112, Train Loss: 4.7730617658959495, Train Raw Loss: 3.8268969141774707, Validation Loss: 5.7181620597839355\n",
      "Epoch 1113, Train Loss: 4.76449817866087, Train Raw Loss: 3.818203830387857, Validation Loss: 5.713481903076172\n",
      "Epoch 1114, Train Loss: 4.787194359385305, Train Raw Loss: 3.835275464546349, Validation Loss: 5.708414554595947\n",
      "Epoch 1115, Train Loss: 4.768253853006495, Train Raw Loss: 3.8172230321913956, Validation Loss: 5.692690849304199\n",
      "Epoch 1116, Train Loss: 4.781321229537328, Train Raw Loss: 3.831744281368123, Validation Loss: 5.711631774902344\n",
      "Epoch 1117, Train Loss: 4.761493867552943, Train Raw Loss: 3.8114702869206667, Validation Loss: 5.702166557312012\n",
      "Epoch 1118, Train Loss: 4.7815883952710365, Train Raw Loss: 3.8335068276358975, Validation Loss: 5.687008857727051\n",
      "Epoch 1119, Train Loss: 4.767323699759112, Train Raw Loss: 3.819229014383422, Validation Loss: 5.704846382141113\n",
      "Epoch 1120, Train Loss: 4.766897666868236, Train Raw Loss: 3.819351941926612, Validation Loss: 5.692906856536865\n",
      "Epoch 1121, Train Loss: 4.766034407334195, Train Raw Loss: 3.821197576324145, Validation Loss: 5.711753845214844\n",
      "Epoch 1122, Train Loss: 4.7747462386886275, Train Raw Loss: 3.8237913243058657, Validation Loss: 5.704880714416504\n",
      "Epoch 1123, Train Loss: 4.761732110712264, Train Raw Loss: 3.8183416784637503, Validation Loss: 5.69546365737915\n",
      "Epoch 1124, Train Loss: 4.776031900528404, Train Raw Loss: 3.8297358850224152, Validation Loss: 5.6846842765808105\n",
      "Epoch 1125, Train Loss: 4.76890093518628, Train Raw Loss: 3.823619112537967, Validation Loss: 5.705825328826904\n",
      "Epoch 1126, Train Loss: 4.760730660292837, Train Raw Loss: 3.8187371969636943, Validation Loss: 5.703005790710449\n",
      "Epoch 1127, Train Loss: 4.772554682609107, Train Raw Loss: 3.826988567080763, Validation Loss: 5.677454948425293\n",
      "Epoch 1128, Train Loss: 4.761270243591732, Train Raw Loss: 3.817366070010596, Validation Loss: 5.712000370025635\n",
      "Epoch 1129, Train Loss: 4.763848890695307, Train Raw Loss: 3.8159091071950066, Validation Loss: 5.702187538146973\n",
      "Epoch 1130, Train Loss: 4.756877634343174, Train Raw Loss: 3.813328548272451, Validation Loss: 5.689634799957275\n",
      "Epoch 1131, Train Loss: 4.764689873821205, Train Raw Loss: 3.823915898344583, Validation Loss: 5.682743549346924\n",
      "Epoch 1132, Train Loss: 4.751328481154309, Train Raw Loss: 3.8076482354766794, Validation Loss: 5.704659461975098\n",
      "Epoch 1133, Train Loss: 4.756163427399264, Train Raw Loss: 3.8162245148171983, Validation Loss: 5.677662372589111\n",
      "Epoch 1134, Train Loss: 4.766817243066099, Train Raw Loss: 3.8223171517252923, Validation Loss: 5.66477632522583\n",
      "Epoch 1135, Train Loss: 4.736618887550301, Train Raw Loss: 3.8019315316031377, Validation Loss: 5.684433460235596\n",
      "Epoch 1136, Train Loss: 4.763681849423382, Train Raw Loss: 3.8203831668943167, Validation Loss: 5.689681529998779\n",
      "Epoch 1137, Train Loss: 4.756025422861179, Train Raw Loss: 3.814843970661362, Validation Loss: 5.689648628234863\n",
      "Epoch 1138, Train Loss: 4.757395108458069, Train Raw Loss: 3.8187073861973153, Validation Loss: 5.672736644744873\n",
      "Epoch 1139, Train Loss: 4.752496375640233, Train Raw Loss: 3.8104375020911294, Validation Loss: 5.679482460021973\n",
      "Epoch 1140, Train Loss: 4.761025211546156, Train Raw Loss: 3.8218929385973346, Validation Loss: 5.682126045227051\n",
      "Epoch 1141, Train Loss: 4.7469249253471695, Train Raw Loss: 3.809755450901058, Validation Loss: 5.672669887542725\n",
      "Epoch 1142, Train Loss: 4.746122049871418, Train Raw Loss: 3.8046968311071394, Validation Loss: 5.691771984100342\n",
      "Epoch 1143, Train Loss: 4.764334387166633, Train Raw Loss: 3.825729941825072, Validation Loss: 5.666252613067627\n",
      "Epoch 1144, Train Loss: 4.754126592063241, Train Raw Loss: 3.815042620152235, Validation Loss: 5.657460689544678\n",
      "Epoch 1145, Train Loss: 4.749153775721789, Train Raw Loss: 3.812703517244922, Validation Loss: 5.664565086364746\n",
      "Epoch 1146, Train Loss: 4.749315050161547, Train Raw Loss: 3.808096233631174, Validation Loss: 5.693571090698242\n",
      "Epoch 1147, Train Loss: 4.7554411808649695, Train Raw Loss: 3.8183471276528307, Validation Loss: 5.695108890533447\n",
      "Epoch 1148, Train Loss: 4.763038278619448, Train Raw Loss: 3.818653384761678, Validation Loss: 5.666545867919922\n",
      "Epoch 1149, Train Loss: 4.75277022363411, Train Raw Loss: 3.81639500082367, Validation Loss: 5.661785125732422\n",
      "Epoch 1150, Train Loss: 4.7443866195778055, Train Raw Loss: 3.8091007160643735, Validation Loss: 5.689793109893799\n",
      "Epoch 1151, Train Loss: 4.741725489083263, Train Raw Loss: 3.8102650709450243, Validation Loss: 5.678394317626953\n",
      "Epoch 1152, Train Loss: 4.744899502396583, Train Raw Loss: 3.8132686742891866, Validation Loss: 5.68657112121582\n",
      "Epoch 1153, Train Loss: 4.7586667069958315, Train Raw Loss: 3.8207818340096207, Validation Loss: 5.644611358642578\n",
      "Epoch 1154, Train Loss: 4.738951694468657, Train Raw Loss: 3.8121759344306256, Validation Loss: 5.682217597961426\n",
      "Epoch 1155, Train Loss: 4.754760943104824, Train Raw Loss: 3.8180354030595884, Validation Loss: 5.679679870605469\n",
      "Epoch 1156, Train Loss: 4.740369163039658, Train Raw Loss: 3.808361420283715, Validation Loss: 5.673280715942383\n",
      "Epoch 1157, Train Loss: 4.749870781020986, Train Raw Loss: 3.817139861153232, Validation Loss: 5.642995834350586\n",
      "Epoch 1158, Train Loss: 4.746279704736339, Train Raw Loss: 3.812969547468755, Validation Loss: 5.664876461029053\n",
      "Epoch 1159, Train Loss: 4.745344447427326, Train Raw Loss: 3.8129853900935915, Validation Loss: 5.645645618438721\n",
      "Epoch 1160, Train Loss: 4.749762938626938, Train Raw Loss: 3.822210226042403, Validation Loss: 5.656886577606201\n",
      "Epoch 1161, Train Loss: 4.748534790012571, Train Raw Loss: 3.8197686691251067, Validation Loss: 5.663031101226807\n",
      "Epoch 1162, Train Loss: 4.753008680873447, Train Raw Loss: 3.8181862444099455, Validation Loss: 5.6727166175842285\n",
      "Epoch 1163, Train Loss: 4.731480425182316, Train Raw Loss: 3.8049519101364746, Validation Loss: 5.678023338317871\n",
      "Epoch 1164, Train Loss: 4.74554637455278, Train Raw Loss: 3.8154167986992333, Validation Loss: 5.655020713806152\n",
      "Epoch 1165, Train Loss: 4.733721977886226, Train Raw Loss: 3.8023882668879296, Validation Loss: 5.66127872467041\n",
      "Epoch 1166, Train Loss: 4.753037180751562, Train Raw Loss: 3.8228643734835916, Validation Loss: 5.671226978302002\n",
      "Epoch 1167, Train Loss: 4.738896710839536, Train Raw Loss: 3.805393357699116, Validation Loss: 5.656278610229492\n",
      "Epoch 1168, Train Loss: 4.745660827226109, Train Raw Loss: 3.816049119871524, Validation Loss: 5.645500183105469\n",
      "Epoch 1169, Train Loss: 4.737189512534274, Train Raw Loss: 3.8104458887957864, Validation Loss: 5.677687168121338\n",
      "Epoch 1170, Train Loss: 4.749005052116182, Train Raw Loss: 3.8160153488318125, Validation Loss: 5.655444622039795\n",
      "Epoch 1171, Train Loss: 4.735357550531626, Train Raw Loss: 3.808799172523949, Validation Loss: 5.666201114654541\n",
      "Epoch 1172, Train Loss: 4.751901658541627, Train Raw Loss: 3.8234169024146265, Validation Loss: 5.645297050476074\n",
      "Epoch 1173, Train Loss: 4.742846819510063, Train Raw Loss: 3.818381430332859, Validation Loss: 5.637157917022705\n",
      "Epoch 1174, Train Loss: 4.729745732496182, Train Raw Loss: 3.8072321659988826, Validation Loss: 5.6601338386535645\n",
      "Epoch 1175, Train Loss: 4.735339562305146, Train Raw Loss: 3.8082696574429673, Validation Loss: 5.6491241455078125\n",
      "Epoch 1176, Train Loss: 4.740175478822655, Train Raw Loss: 3.8133495428909856, Validation Loss: 5.664344310760498\n",
      "Epoch 1177, Train Loss: 4.734967639959521, Train Raw Loss: 3.808185608064135, Validation Loss: 5.662047863006592\n",
      "Epoch 1178, Train Loss: 4.722639060351583, Train Raw Loss: 3.7997874541001186, Validation Loss: 5.654090881347656\n",
      "Epoch 1179, Train Loss: 4.73726165153914, Train Raw Loss: 3.809862165277203, Validation Loss: 5.657177925109863\n",
      "Epoch 1180, Train Loss: 4.736916209260623, Train Raw Loss: 3.810975430160761, Validation Loss: 5.639633655548096\n",
      "Epoch 1181, Train Loss: 4.7247458962102735, Train Raw Loss: 3.7986584064861137, Validation Loss: 5.658350467681885\n",
      "Epoch 1182, Train Loss: 4.732339055753417, Train Raw Loss: 3.8093548083470927, Validation Loss: 5.651031970977783\n",
      "Epoch 1183, Train Loss: 4.739944190118048, Train Raw Loss: 3.81913690066172, Validation Loss: 5.660052299499512\n",
      "Epoch 1184, Train Loss: 4.7337227245999705, Train Raw Loss: 3.806577689035071, Validation Loss: 5.663347244262695\n",
      "Epoch 1185, Train Loss: 4.7276442137857275, Train Raw Loss: 3.8036627144035364, Validation Loss: 5.648568153381348\n",
      "Epoch 1186, Train Loss: 4.739947548177507, Train Raw Loss: 3.8162276522980796, Validation Loss: 5.636108875274658\n",
      "Epoch 1187, Train Loss: 4.737153150886297, Train Raw Loss: 3.809123123064637, Validation Loss: 5.651093482971191\n",
      "Epoch 1188, Train Loss: 4.734939573705196, Train Raw Loss: 3.813109173046218, Validation Loss: 5.6400041580200195\n",
      "Epoch 1189, Train Loss: 4.721011467195219, Train Raw Loss: 3.8022473827832277, Validation Loss: 5.660297870635986\n",
      "Epoch 1190, Train Loss: 4.726783138761918, Train Raw Loss: 3.808401456475258, Validation Loss: 5.657500743865967\n",
      "Epoch 1191, Train Loss: 4.741056634899643, Train Raw Loss: 3.8166108652949333, Validation Loss: 5.6728835105896\n",
      "Epoch 1192, Train Loss: 4.73289860520098, Train Raw Loss: 3.807467597309086, Validation Loss: 5.657899379730225\n",
      "Epoch 1193, Train Loss: 4.734424690571096, Train Raw Loss: 3.8125221702787613, Validation Loss: 5.638657569885254\n",
      "Epoch 1194, Train Loss: 4.716313612295521, Train Raw Loss: 3.7996518220338555, Validation Loss: 5.647504806518555\n",
      "Epoch 1195, Train Loss: 4.725195700923602, Train Raw Loss: 3.8067574248131777, Validation Loss: 5.656328201293945\n",
      "Epoch 1196, Train Loss: 4.718632222629256, Train Raw Loss: 3.7992311598112187, Validation Loss: 5.638155460357666\n",
      "Epoch 1197, Train Loss: 4.720176110499435, Train Raw Loss: 3.799512711705433, Validation Loss: 5.652533531188965\n",
      "Epoch 1198, Train Loss: 4.720437107524938, Train Raw Loss: 3.79997846753233, Validation Loss: 5.650187015533447\n",
      "Epoch 1199, Train Loss: 4.72755219919814, Train Raw Loss: 3.807300082056059, Validation Loss: 5.614887714385986\n",
      "Epoch 1200, Train Loss: 4.734318347026904, Train Raw Loss: 3.815651511028409, Validation Loss: 5.649379253387451\n",
      "Epoch 1201, Train Loss: 4.726007060541047, Train Raw Loss: 3.8074786709828508, Validation Loss: 5.6451520919799805\n",
      "Epoch 1202, Train Loss: 4.729695144875182, Train Raw Loss: 3.8093260815160144, Validation Loss: 5.627137660980225\n",
      "Epoch 1203, Train Loss: 4.728419652001725, Train Raw Loss: 3.8103864401578904, Validation Loss: 5.642838478088379\n",
      "Epoch 1204, Train Loss: 4.728791748483975, Train Raw Loss: 3.809982298811277, Validation Loss: 5.637004375457764\n",
      "Epoch 1205, Train Loss: 4.7281091160244415, Train Raw Loss: 3.810262563203772, Validation Loss: 5.622715950012207\n",
      "Epoch 1206, Train Loss: 4.713555428137382, Train Raw Loss: 3.794364181492064, Validation Loss: 5.664911270141602\n",
      "Epoch 1207, Train Loss: 4.718973833653662, Train Raw Loss: 3.798358000194033, Validation Loss: 5.661323070526123\n",
      "Epoch 1208, Train Loss: 4.73641825368007, Train Raw Loss: 3.8166555986222295, Validation Loss: 5.6484761238098145\n",
      "Epoch 1209, Train Loss: 4.718273774286112, Train Raw Loss: 3.803982122283843, Validation Loss: 5.646872043609619\n",
      "Epoch 1210, Train Loss: 4.728352226896418, Train Raw Loss: 3.8080155193805694, Validation Loss: 5.642621994018555\n",
      "Epoch 1211, Train Loss: 4.730520159254471, Train Raw Loss: 3.812974370726281, Validation Loss: 5.660007476806641\n",
      "Epoch 1212, Train Loss: 4.729769243051608, Train Raw Loss: 3.809902490178744, Validation Loss: 5.619524002075195\n",
      "Epoch 1213, Train Loss: 4.7192047184954085, Train Raw Loss: 3.799147963937786, Validation Loss: 5.639030456542969\n",
      "Epoch 1214, Train Loss: 4.714629888865683, Train Raw Loss: 3.8005910225212576, Validation Loss: 5.629859924316406\n",
      "Epoch 1215, Train Loss: 4.724362585528029, Train Raw Loss: 3.8097547546856934, Validation Loss: 5.652877330780029\n",
      "Epoch 1216, Train Loss: 4.727623427949018, Train Raw Loss: 3.807456714618537, Validation Loss: 5.656327247619629\n",
      "Epoch 1217, Train Loss: 4.718852920995818, Train Raw Loss: 3.8030308581474754, Validation Loss: 5.65391731262207\n",
      "Epoch 1218, Train Loss: 4.709410895572768, Train Raw Loss: 3.797238530177209, Validation Loss: 5.643561363220215\n",
      "Epoch 1219, Train Loss: 4.722066054989894, Train Raw Loss: 3.808763745882445, Validation Loss: 5.655120372772217\n",
      "Epoch 1220, Train Loss: 4.713453109562397, Train Raw Loss: 3.8002984270453455, Validation Loss: 5.634364604949951\n",
      "Epoch 1221, Train Loss: 4.7024023075070644, Train Raw Loss: 3.7927318695104786, Validation Loss: 5.647418022155762\n",
      "Epoch 1222, Train Loss: 4.724379871620072, Train Raw Loss: 3.809647224595149, Validation Loss: 5.64005184173584\n",
      "Epoch 1223, Train Loss: 4.719969037671884, Train Raw Loss: 3.80518326850401, Validation Loss: 5.629067420959473\n",
      "Epoch 1224, Train Loss: 4.725349304204186, Train Raw Loss: 3.8105353785471783, Validation Loss: 5.642796516418457\n",
      "Epoch 1225, Train Loss: 4.70231453080972, Train Raw Loss: 3.785979217456447, Validation Loss: 5.644793510437012\n",
      "Epoch 1226, Train Loss: 4.709560295856662, Train Raw Loss: 3.79875217055281, Validation Loss: 5.628193378448486\n",
      "Epoch 1227, Train Loss: 4.716532133643826, Train Raw Loss: 3.8017609349141517, Validation Loss: 5.622514247894287\n",
      "Epoch 1228, Train Loss: 4.703290321264002, Train Raw Loss: 3.795999825000763, Validation Loss: 5.623972415924072\n",
      "Epoch 1229, Train Loss: 4.706053417424361, Train Raw Loss: 3.7943974163383247, Validation Loss: 5.620081424713135\n",
      "Epoch 1230, Train Loss: 4.720172743747631, Train Raw Loss: 3.810498600784275, Validation Loss: 5.602079391479492\n",
      "Epoch 1231, Train Loss: 4.70346323736012, Train Raw Loss: 3.7943482464386356, Validation Loss: 5.598117351531982\n",
      "Epoch 1232, Train Loss: 4.706816778663132, Train Raw Loss: 3.8034936842405136, Validation Loss: 5.6339921951293945\n",
      "Epoch 1233, Train Loss: 4.698920112889674, Train Raw Loss: 3.7885328834255536, Validation Loss: 5.607589244842529\n",
      "Epoch 1234, Train Loss: 4.7127033393830065, Train Raw Loss: 3.8020724827630654, Validation Loss: 5.607965469360352\n",
      "Epoch 1235, Train Loss: 4.703481903837787, Train Raw Loss: 3.7971241247322824, Validation Loss: 5.629676342010498\n",
      "Epoch 1236, Train Loss: 4.711377305371894, Train Raw Loss: 3.80189752748443, Validation Loss: 5.626521587371826\n",
      "Epoch 1237, Train Loss: 4.709401118175851, Train Raw Loss: 3.7965702017976177, Validation Loss: 5.6167449951171875\n",
      "Epoch 1238, Train Loss: 4.7119626483569546, Train Raw Loss: 3.8048685514678557, Validation Loss: 5.6393280029296875\n",
      "Epoch 1239, Train Loss: 4.714157414063811, Train Raw Loss: 3.803776229876611, Validation Loss: 5.6181817054748535\n",
      "Epoch 1240, Train Loss: 4.705214556968874, Train Raw Loss: 3.796024990040395, Validation Loss: 5.625403881072998\n",
      "Epoch 1241, Train Loss: 4.718958574864599, Train Raw Loss: 3.8083685856312512, Validation Loss: 5.644277095794678\n",
      "Epoch 1242, Train Loss: 4.7122631387578116, Train Raw Loss: 3.8003771219402553, Validation Loss: 5.6109442710876465\n",
      "Epoch 1243, Train Loss: 4.7158013626105255, Train Raw Loss: 3.807444507587287, Validation Loss: 5.5985870361328125\n",
      "Epoch 1244, Train Loss: 4.716458471409148, Train Raw Loss: 3.8085134031871957, Validation Loss: 5.595067501068115\n",
      "Epoch 1245, Train Loss: 4.69416790165835, Train Raw Loss: 3.7886377318037883, Validation Loss: 5.64924430847168\n",
      "Epoch 1246, Train Loss: 4.706697046260039, Train Raw Loss: 3.794519016312228, Validation Loss: 5.611502170562744\n",
      "Epoch 1247, Train Loss: 4.70308217956788, Train Raw Loss: 3.7979914285242558, Validation Loss: 5.600613594055176\n",
      "Epoch 1248, Train Loss: 4.697444022322695, Train Raw Loss: 3.790260420574082, Validation Loss: 5.617149829864502\n",
      "Epoch 1249, Train Loss: 4.707368633399407, Train Raw Loss: 3.800909301597211, Validation Loss: 5.616786479949951\n",
      "Epoch 1250, Train Loss: 4.701829233475857, Train Raw Loss: 3.7954857068757217, Validation Loss: 5.633176803588867\n",
      "Epoch 1251, Train Loss: 4.692866464621491, Train Raw Loss: 3.783410776572095, Validation Loss: 5.627985000610352\n",
      "Epoch 1252, Train Loss: 4.701476805988285, Train Raw Loss: 3.797263399304615, Validation Loss: 5.615329265594482\n",
      "Epoch 1253, Train Loss: 4.693919980194834, Train Raw Loss: 3.7888765153785546, Validation Loss: 5.613085746765137\n",
      "Epoch 1254, Train Loss: 4.692432943483194, Train Raw Loss: 3.7903722103685142, Validation Loss: 5.623875617980957\n",
      "Epoch 1255, Train Loss: 4.713450447014637, Train Raw Loss: 3.805416019923157, Validation Loss: 5.616147041320801\n",
      "Epoch 1256, Train Loss: 4.717724859548939, Train Raw Loss: 3.812504924668206, Validation Loss: 5.60067081451416\n",
      "Epoch 1257, Train Loss: 4.691228826592366, Train Raw Loss: 3.792503547709849, Validation Loss: 5.607834815979004\n",
      "Epoch 1258, Train Loss: 4.696012816536758, Train Raw Loss: 3.792496592261725, Validation Loss: 5.619361877441406\n",
      "Epoch 1259, Train Loss: 4.711593089128534, Train Raw Loss: 3.803149462201529, Validation Loss: 5.628504276275635\n",
      "Epoch 1260, Train Loss: 4.703593354754978, Train Raw Loss: 3.79781788289547, Validation Loss: 5.581713676452637\n",
      "Epoch 1261, Train Loss: 4.699468354135751, Train Raw Loss: 3.7991367154651217, Validation Loss: 5.601065635681152\n",
      "Epoch 1262, Train Loss: 4.701875487301085, Train Raw Loss: 3.8000358411007458, Validation Loss: 5.606343746185303\n",
      "Epoch 1263, Train Loss: 4.701265794783831, Train Raw Loss: 3.7999716241326595, Validation Loss: 5.609963893890381\n",
      "Epoch 1264, Train Loss: 4.707667228786482, Train Raw Loss: 3.803220616570777, Validation Loss: 5.587071895599365\n",
      "Epoch 1265, Train Loss: 4.694160787016154, Train Raw Loss: 3.7943235254949994, Validation Loss: 5.600418567657471\n",
      "Epoch 1266, Train Loss: 4.7051358876956835, Train Raw Loss: 3.802396071702242, Validation Loss: 5.6304826736450195\n",
      "Epoch 1267, Train Loss: 4.700667498881618, Train Raw Loss: 3.797844077108635, Validation Loss: 5.610621452331543\n",
      "Epoch 1268, Train Loss: 4.685859261328975, Train Raw Loss: 3.7868642397224903, Validation Loss: 5.618105411529541\n",
      "Epoch 1269, Train Loss: 4.705943679933747, Train Raw Loss: 3.801779771430625, Validation Loss: 5.623503684997559\n",
      "Epoch 1270, Train Loss: 4.701002426528269, Train Raw Loss: 3.7974733049670855, Validation Loss: 5.588943004608154\n",
      "Epoch 1271, Train Loss: 4.689872010383341, Train Raw Loss: 3.7885093186050653, Validation Loss: 5.597836494445801\n",
      "Epoch 1272, Train Loss: 4.693424792339404, Train Raw Loss: 3.7946499653574493, Validation Loss: 5.602751731872559\n",
      "Epoch 1273, Train Loss: 4.705353745280041, Train Raw Loss: 3.806509367542134, Validation Loss: 5.5977864265441895\n",
      "Epoch 1274, Train Loss: 4.684430625786384, Train Raw Loss: 3.7842603663603467, Validation Loss: 5.6221232414245605\n",
      "Epoch 1275, Train Loss: 4.6910287407537306, Train Raw Loss: 3.7955671672605806, Validation Loss: 5.624701499938965\n",
      "Epoch 1276, Train Loss: 4.702722132826845, Train Raw Loss: 3.799656975476278, Validation Loss: 5.595221042633057\n",
      "Epoch 1277, Train Loss: 4.697564761133657, Train Raw Loss: 3.797272986670335, Validation Loss: 5.607699394226074\n",
      "Epoch 1278, Train Loss: 4.67041146490309, Train Raw Loss: 3.775525678942601, Validation Loss: 5.608187198638916\n",
      "Epoch 1279, Train Loss: 4.684328222937054, Train Raw Loss: 3.790039241562287, Validation Loss: 5.586944580078125\n",
      "Epoch 1280, Train Loss: 4.6850881721410484, Train Raw Loss: 3.7868735934297244, Validation Loss: 5.622072696685791\n",
      "Epoch 1281, Train Loss: 4.69531811616487, Train Raw Loss: 3.7962384096450275, Validation Loss: 5.610085487365723\n",
      "Epoch 1282, Train Loss: 4.692196189281013, Train Raw Loss: 3.796700692880485, Validation Loss: 5.590269565582275\n",
      "Epoch 1283, Train Loss: 4.687790799472067, Train Raw Loss: 3.790491979279452, Validation Loss: 5.573177337646484\n",
      "Epoch 1284, Train Loss: 4.675359577230282, Train Raw Loss: 3.7833148318446344, Validation Loss: 5.60275411605835\n",
      "Epoch 1285, Train Loss: 4.69066566961507, Train Raw Loss: 3.7952930700447824, Validation Loss: 5.559440612792969\n",
      "Epoch 1286, Train Loss: 4.6952820302711595, Train Raw Loss: 3.801365345550908, Validation Loss: 5.580554008483887\n",
      "Epoch 1287, Train Loss: 4.689731730396549, Train Raw Loss: 3.792412428930402, Validation Loss: 5.591488838195801\n",
      "Epoch 1288, Train Loss: 4.683871855669551, Train Raw Loss: 3.790466980801688, Validation Loss: 5.601844787597656\n",
      "Epoch 1289, Train Loss: 4.689715791742007, Train Raw Loss: 3.7909790452983643, Validation Loss: 5.594128131866455\n",
      "Epoch 1290, Train Loss: 4.680834849841065, Train Raw Loss: 3.7869153201580046, Validation Loss: 5.590554237365723\n",
      "Epoch 1291, Train Loss: 4.697379113361239, Train Raw Loss: 3.7964340678519672, Validation Loss: 5.590827941894531\n",
      "Epoch 1292, Train Loss: 4.685610558299555, Train Raw Loss: 3.7915525683926212, Validation Loss: 5.572664737701416\n",
      "Epoch 1293, Train Loss: 4.676095982558198, Train Raw Loss: 3.7801276131636565, Validation Loss: 5.5967888832092285\n",
      "Epoch 1294, Train Loss: 4.686701502071487, Train Raw Loss: 3.7926695713566407, Validation Loss: 5.601463794708252\n",
      "Epoch 1295, Train Loss: 4.6853978354069925, Train Raw Loss: 3.792485547479656, Validation Loss: 5.596621513366699\n",
      "Epoch 1296, Train Loss: 4.687229607419836, Train Raw Loss: 3.7917869918048384, Validation Loss: 5.586554527282715\n",
      "Epoch 1297, Train Loss: 4.691562382669913, Train Raw Loss: 3.7985381022509603, Validation Loss: 5.601983070373535\n",
      "Epoch 1298, Train Loss: 4.655120579898357, Train Raw Loss: 3.765604684791631, Validation Loss: 5.610625743865967\n",
      "Epoch 1299, Train Loss: 4.694544312482079, Train Raw Loss: 3.7950961156023872, Validation Loss: 5.625607490539551\n",
      "Epoch 1300, Train Loss: 4.669643693914016, Train Raw Loss: 3.7801319345004027, Validation Loss: 5.610932350158691\n",
      "Epoch 1301, Train Loss: 4.70174435025288, Train Raw Loss: 3.805882676980562, Validation Loss: 5.608987808227539\n",
      "Epoch 1302, Train Loss: 4.685562124310268, Train Raw Loss: 3.7935422484245565, Validation Loss: 5.580039978027344\n",
      "Epoch 1303, Train Loss: 4.694006066065696, Train Raw Loss: 3.7988269784384303, Validation Loss: 5.589084148406982\n",
      "Epoch 1304, Train Loss: 4.671428336161706, Train Raw Loss: 3.779851944785979, Validation Loss: 5.594598770141602\n",
      "Epoch 1305, Train Loss: 4.683027992770076, Train Raw Loss: 3.7932436019182205, Validation Loss: 5.596762180328369\n",
      "Epoch 1306, Train Loss: 4.686439284351137, Train Raw Loss: 3.794645751764377, Validation Loss: 5.578566074371338\n",
      "Epoch 1307, Train Loss: 4.685744851951798, Train Raw Loss: 3.7945892512384387, Validation Loss: 5.587297439575195\n",
      "Epoch 1308, Train Loss: 4.679264355823397, Train Raw Loss: 3.7909966119047667, Validation Loss: 5.594688415527344\n",
      "Epoch 1309, Train Loss: 4.675153788261944, Train Raw Loss: 3.7833635135657255, Validation Loss: 5.591127395629883\n",
      "Epoch 1310, Train Loss: 4.674112195935514, Train Raw Loss: 3.7853488365809125, Validation Loss: 5.602263927459717\n",
      "Epoch 1311, Train Loss: 4.687241494531433, Train Raw Loss: 3.7948608678248195, Validation Loss: 5.579443454742432\n",
      "Epoch 1312, Train Loss: 4.666575578848521, Train Raw Loss: 3.780729262572196, Validation Loss: 5.589314937591553\n",
      "Epoch 1313, Train Loss: 4.665410653998454, Train Raw Loss: 3.7758108983023297, Validation Loss: 5.591691017150879\n",
      "Epoch 1314, Train Loss: 4.668538782662815, Train Raw Loss: 3.782737416277329, Validation Loss: 5.591899394989014\n",
      "Epoch 1315, Train Loss: 4.6822920403546755, Train Raw Loss: 3.79235052789251, Validation Loss: 5.615365505218506\n",
      "Epoch 1316, Train Loss: 4.676703654643562, Train Raw Loss: 3.7845895744860174, Validation Loss: 5.580881595611572\n",
      "Epoch 1317, Train Loss: 4.681374490136902, Train Raw Loss: 3.7917877138902742, Validation Loss: 5.56298303604126\n",
      "Epoch 1318, Train Loss: 4.674280678273903, Train Raw Loss: 3.7839468288338844, Validation Loss: 5.582647323608398\n",
      "Epoch 1319, Train Loss: 4.678758033365011, Train Raw Loss: 3.787673271695773, Validation Loss: 5.558565616607666\n",
      "Epoch 1320, Train Loss: 4.683052875391311, Train Raw Loss: 3.797183095291257, Validation Loss: 5.578580856323242\n",
      "Epoch 1321, Train Loss: 4.681041784874267, Train Raw Loss: 3.7912578757438395, Validation Loss: 5.582249641418457\n",
      "Epoch 1322, Train Loss: 4.669310574440493, Train Raw Loss: 3.7787984617054464, Validation Loss: 5.5800604820251465\n",
      "Epoch 1323, Train Loss: 4.677531734605631, Train Raw Loss: 3.7889124664581484, Validation Loss: 5.607972145080566\n",
      "Epoch 1324, Train Loss: 4.668788515817788, Train Raw Loss: 3.7818397151513232, Validation Loss: 5.600400447845459\n",
      "Epoch 1325, Train Loss: 4.6747672777209015, Train Raw Loss: 3.7843255082352294, Validation Loss: 5.616631507873535\n",
      "Epoch 1326, Train Loss: 4.692047770155801, Train Raw Loss: 3.7974140828268395, Validation Loss: 5.581127166748047\n",
      "Epoch 1327, Train Loss: 4.672211825640665, Train Raw Loss: 3.783161588592662, Validation Loss: 5.58659553527832\n",
      "Epoch 1328, Train Loss: 4.68762444253597, Train Raw Loss: 3.788490555435419, Validation Loss: 5.562331676483154\n",
      "Epoch 1329, Train Loss: 4.650889641212093, Train Raw Loss: 3.7686343532883457, Validation Loss: 5.595442771911621\n",
      "Epoch 1330, Train Loss: 4.6570542526741825, Train Raw Loss: 3.7737469566365083, Validation Loss: 5.609226703643799\n",
      "Epoch 1331, Train Loss: 4.675380767178204, Train Raw Loss: 3.7872319121741587, Validation Loss: 5.573338031768799\n",
      "Epoch 1332, Train Loss: 4.675271596676773, Train Raw Loss: 3.7895574231114653, Validation Loss: 5.569200038909912\n",
      "Epoch 1333, Train Loss: 4.672319060522649, Train Raw Loss: 3.78765216006173, Validation Loss: 5.587373733520508\n",
      "Epoch 1334, Train Loss: 4.694943425349063, Train Raw Loss: 3.806882053655055, Validation Loss: 5.578572750091553\n",
      "Epoch 1335, Train Loss: 4.664919634328948, Train Raw Loss: 3.7815916917390293, Validation Loss: 5.570815086364746\n",
      "Epoch 1336, Train Loss: 4.664754697639081, Train Raw Loss: 3.7805617871797748, Validation Loss: 5.577558994293213\n",
      "Epoch 1337, Train Loss: 4.671977926914891, Train Raw Loss: 3.786593376307024, Validation Loss: 5.563389778137207\n",
      "Epoch 1338, Train Loss: 4.65091614358955, Train Raw Loss: 3.765766822298368, Validation Loss: 5.568455219268799\n",
      "Epoch 1339, Train Loss: 4.671496506904562, Train Raw Loss: 3.786633215803239, Validation Loss: 5.557537078857422\n",
      "Epoch 1340, Train Loss: 4.662771985514296, Train Raw Loss: 3.781286325264308, Validation Loss: 5.579010009765625\n",
      "Epoch 1341, Train Loss: 4.668976683666309, Train Raw Loss: 3.7843733774705064, Validation Loss: 5.586316108703613\n",
      "Epoch 1342, Train Loss: 4.683154321710268, Train Raw Loss: 3.7919152853803504, Validation Loss: 5.559885025024414\n",
      "Epoch 1343, Train Loss: 4.672122121353945, Train Raw Loss: 3.787815221399069, Validation Loss: 5.544692516326904\n",
      "Epoch 1344, Train Loss: 4.660200406404005, Train Raw Loss: 3.7792115059577758, Validation Loss: 5.575737953186035\n",
      "Epoch 1345, Train Loss: 4.657095054661235, Train Raw Loss: 3.774862532524599, Validation Loss: 5.561127662658691\n",
      "Epoch 1346, Train Loss: 4.668825990996427, Train Raw Loss: 3.787296350300312, Validation Loss: 5.562621116638184\n",
      "Epoch 1347, Train Loss: 4.663390208904942, Train Raw Loss: 3.7830897149940332, Validation Loss: 5.5510711669921875\n",
      "Epoch 1348, Train Loss: 4.666986109854447, Train Raw Loss: 3.785368538896243, Validation Loss: 5.552585601806641\n",
      "Epoch 1349, Train Loss: 4.641084968878163, Train Raw Loss: 3.766712539601657, Validation Loss: 5.586883068084717\n",
      "Epoch 1350, Train Loss: 4.663535977775852, Train Raw Loss: 3.78126429931985, Validation Loss: 5.56131649017334\n",
      "Epoch 1351, Train Loss: 4.661536043178704, Train Raw Loss: 3.781009206134412, Validation Loss: 5.564998149871826\n",
      "Epoch 1352, Train Loss: 4.65086471889582, Train Raw Loss: 3.7688706828902165, Validation Loss: 5.598726272583008\n",
      "Epoch 1353, Train Loss: 4.665913893820511, Train Raw Loss: 3.784401929502686, Validation Loss: 5.556639671325684\n",
      "Epoch 1354, Train Loss: 4.668596999223033, Train Raw Loss: 3.789990593492985, Validation Loss: 5.550350666046143\n",
      "Epoch 1355, Train Loss: 4.638755474074019, Train Raw Loss: 3.76071233070559, Validation Loss: 5.566205024719238\n",
      "Epoch 1356, Train Loss: 4.654305060580373, Train Raw Loss: 3.779469758934445, Validation Loss: 5.549308776855469\n",
      "Epoch 1357, Train Loss: 4.658044165041711, Train Raw Loss: 3.7811105539815295, Validation Loss: 5.569516658782959\n",
      "Epoch 1358, Train Loss: 4.654013170922796, Train Raw Loss: 3.7740125907378066, Validation Loss: 5.566278457641602\n",
      "Epoch 1359, Train Loss: 4.658782355321778, Train Raw Loss: 3.780529310885403, Validation Loss: 5.567760944366455\n",
      "Epoch 1360, Train Loss: 4.664378917051686, Train Raw Loss: 3.785547443934613, Validation Loss: 5.560458183288574\n",
      "Epoch 1361, Train Loss: 4.657823957627018, Train Raw Loss: 3.7754515955431596, Validation Loss: 5.52565860748291\n",
      "Epoch 1362, Train Loss: 4.646993505747782, Train Raw Loss: 3.7701071365425984, Validation Loss: 5.570555210113525\n",
      "Epoch 1363, Train Loss: 4.662799022512304, Train Raw Loss: 3.785210662500726, Validation Loss: 5.554996013641357\n",
      "Epoch 1364, Train Loss: 4.67170008826587, Train Raw Loss: 3.789110984736019, Validation Loss: 5.5447678565979\n",
      "Epoch 1365, Train Loss: 4.656055471673608, Train Raw Loss: 3.775776044569082, Validation Loss: 5.559783458709717\n",
      "Epoch 1366, Train Loss: 4.655431182889474, Train Raw Loss: 3.7797609580059848, Validation Loss: 5.560934066772461\n",
      "Epoch 1367, Train Loss: 4.653635524751412, Train Raw Loss: 3.7758450599180327, Validation Loss: 5.552074909210205\n",
      "Epoch 1368, Train Loss: 4.648719906268848, Train Raw Loss: 3.7724659669316476, Validation Loss: 5.589301109313965\n",
      "Epoch 1369, Train Loss: 4.658811295487815, Train Raw Loss: 3.777247942156262, Validation Loss: 5.544199466705322\n",
      "Epoch 1370, Train Loss: 4.649352043825719, Train Raw Loss: 3.7742244942734637, Validation Loss: 5.557018756866455\n",
      "Epoch 1371, Train Loss: 4.653592347767618, Train Raw Loss: 3.7770263208283317, Validation Loss: 5.544890880584717\n",
      "Epoch 1372, Train Loss: 4.650872495646278, Train Raw Loss: 3.7739763061619467, Validation Loss: 5.551385879516602\n",
      "Epoch 1373, Train Loss: 4.6583115510228605, Train Raw Loss: 3.7826904558059242, Validation Loss: 5.557785987854004\n",
      "Epoch 1374, Train Loss: 4.664566111523244, Train Raw Loss: 3.7858427669025128, Validation Loss: 5.552924156188965\n",
      "Epoch 1375, Train Loss: 4.6505776124695934, Train Raw Loss: 3.772557634860277, Validation Loss: 5.552603721618652\n",
      "Epoch 1376, Train Loss: 4.660130737183822, Train Raw Loss: 3.782884103515082, Validation Loss: 5.552073955535889\n",
      "Epoch 1377, Train Loss: 4.636303070849842, Train Raw Loss: 3.7617418055319125, Validation Loss: 5.514936447143555\n",
      "Epoch 1378, Train Loss: 4.6394612130191595, Train Raw Loss: 3.7670463394373654, Validation Loss: 5.57580041885376\n",
      "Epoch 1379, Train Loss: 4.646857817098498, Train Raw Loss: 3.773939921086033, Validation Loss: 5.552151679992676\n",
      "Epoch 1380, Train Loss: 4.652409887396627, Train Raw Loss: 3.778408930202325, Validation Loss: 5.564152717590332\n",
      "Epoch 1381, Train Loss: 4.664166411881646, Train Raw Loss: 3.7894097194489507, Validation Loss: 5.568763732910156\n",
      "Epoch 1382, Train Loss: 4.652283827381002, Train Raw Loss: 3.7762599137922126, Validation Loss: 5.539879322052002\n",
      "Epoch 1383, Train Loss: 4.6560365009639, Train Raw Loss: 3.7797433187978133, Validation Loss: 5.54724645614624\n",
      "Epoch 1384, Train Loss: 4.647028065514234, Train Raw Loss: 3.7725145436823366, Validation Loss: 5.556972026824951\n",
      "Epoch 1385, Train Loss: 4.64682018512653, Train Raw Loss: 3.774705819454458, Validation Loss: 5.556264400482178\n",
      "Epoch 1386, Train Loss: 4.660364931118157, Train Raw Loss: 3.7874958238253993, Validation Loss: 5.548576831817627\n",
      "Epoch 1387, Train Loss: 4.642931533356507, Train Raw Loss: 3.7700599703109927, Validation Loss: 5.589520454406738\n",
      "Epoch 1388, Train Loss: 4.648691159114241, Train Raw Loss: 3.776727668278747, Validation Loss: 5.521576404571533\n",
      "Epoch 1389, Train Loss: 4.656341289314959, Train Raw Loss: 3.783438742491934, Validation Loss: 5.515632629394531\n",
      "Epoch 1390, Train Loss: 4.642380395283301, Train Raw Loss: 3.7695937365707426, Validation Loss: 5.557690620422363\n",
      "Epoch 1391, Train Loss: 4.656599799212482, Train Raw Loss: 3.782168835070398, Validation Loss: 5.537383079528809\n",
      "Epoch 1392, Train Loss: 4.634748175864418, Train Raw Loss: 3.7682247816274566, Validation Loss: 5.569921493530273\n",
      "Epoch 1393, Train Loss: 4.647833982316984, Train Raw Loss: 3.7761364911579425, Validation Loss: 5.525500774383545\n",
      "Epoch 1394, Train Loss: 4.647549134492874, Train Raw Loss: 3.7804816317641072, Validation Loss: 5.535977363586426\n",
      "Epoch 1395, Train Loss: 4.637115544536048, Train Raw Loss: 3.7678092261983287, Validation Loss: 5.545811176300049\n",
      "Epoch 1396, Train Loss: 4.6489679594006805, Train Raw Loss: 3.7787238765094013, Validation Loss: 5.528280258178711\n",
      "Epoch 1397, Train Loss: 4.641238625802927, Train Raw Loss: 3.7716322976268, Validation Loss: 5.5530548095703125\n",
      "Epoch 1398, Train Loss: 4.647214952649342, Train Raw Loss: 3.779513391810987, Validation Loss: 5.535383224487305\n",
      "Epoch 1399, Train Loss: 4.641212839633226, Train Raw Loss: 3.7724477994359202, Validation Loss: 5.542999267578125\n",
      "Epoch 1400, Train Loss: 4.637549772775835, Train Raw Loss: 3.77225549577011, Validation Loss: 5.538424491882324\n",
      "Epoch 1401, Train Loss: 4.631206193359362, Train Raw Loss: 3.763202327531245, Validation Loss: 5.540958881378174\n",
      "Epoch 1402, Train Loss: 4.644798076152801, Train Raw Loss: 3.777315025818017, Validation Loss: 5.535694122314453\n",
      "Epoch 1403, Train Loss: 4.651876035746601, Train Raw Loss: 3.7805154689898095, Validation Loss: 5.523022174835205\n",
      "Epoch 1404, Train Loss: 4.628875379885236, Train Raw Loss: 3.7598018906596633, Validation Loss: 5.529795169830322\n",
      "Epoch 1405, Train Loss: 4.638148955379923, Train Raw Loss: 3.772319541954332, Validation Loss: 5.524885177612305\n",
      "Epoch 1406, Train Loss: 4.631417256883449, Train Raw Loss: 3.7637181093295413, Validation Loss: 5.522149085998535\n",
      "Epoch 1407, Train Loss: 4.63062814027071, Train Raw Loss: 3.7664753005736404, Validation Loss: 5.5345458984375\n",
      "Epoch 1408, Train Loss: 4.646070109183589, Train Raw Loss: 3.7715376362204553, Validation Loss: 5.535261154174805\n",
      "Epoch 1409, Train Loss: 4.652859406794111, Train Raw Loss: 3.7883716667277945, Validation Loss: 5.541874408721924\n",
      "Epoch 1410, Train Loss: 4.637223750228683, Train Raw Loss: 3.7669323650499185, Validation Loss: 5.529765605926514\n",
      "Epoch 1411, Train Loss: 4.62606819599039, Train Raw Loss: 3.7579597884582148, Validation Loss: 5.5588459968566895\n",
      "Epoch 1412, Train Loss: 4.635591953289178, Train Raw Loss: 3.7642628802193534, Validation Loss: 5.528435230255127\n",
      "Epoch 1413, Train Loss: 4.639821302062935, Train Raw Loss: 3.7738888388706577, Validation Loss: 5.514953136444092\n",
      "Epoch 1414, Train Loss: 4.640151311871079, Train Raw Loss: 3.7662049531936646, Validation Loss: 5.512260913848877\n",
      "Epoch 1415, Train Loss: 4.640818749864896, Train Raw Loss: 3.7716820160547893, Validation Loss: 5.535212993621826\n",
      "Epoch 1416, Train Loss: 4.630888195873963, Train Raw Loss: 3.765237261727452, Validation Loss: 5.563552379608154\n",
      "Epoch 1417, Train Loss: 4.65521707886623, Train Raw Loss: 3.7847479356659783, Validation Loss: 5.542852878570557\n",
      "Epoch 1418, Train Loss: 4.644306139606568, Train Raw Loss: 3.7758838685436382, Validation Loss: 5.523788928985596\n",
      "Epoch 1419, Train Loss: 4.622132564170493, Train Raw Loss: 3.75731492307451, Validation Loss: 5.55012321472168\n",
      "Epoch 1420, Train Loss: 4.649351464708646, Train Raw Loss: 3.777354285576277, Validation Loss: 5.532937049865723\n",
      "Epoch 1421, Train Loss: 4.626384471936358, Train Raw Loss: 3.761004555142588, Validation Loss: 5.539725303649902\n",
      "Epoch 1422, Train Loss: 4.6480775588502485, Train Raw Loss: 3.779909309786227, Validation Loss: 5.536293029785156\n",
      "Epoch 1423, Train Loss: 4.654908824712038, Train Raw Loss: 3.788243931449122, Validation Loss: 5.538495063781738\n",
      "Epoch 1424, Train Loss: 4.638368806449903, Train Raw Loss: 3.7719370637916856, Validation Loss: 5.547399044036865\n",
      "Epoch 1425, Train Loss: 4.651476651264561, Train Raw Loss: 3.785144063954552, Validation Loss: 5.537227153778076\n",
      "Epoch 1426, Train Loss: 4.645002059679892, Train Raw Loss: 3.7773074571043255, Validation Loss: 5.512206554412842\n",
      "Epoch 1427, Train Loss: 4.629886373629173, Train Raw Loss: 3.765055105628239, Validation Loss: 5.562689781188965\n",
      "Epoch 1428, Train Loss: 4.651319855617152, Train Raw Loss: 3.78410364691582, Validation Loss: 5.525947570800781\n",
      "Epoch 1429, Train Loss: 4.639655125886202, Train Raw Loss: 3.7705278312580455, Validation Loss: 5.503407001495361\n",
      "Epoch 1430, Train Loss: 4.619395463872287, Train Raw Loss: 3.7585041123545833, Validation Loss: 5.534861087799072\n",
      "Epoch 1431, Train Loss: 4.642048249269525, Train Raw Loss: 3.7738244214405614, Validation Loss: 5.521933555603027\n",
      "Epoch 1432, Train Loss: 4.644296148377988, Train Raw Loss: 3.7811664091216195, Validation Loss: 5.531388759613037\n",
      "Epoch 1433, Train Loss: 4.632871673545903, Train Raw Loss: 3.771352790420254, Validation Loss: 5.559937000274658\n",
      "Epoch 1434, Train Loss: 4.643129999811451, Train Raw Loss: 3.7737683398442137, Validation Loss: 5.5195231437683105\n",
      "Epoch 1435, Train Loss: 4.628021054135429, Train Raw Loss: 3.7678714013348023, Validation Loss: 5.523452281951904\n",
      "Epoch 1436, Train Loss: 4.636532657717665, Train Raw Loss: 3.770797445459498, Validation Loss: 5.521411418914795\n",
      "Epoch 1437, Train Loss: 4.6275684620771145, Train Raw Loss: 3.7659738967816034, Validation Loss: 5.55892276763916\n",
      "Epoch 1438, Train Loss: 4.637488134909007, Train Raw Loss: 3.772118757996294, Validation Loss: 5.540211200714111\n",
      "Epoch 1439, Train Loss: 4.626846769410703, Train Raw Loss: 3.764730642984311, Validation Loss: 5.53128719329834\n",
      "Epoch 1440, Train Loss: 4.626400859943694, Train Raw Loss: 3.7651863657352, Validation Loss: 5.551797866821289\n",
      "Epoch 1441, Train Loss: 4.653460433789426, Train Raw Loss: 3.7875892404466867, Validation Loss: 5.52379846572876\n",
      "Epoch 1442, Train Loss: 4.613801144229042, Train Raw Loss: 3.749336948618293, Validation Loss: 5.5352067947387695\n",
      "Epoch 1443, Train Loss: 4.640874756872654, Train Raw Loss: 3.7778678418447575, Validation Loss: 5.558112621307373\n",
      "Epoch 1444, Train Loss: 4.645678058556385, Train Raw Loss: 3.7818237808015613, Validation Loss: 5.546632766723633\n",
      "Epoch 1445, Train Loss: 4.638955740133921, Train Raw Loss: 3.7749142882310682, Validation Loss: 5.526693820953369\n",
      "Epoch 1446, Train Loss: 4.6384163078334595, Train Raw Loss: 3.7720661762273973, Validation Loss: 5.551176071166992\n",
      "Epoch 1447, Train Loss: 4.636958504468202, Train Raw Loss: 3.7734339368012217, Validation Loss: 5.518233776092529\n",
      "Epoch 1448, Train Loss: 4.640778674930334, Train Raw Loss: 3.7753263444950185, Validation Loss: 5.5128583908081055\n",
      "Epoch 1449, Train Loss: 4.634135000324911, Train Raw Loss: 3.7716868385258646, Validation Loss: 5.5162811279296875\n",
      "Epoch 1450, Train Loss: 4.6284047820087935, Train Raw Loss: 3.768844355311659, Validation Loss: 5.5235066413879395\n",
      "Epoch 1451, Train Loss: 4.63718201580147, Train Raw Loss: 3.771854219958186, Validation Loss: 5.519224166870117\n",
      "Epoch 1452, Train Loss: 4.620019803568721, Train Raw Loss: 3.761559837890996, Validation Loss: 5.536716938018799\n",
      "Epoch 1453, Train Loss: 4.624743262719777, Train Raw Loss: 3.764338960126042, Validation Loss: 5.573835372924805\n",
      "Epoch 1454, Train Loss: 4.634307322982285, Train Raw Loss: 3.7671338078462417, Validation Loss: 5.542026519775391\n",
      "Epoch 1455, Train Loss: 4.643182630630003, Train Raw Loss: 3.780655074078176, Validation Loss: 5.528598785400391\n",
      "Epoch 1456, Train Loss: 4.635670789413982, Train Raw Loss: 3.7704358918799294, Validation Loss: 5.490643501281738\n",
      "Epoch 1457, Train Loss: 4.6165474081205, Train Raw Loss: 3.7579120928214658, Validation Loss: 5.53480339050293\n",
      "Epoch 1458, Train Loss: 4.628668455613984, Train Raw Loss: 3.7675213631242515, Validation Loss: 5.5471296310424805\n",
      "Epoch 1459, Train Loss: 4.636649090051651, Train Raw Loss: 3.773827477875683, Validation Loss: 5.523095607757568\n",
      "Epoch 1460, Train Loss: 4.63708831783798, Train Raw Loss: 3.7776245784428384, Validation Loss: 5.50180196762085\n",
      "Epoch 1461, Train Loss: 4.623415539372298, Train Raw Loss: 3.7637937370687724, Validation Loss: 5.522687911987305\n",
      "Epoch 1462, Train Loss: 4.639040992947089, Train Raw Loss: 3.7796515125781296, Validation Loss: 5.508607864379883\n",
      "Epoch 1463, Train Loss: 4.623190895054075, Train Raw Loss: 3.7649221292386454, Validation Loss: 5.52371883392334\n",
      "Epoch 1464, Train Loss: 4.635972966667679, Train Raw Loss: 3.775931860630711, Validation Loss: 5.52159309387207\n",
      "Epoch 1465, Train Loss: 4.636300941639476, Train Raw Loss: 3.774819938258992, Validation Loss: 5.513553619384766\n",
      "Epoch 1466, Train Loss: 4.6188230203671585, Train Raw Loss: 3.7605300652070177, Validation Loss: 5.515003681182861\n",
      "Epoch 1467, Train Loss: 4.631762707440389, Train Raw Loss: 3.769730450668269, Validation Loss: 5.537670135498047\n",
      "Epoch 1468, Train Loss: 4.627924774338802, Train Raw Loss: 3.7711379410492047, Validation Loss: 5.494423866271973\n",
      "Epoch 1469, Train Loss: 4.618867641190688, Train Raw Loss: 3.764644089051419, Validation Loss: 5.546508312225342\n",
      "Epoch 1470, Train Loss: 4.620268478368719, Train Raw Loss: 3.7579506660915083, Validation Loss: 5.519659519195557\n",
      "Epoch 1471, Train Loss: 4.623903339687321, Train Raw Loss: 3.768711905967858, Validation Loss: 5.522620677947998\n",
      "Epoch 1472, Train Loss: 4.631873365946942, Train Raw Loss: 3.772572460811999, Validation Loss: 5.508746147155762\n",
      "Epoch 1473, Train Loss: 4.627330012950632, Train Raw Loss: 3.767371594491932, Validation Loss: 5.532565116882324\n",
      "Epoch 1474, Train Loss: 4.637826839296354, Train Raw Loss: 3.775202525779605, Validation Loss: 5.510794639587402\n",
      "Epoch 1475, Train Loss: 4.616895300315486, Train Raw Loss: 3.7623540290113953, Validation Loss: 5.524234294891357\n",
      "Epoch 1476, Train Loss: 4.6316276976217825, Train Raw Loss: 3.770775914647513, Validation Loss: 5.546559810638428\n",
      "Epoch 1477, Train Loss: 4.633993101161387, Train Raw Loss: 3.777399085751838, Validation Loss: 5.534507751464844\n",
      "Epoch 1478, Train Loss: 4.636555064428184, Train Raw Loss: 3.7785870955636103, Validation Loss: 5.511567115783691\n",
      "Epoch 1479, Train Loss: 4.635331072409948, Train Raw Loss: 3.7774566588302454, Validation Loss: 5.521491050720215\n",
      "Epoch 1480, Train Loss: 4.618082804108659, Train Raw Loss: 3.76266054213047, Validation Loss: 5.516964912414551\n",
      "Epoch 1481, Train Loss: 4.62088971051077, Train Raw Loss: 3.764039713806576, Validation Loss: 5.5204668045043945\n",
      "Epoch 1482, Train Loss: 4.6249017469584945, Train Raw Loss: 3.7661274077991647, Validation Loss: 5.494515895843506\n",
      "Epoch 1483, Train Loss: 4.627151185563869, Train Raw Loss: 3.772155417998632, Validation Loss: 5.4977874755859375\n",
      "Epoch 1484, Train Loss: 4.636013585163488, Train Raw Loss: 3.7761589393019674, Validation Loss: 5.523900508880615\n",
      "Epoch 1485, Train Loss: 4.62690432448354, Train Raw Loss: 3.769504832973083, Validation Loss: 5.503560543060303\n",
      "Epoch 1486, Train Loss: 4.610351881509026, Train Raw Loss: 3.7552460672126875, Validation Loss: 5.528018951416016\n",
      "Epoch 1487, Train Loss: 4.625399815994832, Train Raw Loss: 3.7706842774732245, Validation Loss: 5.526286602020264\n",
      "Epoch 1488, Train Loss: 4.627866612043646, Train Raw Loss: 3.7739519933031667, Validation Loss: 5.502638339996338\n",
      "Epoch 1489, Train Loss: 4.627873818245199, Train Raw Loss: 3.770463977712724, Validation Loss: 5.522567272186279\n",
      "Epoch 1490, Train Loss: 4.6223706703219145, Train Raw Loss: 3.7662649979194005, Validation Loss: 5.505163669586182\n",
      "Epoch 1491, Train Loss: 4.629322114131517, Train Raw Loss: 3.774629190398587, Validation Loss: 5.502264976501465\n",
      "Epoch 1492, Train Loss: 4.632688479456637, Train Raw Loss: 3.7786331112186113, Validation Loss: 5.516991138458252\n",
      "Epoch 1493, Train Loss: 4.621445055430134, Train Raw Loss: 3.768758008090986, Validation Loss: 5.528736114501953\n",
      "Epoch 1494, Train Loss: 4.622691023680899, Train Raw Loss: 3.767386920708749, Validation Loss: 5.520132541656494\n",
      "Epoch 1495, Train Loss: 4.63617108443545, Train Raw Loss: 3.7785896493742865, Validation Loss: 5.506655693054199\n",
      "Epoch 1496, Train Loss: 4.61661578271952, Train Raw Loss: 3.7650086403306986, Validation Loss: 5.525335311889648\n",
      "Epoch 1497, Train Loss: 4.62211550536255, Train Raw Loss: 3.765700105412139, Validation Loss: 5.52109432220459\n",
      "Epoch 1498, Train Loss: 4.633149890353282, Train Raw Loss: 3.776914931047294, Validation Loss: 5.50768518447876\n",
      "Epoch 1499, Train Loss: 4.622006884713968, Train Raw Loss: 3.7678187001910475, Validation Loss: 5.513169765472412\n",
      "Epoch 1500, Train Loss: 4.628951697465446, Train Raw Loss: 3.7770182868672744, Validation Loss: 5.507195472717285\n",
      "Epoch 1501, Train Loss: 4.622609144035313, Train Raw Loss: 3.769146041158173, Validation Loss: 5.516749858856201\n",
      "Epoch 1502, Train Loss: 4.626807905609409, Train Raw Loss: 3.77335023548868, Validation Loss: 5.50554084777832\n",
      "Epoch 1503, Train Loss: 4.625758404201932, Train Raw Loss: 3.769503024799956, Validation Loss: 5.503614902496338\n",
      "Epoch 1504, Train Loss: 4.619034373056557, Train Raw Loss: 3.7707554305593174, Validation Loss: 5.498026371002197\n",
      "Epoch 1505, Train Loss: 4.634046854202946, Train Raw Loss: 3.779548037714428, Validation Loss: 5.502060413360596\n",
      "Epoch 1506, Train Loss: 4.6133235543552376, Train Raw Loss: 3.7615795949267015, Validation Loss: 5.514327049255371\n",
      "Epoch 1507, Train Loss: 4.6246091668804485, Train Raw Loss: 3.7741894035703605, Validation Loss: 5.508806228637695\n",
      "Epoch 1508, Train Loss: 4.621827625690235, Train Raw Loss: 3.766503965978821, Validation Loss: 5.478891372680664\n",
      "Epoch 1509, Train Loss: 4.618436054761211, Train Raw Loss: 3.7674209801273215, Validation Loss: 5.491749286651611\n",
      "Epoch 1510, Train Loss: 4.622457063322266, Train Raw Loss: 3.7704122021794317, Validation Loss: 5.520843029022217\n",
      "Epoch 1511, Train Loss: 4.6212428227480915, Train Raw Loss: 3.7747583572649295, Validation Loss: 5.506552219390869\n",
      "Epoch 1512, Train Loss: 4.618932894617319, Train Raw Loss: 3.765499822753999, Validation Loss: 5.518149375915527\n",
      "Epoch 1513, Train Loss: 4.62634477449788, Train Raw Loss: 3.774636074776451, Validation Loss: 5.507841110229492\n",
      "Epoch 1514, Train Loss: 4.607019703379936, Train Raw Loss: 3.7587179985311296, Validation Loss: 5.5365166664123535\n",
      "Epoch 1515, Train Loss: 4.619874624452657, Train Raw Loss: 3.769309860095382, Validation Loss: 5.501576900482178\n",
      "Epoch 1516, Train Loss: 4.617729406514101, Train Raw Loss: 3.7691044153852595, Validation Loss: 5.548970699310303\n",
      "Epoch 1517, Train Loss: 4.605848128058844, Train Raw Loss: 3.756024698830313, Validation Loss: 5.537210941314697\n",
      "Epoch 1518, Train Loss: 4.625050175603893, Train Raw Loss: 3.771321377033989, Validation Loss: 5.532631874084473\n",
      "Epoch 1519, Train Loss: 4.611357330820627, Train Raw Loss: 3.759796986770299, Validation Loss: 5.514632701873779\n",
      "Epoch 1520, Train Loss: 4.617657156619761, Train Raw Loss: 3.7656149273117383, Validation Loss: 5.488708019256592\n",
      "Epoch 1521, Train Loss: 4.606460882226626, Train Raw Loss: 3.7573189875731865, Validation Loss: 5.506899833679199\n",
      "Epoch 1522, Train Loss: 4.62359354197979, Train Raw Loss: 3.772693913843897, Validation Loss: 5.488696098327637\n",
      "Epoch 1523, Train Loss: 4.606413770715395, Train Raw Loss: 3.7596033405512572, Validation Loss: 5.511735916137695\n",
      "Epoch 1524, Train Loss: 4.623022646374173, Train Raw Loss: 3.7725497679164013, Validation Loss: 5.49640417098999\n",
      "Epoch 1525, Train Loss: 4.617302210256457, Train Raw Loss: 3.7685632089359893, Validation Loss: 5.49342679977417\n",
      "Epoch 1526, Train Loss: 4.618591279163956, Train Raw Loss: 3.7663209408521654, Validation Loss: 5.489602088928223\n",
      "Epoch 1527, Train Loss: 4.610009211922685, Train Raw Loss: 3.762112137468325, Validation Loss: 5.48463249206543\n",
      "Epoch 1528, Train Loss: 4.6014294029937854, Train Raw Loss: 3.7547894679423837, Validation Loss: 5.516888618469238\n",
      "Epoch 1529, Train Loss: 4.6114389235774675, Train Raw Loss: 3.763422582878007, Validation Loss: 5.512947082519531\n",
      "Epoch 1530, Train Loss: 4.62544138253563, Train Raw Loss: 3.7738823697798782, Validation Loss: 5.480777263641357\n",
      "Epoch 1531, Train Loss: 4.610038867344459, Train Raw Loss: 3.7598901454773213, Validation Loss: 5.536484241485596\n",
      "Epoch 1532, Train Loss: 4.621162355649802, Train Raw Loss: 3.7718118964384, Validation Loss: 5.493660926818848\n",
      "Epoch 1533, Train Loss: 4.59967662340237, Train Raw Loss: 3.754967889148328, Validation Loss: 5.512909412384033\n",
      "Epoch 1534, Train Loss: 4.6284250090105665, Train Raw Loss: 3.7732298058768112, Validation Loss: 5.468410015106201\n",
      "Epoch 1535, Train Loss: 4.5964621750844845, Train Raw Loss: 3.7508911184138722, Validation Loss: 5.496222019195557\n",
      "Epoch 1536, Train Loss: 4.608453543898132, Train Raw Loss: 3.7644044456796513, Validation Loss: 5.492291450500488\n",
      "Epoch 1537, Train Loss: 4.600800107957588, Train Raw Loss: 3.757989677786827, Validation Loss: 5.487346649169922\n",
      "Epoch 1538, Train Loss: 4.607991595856017, Train Raw Loss: 3.761569560484754, Validation Loss: 5.498929977416992\n",
      "Epoch 1539, Train Loss: 4.608474865762724, Train Raw Loss: 3.7656446050438617, Validation Loss: 5.463565826416016\n",
      "Epoch 1540, Train Loss: 4.605799199806319, Train Raw Loss: 3.764100505080488, Validation Loss: 5.479584217071533\n",
      "Epoch 1541, Train Loss: 4.616536173679762, Train Raw Loss: 3.770894031599164, Validation Loss: 5.473685264587402\n",
      "Epoch 1542, Train Loss: 4.6101777172336975, Train Raw Loss: 3.7647334904720386, Validation Loss: 5.49136209487915\n",
      "Epoch 1543, Train Loss: 4.599796795720855, Train Raw Loss: 3.7545180493344863, Validation Loss: 5.494319438934326\n",
      "Epoch 1544, Train Loss: 4.613084178417921, Train Raw Loss: 3.766663254880243, Validation Loss: 5.488191604614258\n",
      "Epoch 1545, Train Loss: 4.59084296785295, Train Raw Loss: 3.746550074385272, Validation Loss: 5.525252819061279\n",
      "Epoch 1546, Train Loss: 4.609706802666187, Train Raw Loss: 3.765194742961062, Validation Loss: 5.4913010597229\n",
      "Epoch 1547, Train Loss: 4.599951309959094, Train Raw Loss: 3.7575360260489914, Validation Loss: 5.499513626098633\n",
      "Epoch 1548, Train Loss: 4.602494971247183, Train Raw Loss: 3.759116327183114, Validation Loss: 5.477916240692139\n",
      "Epoch 1549, Train Loss: 4.610203391686082, Train Raw Loss: 3.7636394737495316, Validation Loss: 5.495799541473389\n",
      "Epoch 1550, Train Loss: 4.598832785627908, Train Raw Loss: 3.7560528064353598, Validation Loss: 5.500106334686279\n",
      "Epoch 1551, Train Loss: 4.615515828380982, Train Raw Loss: 3.770301803201437, Validation Loss: 5.500508785247803\n",
      "Epoch 1552, Train Loss: 4.612454678449366, Train Raw Loss: 3.768361008291443, Validation Loss: 5.49217414855957\n",
      "Epoch 1553, Train Loss: 4.609507132073244, Train Raw Loss: 3.763608728804522, Validation Loss: 5.471449375152588\n",
      "Epoch 1554, Train Loss: 4.603870062737001, Train Raw Loss: 3.7592452770719924, Validation Loss: 5.481023788452148\n",
      "Epoch 1555, Train Loss: 4.606118154649933, Train Raw Loss: 3.7624512256433564, Validation Loss: 5.4787468910217285\n",
      "Epoch 1556, Train Loss: 4.602938884786433, Train Raw Loss: 3.758471568549673, Validation Loss: 5.49471378326416\n",
      "Epoch 1557, Train Loss: 4.612304011483987, Train Raw Loss: 3.7675438432643813, Validation Loss: 5.469650745391846\n",
      "Epoch 1558, Train Loss: 4.60888870689604, Train Raw Loss: 3.7637160321904553, Validation Loss: 5.481038570404053\n",
      "Epoch 1559, Train Loss: 4.608180297041932, Train Raw Loss: 3.763968552649021, Validation Loss: 5.476179122924805\n",
      "Epoch 1560, Train Loss: 4.596172046371632, Train Raw Loss: 3.754805057330264, Validation Loss: 5.487668037414551\n",
      "Epoch 1561, Train Loss: 4.601919428383311, Train Raw Loss: 3.760258655457033, Validation Loss: 5.48359489440918\n",
      "Epoch 1562, Train Loss: 4.605234743240807, Train Raw Loss: 3.765225355244345, Validation Loss: 5.484319686889648\n",
      "Epoch 1563, Train Loss: 4.611314372552766, Train Raw Loss: 3.7680670500215556, Validation Loss: 5.501079082489014\n",
      "Epoch 1564, Train Loss: 4.594168368685577, Train Raw Loss: 3.7502158450997536, Validation Loss: 5.47643518447876\n",
      "Epoch 1565, Train Loss: 4.611622286587954, Train Raw Loss: 3.768981539375252, Validation Loss: 5.474635601043701\n",
      "Epoch 1566, Train Loss: 4.600334776399864, Train Raw Loss: 3.758223598698775, Validation Loss: 5.469703674316406\n",
      "Epoch 1567, Train Loss: 4.5874718794806135, Train Raw Loss: 3.7477085242668786, Validation Loss: 5.4984354972839355\n",
      "Epoch 1568, Train Loss: 4.6006353576564125, Train Raw Loss: 3.759710633299417, Validation Loss: 5.477104187011719\n",
      "Epoch 1569, Train Loss: 4.595251898881462, Train Raw Loss: 3.7557694832070005, Validation Loss: 5.486515522003174\n",
      "Epoch 1570, Train Loss: 4.599497034028173, Train Raw Loss: 3.7607289490186506, Validation Loss: 5.484541416168213\n",
      "Epoch 1571, Train Loss: 4.59682645926045, Train Raw Loss: 3.7570395630680853, Validation Loss: 5.468791961669922\n",
      "Epoch 1572, Train Loss: 4.5980553291324116, Train Raw Loss: 3.7574661858793763, Validation Loss: 5.47710657119751\n",
      "Epoch 1573, Train Loss: 4.598934577943551, Train Raw Loss: 3.7598539469142755, Validation Loss: 5.474086284637451\n",
      "Epoch 1574, Train Loss: 4.602047548567255, Train Raw Loss: 3.7632124410321315, Validation Loss: 5.460309982299805\n",
      "Epoch 1575, Train Loss: 4.5872172076669, Train Raw Loss: 3.7495410982105466, Validation Loss: 5.475636005401611\n",
      "Epoch 1576, Train Loss: 4.583729638200667, Train Raw Loss: 3.746807446496354, Validation Loss: 5.476434230804443\n",
      "Epoch 1577, Train Loss: 4.591352038913303, Train Raw Loss: 3.7530807813422546, Validation Loss: 5.47634220123291\n",
      "Epoch 1578, Train Loss: 4.611392693138785, Train Raw Loss: 3.7664473675191403, Validation Loss: 5.469629764556885\n",
      "Epoch 1579, Train Loss: 4.577441295774447, Train Raw Loss: 3.737147647018234, Validation Loss: 5.5300517082214355\n",
      "Epoch 1580, Train Loss: 4.605736060274972, Train Raw Loss: 3.7656446737961637, Validation Loss: 5.468523025512695\n",
      "Epoch 1581, Train Loss: 4.590565691267451, Train Raw Loss: 3.752600967677103, Validation Loss: 5.486038684844971\n",
      "Epoch 1582, Train Loss: 4.5966800533235075, Train Raw Loss: 3.758835137883822, Validation Loss: 5.47178840637207\n",
      "Epoch 1583, Train Loss: 4.600751097831461, Train Raw Loss: 3.76070773895416, Validation Loss: 5.446621894836426\n",
      "Epoch 1584, Train Loss: 4.589107677050762, Train Raw Loss: 3.754513427284029, Validation Loss: 5.478654861450195\n",
      "Epoch 1585, Train Loss: 4.59424066286948, Train Raw Loss: 3.755502423271537, Validation Loss: 5.516403675079346\n",
      "Epoch 1586, Train Loss: 4.5962817473957935, Train Raw Loss: 3.753376026658548, Validation Loss: 5.481488227844238\n",
      "Epoch 1587, Train Loss: 4.607338956246774, Train Raw Loss: 3.770754115945763, Validation Loss: 5.509085655212402\n",
      "Epoch 1588, Train Loss: 4.600875687847535, Train Raw Loss: 3.7604158031443755, Validation Loss: 5.473000526428223\n",
      "Epoch 1589, Train Loss: 4.602007629060083, Train Raw Loss: 3.7624709692680174, Validation Loss: 5.47302770614624\n",
      "Epoch 1590, Train Loss: 4.596649575523204, Train Raw Loss: 3.7583759397268297, Validation Loss: 5.48800802230835\n",
      "Epoch 1591, Train Loss: 4.583947766779198, Train Raw Loss: 3.7431208206961553, Validation Loss: 5.475127696990967\n",
      "Epoch 1592, Train Loss: 4.59628251021107, Train Raw Loss: 3.758884512550301, Validation Loss: 5.46862268447876\n",
      "Epoch 1593, Train Loss: 4.602414872166183, Train Raw Loss: 3.765568417848812, Validation Loss: 5.462275981903076\n",
      "Epoch 1594, Train Loss: 4.594417280414039, Train Raw Loss: 3.758216127877434, Validation Loss: 5.45803165435791\n",
      "Epoch 1595, Train Loss: 4.602104432425565, Train Raw Loss: 3.7631765854027535, Validation Loss: 5.474740982055664\n",
      "Epoch 1596, Train Loss: 4.588377561668555, Train Raw Loss: 3.7505196312649383, Validation Loss: 5.45454216003418\n",
      "Epoch 1597, Train Loss: 4.592241835759745, Train Raw Loss: 3.752671244781878, Validation Loss: 5.463418006896973\n",
      "Epoch 1598, Train Loss: 4.582637388176388, Train Raw Loss: 3.7461832797775667, Validation Loss: 5.484886169433594\n",
      "Epoch 1599, Train Loss: 4.582431844704681, Train Raw Loss: 3.7472497107668055, Validation Loss: 5.460169792175293\n",
      "Epoch 1600, Train Loss: 4.597238466060824, Train Raw Loss: 3.7589623761673767, Validation Loss: 5.494832992553711\n",
      "Epoch 1601, Train Loss: 4.585829050176674, Train Raw Loss: 3.74884343130721, Validation Loss: 5.505202770233154\n",
      "Epoch 1602, Train Loss: 4.593749684964617, Train Raw Loss: 3.756861301511526, Validation Loss: 5.475204944610596\n",
      "Epoch 1603, Train Loss: 4.605774518061016, Train Raw Loss: 3.7637039995441834, Validation Loss: 5.498583793640137\n",
      "Epoch 1604, Train Loss: 4.598061227343148, Train Raw Loss: 3.758051678124401, Validation Loss: 5.4778523445129395\n",
      "Epoch 1605, Train Loss: 4.587156390067604, Train Raw Loss: 3.7525966450572015, Validation Loss: 5.4637131690979\n",
      "Epoch 1606, Train Loss: 4.601300851379832, Train Raw Loss: 3.7631847545918493, Validation Loss: 5.477669715881348\n",
      "Epoch 1607, Train Loss: 4.599440899325741, Train Raw Loss: 3.7643167442745633, Validation Loss: 5.484986782073975\n",
      "Epoch 1608, Train Loss: 4.589502222215136, Train Raw Loss: 3.755397282168269, Validation Loss: 5.504048824310303\n",
      "Epoch 1609, Train Loss: 4.6020940959453585, Train Raw Loss: 3.7637590174459747, Validation Loss: 5.455843925476074\n",
      "Epoch 1610, Train Loss: 4.5755355929334955, Train Raw Loss: 3.7436427392893368, Validation Loss: 5.457961082458496\n",
      "Epoch 1611, Train Loss: 4.5919277171707815, Train Raw Loss: 3.7549927039278876, Validation Loss: 5.468504905700684\n",
      "Epoch 1612, Train Loss: 4.602161206884516, Train Raw Loss: 3.7642010154823464, Validation Loss: 5.462387561798096\n",
      "Epoch 1613, Train Loss: 4.588077486306429, Train Raw Loss: 3.7517585932380624, Validation Loss: 5.481083869934082\n",
      "Epoch 1614, Train Loss: 4.583492061454389, Train Raw Loss: 3.7489715857638255, Validation Loss: 5.495993614196777\n",
      "Epoch 1615, Train Loss: 4.595335588314467, Train Raw Loss: 3.7563784068657293, Validation Loss: 5.442403793334961\n",
      "Epoch 1616, Train Loss: 4.593479786854651, Train Raw Loss: 3.7609553427745896, Validation Loss: 5.4616522789001465\n",
      "Epoch 1617, Train Loss: 4.5881964067618055, Train Raw Loss: 3.7506343281931347, Validation Loss: 5.483339786529541\n",
      "Epoch 1618, Train Loss: 4.599088559879197, Train Raw Loss: 3.76266304022736, Validation Loss: 5.452687740325928\n",
      "Epoch 1619, Train Loss: 4.580114230803318, Train Raw Loss: 3.748398395917482, Validation Loss: 5.468255996704102\n",
      "Epoch 1620, Train Loss: 4.583972236969405, Train Raw Loss: 3.751462694009145, Validation Loss: 5.488129138946533\n",
      "Epoch 1621, Train Loss: 4.5993013600922295, Train Raw Loss: 3.762230862552921, Validation Loss: 5.491760730743408\n",
      "Epoch 1622, Train Loss: 4.593908845840229, Train Raw Loss: 3.7543417198376523, Validation Loss: 5.505095958709717\n",
      "Epoch 1623, Train Loss: 4.59320895212392, Train Raw Loss: 3.7530544265276857, Validation Loss: 5.474552154541016\n",
      "Epoch 1624, Train Loss: 4.589157916232944, Train Raw Loss: 3.7540939095533554, Validation Loss: 5.449199199676514\n",
      "Epoch 1625, Train Loss: 4.586946049291226, Train Raw Loss: 3.7573697595960565, Validation Loss: 5.477407455444336\n",
      "Epoch 1626, Train Loss: 4.5878762297332285, Train Raw Loss: 3.7536879900428985, Validation Loss: 5.472758769989014\n",
      "Epoch 1627, Train Loss: 4.588368591459261, Train Raw Loss: 3.7561116493410536, Validation Loss: 5.4347991943359375\n",
      "Epoch 1628, Train Loss: 4.577881105947825, Train Raw Loss: 3.745130510379871, Validation Loss: 5.4707841873168945\n",
      "Epoch 1629, Train Loss: 4.5930036927262945, Train Raw Loss: 3.7606969489819475, Validation Loss: 5.446858882904053\n",
      "Epoch 1630, Train Loss: 4.58798500560224, Train Raw Loss: 3.7513284555325908, Validation Loss: 5.486715316772461\n",
      "Epoch 1631, Train Loss: 4.587586847775512, Train Raw Loss: 3.753592018120819, Validation Loss: 5.470494747161865\n",
      "Epoch 1632, Train Loss: 4.588113764963216, Train Raw Loss: 3.757919945857591, Validation Loss: 5.433359622955322\n",
      "Epoch 1633, Train Loss: 4.59037868036992, Train Raw Loss: 3.7589252120090855, Validation Loss: 5.442922115325928\n",
      "Epoch 1634, Train Loss: 4.588277609563536, Train Raw Loss: 3.7562550361785623, Validation Loss: 5.46214485168457\n",
      "Epoch 1635, Train Loss: 4.583822117083603, Train Raw Loss: 3.7502485332389672, Validation Loss: 5.444593906402588\n",
      "Epoch 1636, Train Loss: 4.582951729413536, Train Raw Loss: 3.752581746959024, Validation Loss: 5.45262336730957\n",
      "Epoch 1637, Train Loss: 4.583013096286191, Train Raw Loss: 3.75288242271377, Validation Loss: 5.4483137130737305\n",
      "Epoch 1638, Train Loss: 4.575055860024359, Train Raw Loss: 3.7478308469471004, Validation Loss: 5.4550251960754395\n",
      "Epoch 1639, Train Loss: 4.577631695651346, Train Raw Loss: 3.7500644877139067, Validation Loss: 5.478527069091797\n",
      "Epoch 1640, Train Loss: 4.581583176801602, Train Raw Loss: 3.7523988177792895, Validation Loss: 5.47211217880249\n",
      "Epoch 1641, Train Loss: 4.587620898129211, Train Raw Loss: 3.755235954539643, Validation Loss: 5.447015762329102\n",
      "Epoch 1642, Train Loss: 4.584260352162851, Train Raw Loss: 3.758900665367643, Validation Loss: 5.451780796051025\n",
      "Epoch 1643, Train Loss: 4.580025124135944, Train Raw Loss: 3.753739122259948, Validation Loss: 5.448472023010254\n",
      "Epoch 1644, Train Loss: 4.584635226883822, Train Raw Loss: 3.7568228853659495, Validation Loss: 5.448241710662842\n",
      "Epoch 1645, Train Loss: 4.574509857719144, Train Raw Loss: 3.745007905198468, Validation Loss: 5.458287715911865\n",
      "Epoch 1646, Train Loss: 4.570692683011293, Train Raw Loss: 3.746043521124456, Validation Loss: 5.467337608337402\n",
      "Epoch 1647, Train Loss: 4.577718686353829, Train Raw Loss: 3.7496077097124525, Validation Loss: 5.4608049392700195\n",
      "Epoch 1648, Train Loss: 4.579928599504961, Train Raw Loss: 3.7515588560452064, Validation Loss: 5.442412853240967\n",
      "Epoch 1649, Train Loss: 4.580827812891868, Train Raw Loss: 3.75049611714979, Validation Loss: 5.426732540130615\n",
      "Epoch 1650, Train Loss: 4.557025991587175, Train Raw Loss: 3.7351918449418413, Validation Loss: 5.484720706939697\n",
      "Epoch 1651, Train Loss: 4.591596531867981, Train Raw Loss: 3.7612236358225344, Validation Loss: 5.464163780212402\n",
      "Epoch 1652, Train Loss: 4.579370791340867, Train Raw Loss: 3.756194078384174, Validation Loss: 5.4524641036987305\n",
      "Epoch 1653, Train Loss: 4.571277014704214, Train Raw Loss: 3.7433352028330167, Validation Loss: 5.442017078399658\n",
      "Epoch 1654, Train Loss: 4.578749203392201, Train Raw Loss: 3.7545472252700063, Validation Loss: 5.4412078857421875\n",
      "Epoch 1655, Train Loss: 4.571822432138854, Train Raw Loss: 3.745663324619333, Validation Loss: 5.433658599853516\n",
      "Epoch 1656, Train Loss: 4.570756873447034, Train Raw Loss: 3.7437170157415998, Validation Loss: 5.480921745300293\n",
      "Epoch 1657, Train Loss: 4.576124942551057, Train Raw Loss: 3.753790060761902, Validation Loss: 5.453372001647949\n",
      "Epoch 1658, Train Loss: 4.570010266080499, Train Raw Loss: 3.7423527461373145, Validation Loss: 5.438418865203857\n",
      "Epoch 1659, Train Loss: 4.567451990561353, Train Raw Loss: 3.744262384209368, Validation Loss: 5.461197376251221\n",
      "Epoch 1660, Train Loss: 4.568262907655702, Train Raw Loss: 3.744209287232823, Validation Loss: 5.4529876708984375\n",
      "Epoch 1661, Train Loss: 4.577836317113704, Train Raw Loss: 3.7493778513951432, Validation Loss: 5.467146873474121\n",
      "Epoch 1662, Train Loss: 4.5762793831113315, Train Raw Loss: 3.7501236265318263, Validation Loss: 5.4302544593811035\n",
      "Epoch 1663, Train Loss: 4.5716953287935915, Train Raw Loss: 3.7483583266950316, Validation Loss: 5.4456024169921875\n",
      "Epoch 1664, Train Loss: 4.574910224849979, Train Raw Loss: 3.7543588255428606, Validation Loss: 5.443560600280762\n",
      "Epoch 1665, Train Loss: 4.571936797350645, Train Raw Loss: 3.748353782378965, Validation Loss: 5.422517776489258\n",
      "Epoch 1666, Train Loss: 4.56537425774667, Train Raw Loss: 3.7453135822382237, Validation Loss: 5.4470906257629395\n",
      "Epoch 1667, Train Loss: 4.573405041918159, Train Raw Loss: 3.749069554772642, Validation Loss: 5.421896934509277\n",
      "Epoch 1668, Train Loss: 4.5728877124273115, Train Raw Loss: 3.7514903059436215, Validation Loss: 5.447268486022949\n",
      "Epoch 1669, Train Loss: 4.573847016195456, Train Raw Loss: 3.7513631115357082, Validation Loss: 5.447790145874023\n",
      "Epoch 1670, Train Loss: 4.568793909872571, Train Raw Loss: 3.7442667220201757, Validation Loss: 5.4367194175720215\n",
      "Epoch 1671, Train Loss: 4.558934179859029, Train Raw Loss: 3.73887678504818, Validation Loss: 5.421535015106201\n",
      "Epoch 1672, Train Loss: 4.572577833301491, Train Raw Loss: 3.7488952368083925, Validation Loss: 5.454929828643799\n",
      "Epoch 1673, Train Loss: 4.576558268318574, Train Raw Loss: 3.7563495744019746, Validation Loss: 5.430994510650635\n",
      "Epoch 1674, Train Loss: 4.5609538399097, Train Raw Loss: 3.739595224087437, Validation Loss: 5.441841125488281\n",
      "Epoch 1675, Train Loss: 4.578970357444551, Train Raw Loss: 3.7570985238585206, Validation Loss: 5.446996688842773\n",
      "Epoch 1676, Train Loss: 4.569054434365697, Train Raw Loss: 3.746814953411619, Validation Loss: 5.440393447875977\n",
      "Epoch 1677, Train Loss: 4.558035959551732, Train Raw Loss: 3.734398153796792, Validation Loss: 5.415808200836182\n",
      "Epoch 1678, Train Loss: 4.56004792108304, Train Raw Loss: 3.7414252329203816, Validation Loss: 5.439807891845703\n",
      "Epoch 1679, Train Loss: 4.5750337223211925, Train Raw Loss: 3.756513594918781, Validation Loss: 5.444395065307617\n",
      "Epoch 1680, Train Loss: 4.5603746970494585, Train Raw Loss: 3.7399239326516787, Validation Loss: 5.457353591918945\n",
      "Epoch 1681, Train Loss: 4.577564165203108, Train Raw Loss: 3.758314554227723, Validation Loss: 5.430890083312988\n",
      "Epoch 1682, Train Loss: 4.564974706785547, Train Raw Loss: 3.744165457495385, Validation Loss: 5.4637556076049805\n",
      "Epoch 1683, Train Loss: 4.570296068779296, Train Raw Loss: 3.749815838577019, Validation Loss: 5.45646333694458\n",
      "Epoch 1684, Train Loss: 4.571424878347251, Train Raw Loss: 3.751461717279421, Validation Loss: 5.439812183380127\n",
      "Epoch 1685, Train Loss: 4.577394931970371, Train Raw Loss: 3.7551597459448707, Validation Loss: 5.421184062957764\n",
      "Epoch 1686, Train Loss: 4.5564589462760425, Train Raw Loss: 3.7422629839844173, Validation Loss: 5.432841777801514\n",
      "Epoch 1687, Train Loss: 4.564124493963188, Train Raw Loss: 3.7439715909047258, Validation Loss: 5.442938804626465\n",
      "Epoch 1688, Train Loss: 4.56610785379178, Train Raw Loss: 3.7450335130923325, Validation Loss: 5.428375720977783\n",
      "Epoch 1689, Train Loss: 4.576943305879832, Train Raw Loss: 3.7566815212782885, Validation Loss: 5.405020236968994\n",
      "Epoch 1690, Train Loss: 4.560072371239463, Train Raw Loss: 3.7446572898576655, Validation Loss: 5.431743621826172\n",
      "Epoch 1691, Train Loss: 4.5732424381292525, Train Raw Loss: 3.754071256973677, Validation Loss: 5.4165358543396\n",
      "Epoch 1692, Train Loss: 4.578530790780982, Train Raw Loss: 3.759463842585683, Validation Loss: 5.391272068023682\n",
      "Epoch 1693, Train Loss: 4.552483867191606, Train Raw Loss: 3.736326689148943, Validation Loss: 5.434858798980713\n",
      "Epoch 1694, Train Loss: 4.559917491426071, Train Raw Loss: 3.7429646090086965, Validation Loss: 5.419417858123779\n",
      "Epoch 1695, Train Loss: 4.554146433869998, Train Raw Loss: 3.740736702539855, Validation Loss: 5.41428804397583\n",
      "Epoch 1696, Train Loss: 4.561110729061895, Train Raw Loss: 3.743946618669563, Validation Loss: 5.405172348022461\n",
      "Epoch 1697, Train Loss: 4.567104210249251, Train Raw Loss: 3.752325512924128, Validation Loss: 5.413839340209961\n",
      "Epoch 1698, Train Loss: 4.559347633272409, Train Raw Loss: 3.7476184150411025, Validation Loss: 5.415963649749756\n",
      "Epoch 1699, Train Loss: 4.558748322559728, Train Raw Loss: 3.743700554759966, Validation Loss: 5.42526388168335\n",
      "Epoch 1700, Train Loss: 4.556860941648483, Train Raw Loss: 3.7399488526913856, Validation Loss: 5.398227691650391\n",
      "Epoch 1701, Train Loss: 4.553297587152985, Train Raw Loss: 3.7399834589411816, Validation Loss: 5.41706657409668\n",
      "Epoch 1702, Train Loss: 4.544557072180841, Train Raw Loss: 3.7318078587038648, Validation Loss: 5.448371410369873\n",
      "Epoch 1703, Train Loss: 4.561991769903236, Train Raw Loss: 3.751432848183645, Validation Loss: 5.441979885101318\n",
      "Epoch 1704, Train Loss: 4.5634758479065365, Train Raw Loss: 3.7486671631534896, Validation Loss: 5.43862771987915\n",
      "Epoch 1705, Train Loss: 4.552553556942278, Train Raw Loss: 3.7371511554966372, Validation Loss: 5.438519477844238\n",
      "Epoch 1706, Train Loss: 4.563139827921987, Train Raw Loss: 3.7493119353635445, Validation Loss: 5.452768325805664\n",
      "Epoch 1707, Train Loss: 4.571334279121624, Train Raw Loss: 3.751624703821209, Validation Loss: 5.446775436401367\n",
      "Epoch 1708, Train Loss: 4.539122096076608, Train Raw Loss: 3.7246143920554053, Validation Loss: 5.443314075469971\n",
      "Epoch 1709, Train Loss: 4.564602922689583, Train Raw Loss: 3.748634004758464, Validation Loss: 5.420163631439209\n",
      "Epoch 1710, Train Loss: 4.562271414407426, Train Raw Loss: 3.7472288737694424, Validation Loss: 5.426577091217041\n",
      "Epoch 1711, Train Loss: 4.563201083615422, Train Raw Loss: 3.7476395446393225, Validation Loss: 5.440056800842285\n",
      "Epoch 1712, Train Loss: 4.552577680390742, Train Raw Loss: 3.744028869064318, Validation Loss: 5.453187465667725\n",
      "Epoch 1713, Train Loss: 4.5593085210356445, Train Raw Loss: 3.7462308315767183, Validation Loss: 5.424437046051025\n",
      "Epoch 1714, Train Loss: 4.564625066353215, Train Raw Loss: 3.748336669595705, Validation Loss: 5.415084362030029\n",
      "Epoch 1715, Train Loss: 4.550303523697787, Train Raw Loss: 3.737970128821002, Validation Loss: 5.461328983306885\n",
      "Epoch 1716, Train Loss: 4.568710854442584, Train Raw Loss: 3.754260583842794, Validation Loss: 5.435117721557617\n",
      "Epoch 1717, Train Loss: 4.552427207595772, Train Raw Loss: 3.7410823221835825, Validation Loss: 5.418107032775879\n",
      "Epoch 1718, Train Loss: 4.541843684721324, Train Raw Loss: 3.7337042538242207, Validation Loss: 5.426441669464111\n",
      "Epoch 1719, Train Loss: 4.551102656498552, Train Raw Loss: 3.740958277798361, Validation Loss: 5.4406914710998535\n",
      "Epoch 1720, Train Loss: 4.559576387372282, Train Raw Loss: 3.7479867654128207, Validation Loss: 5.437655448913574\n",
      "Epoch 1721, Train Loss: 4.5735734635343155, Train Raw Loss: 3.759157260052032, Validation Loss: 5.432248115539551\n",
      "Epoch 1722, Train Loss: 4.556094641652372, Train Raw Loss: 3.74421022242556, Validation Loss: 5.431549549102783\n",
      "Epoch 1723, Train Loss: 4.549857077457839, Train Raw Loss: 3.7392851921005383, Validation Loss: 5.423011779785156\n",
      "Epoch 1724, Train Loss: 4.554321261826488, Train Raw Loss: 3.7455884098592733, Validation Loss: 5.413258075714111\n",
      "Epoch 1725, Train Loss: 4.563437834547626, Train Raw Loss: 3.7505906766487493, Validation Loss: 5.3961262702941895\n",
      "Epoch 1726, Train Loss: 4.553152051857776, Train Raw Loss: 3.7417610068702034, Validation Loss: 5.403470039367676\n",
      "Epoch 1727, Train Loss: 4.548574846030937, Train Raw Loss: 3.74049283158448, Validation Loss: 5.42369270324707\n",
      "Epoch 1728, Train Loss: 4.56365228858259, Train Raw Loss: 3.7496301845543916, Validation Loss: 5.419173240661621\n",
      "Epoch 1729, Train Loss: 4.532591237955623, Train Raw Loss: 3.7227351538836957, Validation Loss: 5.430583953857422\n",
      "Epoch 1730, Train Loss: 4.556112876161933, Train Raw Loss: 3.7461626227531166, Validation Loss: 5.398448944091797\n",
      "Epoch 1731, Train Loss: 4.54079265097777, Train Raw Loss: 3.735322619312339, Validation Loss: 5.421142101287842\n",
      "Epoch 1732, Train Loss: 4.5493374281873304, Train Raw Loss: 3.7413350741896365, Validation Loss: 5.413296222686768\n",
      "Epoch 1733, Train Loss: 4.560997965269618, Train Raw Loss: 3.7497391629964114, Validation Loss: 5.405284881591797\n",
      "Epoch 1734, Train Loss: 4.545161138350765, Train Raw Loss: 3.736297615617514, Validation Loss: 5.410698890686035\n",
      "Epoch 1735, Train Loss: 4.55616535105639, Train Raw Loss: 3.7462080212516917, Validation Loss: 5.4066162109375\n",
      "Epoch 1736, Train Loss: 4.555975618958473, Train Raw Loss: 3.7459903124305938, Validation Loss: 5.386214733123779\n",
      "Epoch 1737, Train Loss: 4.549445332007275, Train Raw Loss: 3.7430416346838076, Validation Loss: 5.416499614715576\n",
      "Epoch 1738, Train Loss: 4.548784519442254, Train Raw Loss: 3.7430871756126485, Validation Loss: 5.426868438720703\n",
      "Epoch 1739, Train Loss: 4.5607179664075375, Train Raw Loss: 3.751045435211725, Validation Loss: 5.405162334442139\n",
      "Epoch 1740, Train Loss: 4.55465387834443, Train Raw Loss: 3.746918425998754, Validation Loss: 5.405121803283691\n",
      "Epoch 1741, Train Loss: 4.552837339747283, Train Raw Loss: 3.742910640397006, Validation Loss: 5.39746618270874\n",
      "Epoch 1742, Train Loss: 4.542179325926635, Train Raw Loss: 3.73161671227879, Validation Loss: 5.388890266418457\n",
      "Epoch 1743, Train Loss: 4.521216758423382, Train Raw Loss: 3.713662143175801, Validation Loss: 5.415890693664551\n",
      "Epoch 1744, Train Loss: 4.549617783021596, Train Raw Loss: 3.744178024555246, Validation Loss: 5.390976905822754\n",
      "Epoch 1745, Train Loss: 4.547441514871187, Train Raw Loss: 3.7387634040166935, Validation Loss: 5.390108585357666\n",
      "Epoch 1746, Train Loss: 4.539378815972142, Train Raw Loss: 3.7333968298302755, Validation Loss: 5.418384552001953\n",
      "Epoch 1747, Train Loss: 4.5565384020407995, Train Raw Loss: 3.7479885516481266, Validation Loss: 5.386364459991455\n",
      "Epoch 1748, Train Loss: 4.526245053898957, Train Raw Loss: 3.722662346603142, Validation Loss: 5.415699481964111\n",
      "Epoch 1749, Train Loss: 4.546117477284538, Train Raw Loss: 3.7409725211146805, Validation Loss: 5.422667980194092\n",
      "Epoch 1750, Train Loss: 4.546402206809985, Train Raw Loss: 3.738947402023607, Validation Loss: 5.430810928344727\n",
      "Epoch 1751, Train Loss: 4.539821290514535, Train Raw Loss: 3.735460677991311, Validation Loss: 5.405996322631836\n",
      "Epoch 1752, Train Loss: 4.547335516868366, Train Raw Loss: 3.74190721495284, Validation Loss: 5.379795551300049\n",
      "Epoch 1753, Train Loss: 4.528995489867198, Train Raw Loss: 3.726132153512703, Validation Loss: 5.408310413360596\n",
      "Epoch 1754, Train Loss: 4.532544382992718, Train Raw Loss: 3.7282348740431996, Validation Loss: 5.3927507400512695\n",
      "Epoch 1755, Train Loss: 4.543937840147151, Train Raw Loss: 3.737883761773507, Validation Loss: 5.387181758880615\n",
      "Epoch 1756, Train Loss: 4.530714899591274, Train Raw Loss: 3.7282838774638045, Validation Loss: 5.406426429748535\n",
      "Epoch 1757, Train Loss: 4.543375491226713, Train Raw Loss: 3.7388871501717302, Validation Loss: 5.409465312957764\n",
      "Epoch 1758, Train Loss: 4.543960335064265, Train Raw Loss: 3.7401195000857115, Validation Loss: 5.442702770233154\n",
      "Epoch 1759, Train Loss: 4.551394843185942, Train Raw Loss: 3.7445777656717434, Validation Loss: 5.395042419433594\n",
      "Epoch 1760, Train Loss: 4.532027175939746, Train Raw Loss: 3.7272619143956236, Validation Loss: 5.40732479095459\n",
      "Epoch 1761, Train Loss: 4.551896535191271, Train Raw Loss: 3.7445422772732044, Validation Loss: 5.397055625915527\n",
      "Epoch 1762, Train Loss: 4.535154187223977, Train Raw Loss: 3.732474806863401, Validation Loss: 5.414299488067627\n",
      "Epoch 1763, Train Loss: 4.541048594853944, Train Raw Loss: 3.739365307945344, Validation Loss: 5.42158842086792\n",
      "Epoch 1764, Train Loss: 4.530535270687607, Train Raw Loss: 3.723764595347974, Validation Loss: 5.397707462310791\n",
      "Epoch 1765, Train Loss: 4.542664579384857, Train Raw Loss: 3.7396053812570043, Validation Loss: 5.425286293029785\n",
      "Epoch 1766, Train Loss: 4.5436979341424175, Train Raw Loss: 3.736273695901036, Validation Loss: 5.407795429229736\n",
      "Epoch 1767, Train Loss: 4.532466511428356, Train Raw Loss: 3.733460554273592, Validation Loss: 5.402778625488281\n",
      "Epoch 1768, Train Loss: 4.540907062507338, Train Raw Loss: 3.7367298413068055, Validation Loss: 5.415146350860596\n",
      "Epoch 1769, Train Loss: 4.554442993220356, Train Raw Loss: 3.752167426298062, Validation Loss: 5.412096977233887\n",
      "Epoch 1770, Train Loss: 4.530544811321629, Train Raw Loss: 3.727055163308978, Validation Loss: 5.419063091278076\n",
      "Epoch 1771, Train Loss: 4.554298036959437, Train Raw Loss: 3.7514902190615733, Validation Loss: 5.420863151550293\n",
      "Epoch 1772, Train Loss: 4.53421936871277, Train Raw Loss: 3.731862619229489, Validation Loss: 5.399971008300781\n",
      "Epoch 1773, Train Loss: 4.543887217798167, Train Raw Loss: 3.7376690855456722, Validation Loss: 5.406286716461182\n",
      "Epoch 1774, Train Loss: 4.552856765107976, Train Raw Loss: 3.7504648032287755, Validation Loss: 5.401072978973389\n",
      "Epoch 1775, Train Loss: 4.5316738999138275, Train Raw Loss: 3.731009919775857, Validation Loss: 5.401352882385254\n",
      "Epoch 1776, Train Loss: 4.544530609622598, Train Raw Loss: 3.74193359406458, Validation Loss: 5.40441370010376\n",
      "Epoch 1777, Train Loss: 4.536944372620847, Train Raw Loss: 3.7366332796298796, Validation Loss: 5.409066677093506\n",
      "Epoch 1778, Train Loss: 4.537862546866139, Train Raw Loss: 3.736352906955613, Validation Loss: 5.410743236541748\n",
      "Epoch 1779, Train Loss: 4.545912542483872, Train Raw Loss: 3.7423483334481715, Validation Loss: 5.383995056152344\n",
      "Epoch 1780, Train Loss: 4.537582980427477, Train Raw Loss: 3.7377648315081995, Validation Loss: 5.4114251136779785\n",
      "Epoch 1781, Train Loss: 4.535492884574665, Train Raw Loss: 3.734530279247297, Validation Loss: 5.408682823181152\n",
      "Epoch 1782, Train Loss: 4.546076566022303, Train Raw Loss: 3.742500399880939, Validation Loss: 5.413613796234131\n",
      "Epoch 1783, Train Loss: 4.544701433264547, Train Raw Loss: 3.743666647705767, Validation Loss: 5.38980770111084\n",
      "Epoch 1784, Train Loss: 4.545672178889315, Train Raw Loss: 3.7442529420471855, Validation Loss: 5.376691818237305\n",
      "Epoch 1785, Train Loss: 4.535896560218599, Train Raw Loss: 3.7348275198290746, Validation Loss: 5.383251190185547\n",
      "Epoch 1786, Train Loss: 4.527150466417273, Train Raw Loss: 3.7309957526624205, Validation Loss: 5.412432670593262\n",
      "Epoch 1787, Train Loss: 4.542669847566221, Train Raw Loss: 3.7408468743993177, Validation Loss: 5.392605304718018\n",
      "Epoch 1788, Train Loss: 4.5401770704322395, Train Raw Loss: 3.7416143119335175, Validation Loss: 5.42989444732666\n",
      "Epoch 1789, Train Loss: 4.533815586980846, Train Raw Loss: 3.73775599528518, Validation Loss: 5.380605220794678\n",
      "Epoch 1790, Train Loss: 4.528628197188179, Train Raw Loss: 3.731718454013268, Validation Loss: 5.379561424255371\n",
      "Epoch 1791, Train Loss: 4.5348811623122955, Train Raw Loss: 3.735295083332393, Validation Loss: 5.396842956542969\n",
      "Epoch 1792, Train Loss: 4.533651557688912, Train Raw Loss: 3.7357830637858975, Validation Loss: 5.406007766723633\n",
      "Epoch 1793, Train Loss: 4.534672319268187, Train Raw Loss: 3.7353461650096706, Validation Loss: 5.378414154052734\n",
      "Epoch 1794, Train Loss: 4.530636084121134, Train Raw Loss: 3.7376208428707387, Validation Loss: 5.403762340545654\n",
      "Epoch 1795, Train Loss: 4.5374704618006945, Train Raw Loss: 3.735009955449237, Validation Loss: 5.3887104988098145\n",
      "Epoch 1796, Train Loss: 4.530291933442156, Train Raw Loss: 3.7343766294005842, Validation Loss: 5.400720596313477\n",
      "Epoch 1797, Train Loss: 4.533624239638447, Train Raw Loss: 3.7370376081102425, Validation Loss: 5.390408515930176\n",
      "Epoch 1798, Train Loss: 4.538663694593642, Train Raw Loss: 3.736383743128843, Validation Loss: 5.390331745147705\n",
      "Epoch 1799, Train Loss: 4.520397259998653, Train Raw Loss: 3.7234147841317786, Validation Loss: 5.37367057800293\n",
      "Epoch 1800, Train Loss: 4.525413912410538, Train Raw Loss: 3.728323149142994, Validation Loss: 5.4094319343566895\n",
      "Epoch 1801, Train Loss: 4.5384280340953005, Train Raw Loss: 3.7406410532279146, Validation Loss: 5.393184185028076\n",
      "Epoch 1802, Train Loss: 4.54484010450542, Train Raw Loss: 3.743790619406435, Validation Loss: 5.366362571716309\n",
      "Epoch 1803, Train Loss: 4.520780421296755, Train Raw Loss: 3.724669135444694, Validation Loss: 5.367815971374512\n",
      "Epoch 1804, Train Loss: 4.518760559252566, Train Raw Loss: 3.7226076940281523, Validation Loss: 5.397026062011719\n",
      "Epoch 1805, Train Loss: 4.530208260276251, Train Raw Loss: 3.7332083410893877, Validation Loss: 5.368745803833008\n",
      "Epoch 1806, Train Loss: 4.539945677419504, Train Raw Loss: 3.7410186072604525, Validation Loss: 5.349108695983887\n",
      "Epoch 1807, Train Loss: 4.5126335247109335, Train Raw Loss: 3.721555607765913, Validation Loss: 5.395745754241943\n",
      "Epoch 1808, Train Loss: 4.5305756386783385, Train Raw Loss: 3.7321821971486013, Validation Loss: 5.352667808532715\n",
      "Epoch 1809, Train Loss: 4.525921546005541, Train Raw Loss: 3.73452500903772, Validation Loss: 5.415849685668945\n",
      "Epoch 1810, Train Loss: 4.527057063662344, Train Raw Loss: 3.731608842396074, Validation Loss: 5.38576602935791\n",
      "Epoch 1811, Train Loss: 4.520577155757281, Train Raw Loss: 3.7276303513182536, Validation Loss: 5.3794145584106445\n",
      "Epoch 1812, Train Loss: 4.520410168709026, Train Raw Loss: 3.730926143253843, Validation Loss: 5.387234210968018\n",
      "Epoch 1813, Train Loss: 4.51985406908724, Train Raw Loss: 3.7280841617120637, Validation Loss: 5.384435653686523\n",
      "Epoch 1814, Train Loss: 4.526450570714142, Train Raw Loss: 3.73492748008834, Validation Loss: 5.399306297302246\n",
      "Epoch 1815, Train Loss: 4.527379498175449, Train Raw Loss: 3.732034864193863, Validation Loss: 5.3963494300842285\n",
      "Epoch 1816, Train Loss: 4.5348212901088925, Train Raw Loss: 3.7431226265099315, Validation Loss: 5.385475158691406\n",
      "Epoch 1817, Train Loss: 4.523462814465165, Train Raw Loss: 3.7321953715549574, Validation Loss: 5.385114669799805\n",
      "Epoch 1818, Train Loss: 4.528781070643001, Train Raw Loss: 3.7330298810783358, Validation Loss: 5.381436347961426\n",
      "Epoch 1819, Train Loss: 4.510772124595112, Train Raw Loss: 3.7229690581767096, Validation Loss: 5.379441261291504\n",
      "Epoch 1820, Train Loss: 4.521792544383142, Train Raw Loss: 3.7292752330915797, Validation Loss: 5.394509315490723\n",
      "Epoch 1821, Train Loss: 4.519792754534218, Train Raw Loss: 3.729219163912866, Validation Loss: 5.396003246307373\n",
      "Epoch 1822, Train Loss: 4.53812136799097, Train Raw Loss: 3.7419926477803123, Validation Loss: 5.374888896942139\n",
      "Epoch 1823, Train Loss: 4.518177395189801, Train Raw Loss: 3.7281486268258757, Validation Loss: 5.400590896606445\n",
      "Epoch 1824, Train Loss: 4.528288725473814, Train Raw Loss: 3.7362891734060315, Validation Loss: 5.3834099769592285\n",
      "Epoch 1825, Train Loss: 4.525506799171368, Train Raw Loss: 3.732045487811168, Validation Loss: 5.372884750366211\n",
      "Epoch 1826, Train Loss: 4.523578612133861, Train Raw Loss: 3.730954277039402, Validation Loss: 5.370587348937988\n",
      "Epoch 1827, Train Loss: 4.514989954315954, Train Raw Loss: 3.7216514327045944, Validation Loss: 5.371514320373535\n",
      "Epoch 1828, Train Loss: 4.528710960182878, Train Raw Loss: 3.7376504508985415, Validation Loss: 5.386151313781738\n",
      "Epoch 1829, Train Loss: 4.521855918938915, Train Raw Loss: 3.722364904938473, Validation Loss: 5.403459548950195\n",
      "Epoch 1830, Train Loss: 4.52378088037173, Train Raw Loss: 3.7328970010081926, Validation Loss: 5.396870136260986\n",
      "Epoch 1831, Train Loss: 4.537477856336368, Train Raw Loss: 3.7466314211901692, Validation Loss: 5.377731800079346\n",
      "Epoch 1832, Train Loss: 4.519664017607768, Train Raw Loss: 3.7320263068295185, Validation Loss: 5.38538122177124\n",
      "Epoch 1833, Train Loss: 4.5294890250182815, Train Raw Loss: 3.736691782457961, Validation Loss: 5.360686779022217\n",
      "Epoch 1834, Train Loss: 4.518744759220216, Train Raw Loss: 3.7310525133377976, Validation Loss: 5.387130260467529\n",
      "Epoch 1835, Train Loss: 4.493612794536683, Train Raw Loss: 3.705997776405679, Validation Loss: 5.370087146759033\n",
      "Epoch 1836, Train Loss: 4.516594982685314, Train Raw Loss: 3.728538795436422, Validation Loss: 5.388732433319092\n",
      "Epoch 1837, Train Loss: 4.521260680879156, Train Raw Loss: 3.734038730089863, Validation Loss: 5.38527774810791\n",
      "Epoch 1838, Train Loss: 4.525871941861179, Train Raw Loss: 3.7351320241060524, Validation Loss: 5.375242233276367\n",
      "Epoch 1839, Train Loss: 4.522950245357222, Train Raw Loss: 3.7332083959960274, Validation Loss: 5.361264228820801\n",
      "Epoch 1840, Train Loss: 4.517911888617609, Train Raw Loss: 3.7305965626405344, Validation Loss: 5.3820905685424805\n",
      "Epoch 1841, Train Loss: 4.521492673332492, Train Raw Loss: 3.732420623385244, Validation Loss: 5.368378162384033\n",
      "Epoch 1842, Train Loss: 4.510590156291922, Train Raw Loss: 3.719591293007963, Validation Loss: 5.3519206047058105\n",
      "Epoch 1843, Train Loss: 4.507204521571596, Train Raw Loss: 3.718173464718792, Validation Loss: 5.412398815155029\n",
      "Epoch 1844, Train Loss: 4.531186604582601, Train Raw Loss: 3.7411405203243095, Validation Loss: 5.397526741027832\n",
      "Epoch 1845, Train Loss: 4.504854375041194, Train Raw Loss: 3.7191934670425124, Validation Loss: 5.378007888793945\n",
      "Epoch 1846, Train Loss: 4.516696460131142, Train Raw Loss: 3.731150089183615, Validation Loss: 5.387889862060547\n",
      "Epoch 1847, Train Loss: 4.521590503512157, Train Raw Loss: 3.7313179140703547, Validation Loss: 5.361843109130859\n",
      "Epoch 1848, Train Loss: 4.505288094364935, Train Raw Loss: 3.720673672668636, Validation Loss: 5.394558906555176\n",
      "Epoch 1849, Train Loss: 4.508921199374729, Train Raw Loss: 3.7241044417851503, Validation Loss: 5.3634562492370605\n",
      "Epoch 1850, Train Loss: 4.5230994308574335, Train Raw Loss: 3.735328162047598, Validation Loss: 5.403276443481445\n",
      "Epoch 1851, Train Loss: 4.514473760417766, Train Raw Loss: 3.725957807319032, Validation Loss: 5.383543014526367\n",
      "Epoch 1852, Train Loss: 4.522884635792838, Train Raw Loss: 3.7379055334048137, Validation Loss: 5.378424167633057\n",
      "Epoch 1853, Train Loss: 4.508163077922331, Train Raw Loss: 3.723799902925061, Validation Loss: 5.3824028968811035\n",
      "Epoch 1854, Train Loss: 4.524481777018971, Train Raw Loss: 3.7367511730641128, Validation Loss: 5.380660533905029\n",
      "Epoch 1855, Train Loss: 4.532631712158521, Train Raw Loss: 3.742105017354091, Validation Loss: 5.363703727722168\n",
      "Epoch 1856, Train Loss: 4.507404239806864, Train Raw Loss: 3.721237972834044, Validation Loss: 5.368548393249512\n",
      "Epoch 1857, Train Loss: 4.510874325657884, Train Raw Loss: 3.723820302469863, Validation Loss: 5.394843578338623\n",
      "Epoch 1858, Train Loss: 4.523827421789368, Train Raw Loss: 3.7390050465861955, Validation Loss: 5.351447105407715\n",
      "Epoch 1859, Train Loss: 4.506901922946175, Train Raw Loss: 3.7229444770349396, Validation Loss: 5.388026714324951\n",
      "Epoch 1860, Train Loss: 4.52837873444789, Train Raw Loss: 3.74421066161659, Validation Loss: 5.3726606369018555\n",
      "Epoch 1861, Train Loss: 4.5235273785889145, Train Raw Loss: 3.735541100282636, Validation Loss: 5.38062047958374\n",
      "Epoch 1862, Train Loss: 4.525980096020632, Train Raw Loss: 3.736365587512652, Validation Loss: 5.355852127075195\n",
      "Epoch 1863, Train Loss: 4.503244227626258, Train Raw Loss: 3.7195818799237412, Validation Loss: 5.35745096206665\n",
      "Epoch 1864, Train Loss: 4.515790380992823, Train Raw Loss: 3.7267507589939566, Validation Loss: 5.337040901184082\n",
      "Epoch 1865, Train Loss: 4.498227362914218, Train Raw Loss: 3.717110966725482, Validation Loss: 5.3764424324035645\n",
      "Epoch 1866, Train Loss: 4.505057963604728, Train Raw Loss: 3.726528698951006, Validation Loss: 5.382882118225098\n",
      "Epoch 1867, Train Loss: 4.523424401548174, Train Raw Loss: 3.7378024816926985, Validation Loss: 5.375106334686279\n",
      "Epoch 1868, Train Loss: 4.5064276791695095, Train Raw Loss: 3.7231880479181805, Validation Loss: 5.398750305175781\n",
      "Epoch 1869, Train Loss: 4.515378373157647, Train Raw Loss: 3.732745657944017, Validation Loss: 5.383471965789795\n",
      "Epoch 1870, Train Loss: 4.525490617834859, Train Raw Loss: 3.7405466672860914, Validation Loss: 5.35904598236084\n",
      "Epoch 1871, Train Loss: 4.513600577579604, Train Raw Loss: 3.7267311431881454, Validation Loss: 5.36152458190918\n",
      "Epoch 1872, Train Loss: 4.5149194403654995, Train Raw Loss: 3.7282407749444246, Validation Loss: 5.35228967666626\n",
      "Epoch 1873, Train Loss: 4.499792118536101, Train Raw Loss: 3.718693844394551, Validation Loss: 5.365692615509033\n",
      "Epoch 1874, Train Loss: 4.51270474013355, Train Raw Loss: 3.7309507234642902, Validation Loss: 5.362293720245361\n",
      "Epoch 1875, Train Loss: 4.5158602370570105, Train Raw Loss: 3.730877547379997, Validation Loss: 5.365318775177002\n",
      "Epoch 1876, Train Loss: 4.51661085250477, Train Raw Loss: 3.7321637430952657, Validation Loss: 5.375479698181152\n",
      "Epoch 1877, Train Loss: 4.510784978626503, Train Raw Loss: 3.72724175873316, Validation Loss: 5.382914066314697\n",
      "Epoch 1878, Train Loss: 4.5053267699976765, Train Raw Loss: 3.7232805501669644, Validation Loss: 5.372011184692383\n",
      "Epoch 1879, Train Loss: 4.517825519873036, Train Raw Loss: 3.7335405671762096, Validation Loss: 5.34229850769043\n",
      "Epoch 1880, Train Loss: 4.504994769394398, Train Raw Loss: 3.725935200498336, Validation Loss: 5.360123634338379\n",
      "Epoch 1881, Train Loss: 4.504895079218679, Train Raw Loss: 3.7220320090651513, Validation Loss: 5.35324764251709\n",
      "Epoch 1882, Train Loss: 4.5021723847422335, Train Raw Loss: 3.721342226345506, Validation Loss: 5.418826103210449\n",
      "Epoch 1883, Train Loss: 4.533214507997036, Train Raw Loss: 3.7476250782608984, Validation Loss: 5.3515238761901855\n",
      "Epoch 1884, Train Loss: 4.506934784476956, Train Raw Loss: 3.7259716764506368, Validation Loss: 5.3694071769714355\n",
      "Epoch 1885, Train Loss: 4.519498552713129, Train Raw Loss: 3.734467039340072, Validation Loss: 5.370160102844238\n",
      "Epoch 1886, Train Loss: 4.5056577271885345, Train Raw Loss: 3.7248585803641214, Validation Loss: 5.364548206329346\n",
      "Epoch 1887, Train Loss: 4.494062147620651, Train Raw Loss: 3.712129545584321, Validation Loss: 5.375152587890625\n",
      "Epoch 1888, Train Loss: 4.496855650014347, Train Raw Loss: 3.7165358384450276, Validation Loss: 5.395938873291016\n",
      "Epoch 1889, Train Loss: 4.513463227326671, Train Raw Loss: 3.731427590093679, Validation Loss: 5.335874557495117\n",
      "Epoch 1890, Train Loss: 4.507268962264061, Train Raw Loss: 3.7268712290045287, Validation Loss: 5.352683067321777\n",
      "Epoch 1891, Train Loss: 4.505752903554175, Train Raw Loss: 3.7216968775830335, Validation Loss: 5.351749420166016\n",
      "Epoch 1892, Train Loss: 4.509711955487728, Train Raw Loss: 3.7312125510225695, Validation Loss: 5.380267143249512\n",
      "Epoch 1893, Train Loss: 4.492333073996836, Train Raw Loss: 3.717484424387415, Validation Loss: 5.402948379516602\n",
      "Epoch 1894, Train Loss: 4.517492587160733, Train Raw Loss: 3.735057334601879, Validation Loss: 5.390899181365967\n",
      "Epoch 1895, Train Loss: 4.505815933768948, Train Raw Loss: 3.72260652627382, Validation Loss: 5.364116191864014\n",
      "Epoch 1896, Train Loss: 4.506432499695156, Train Raw Loss: 3.726821542614036, Validation Loss: 5.362264633178711\n",
      "Epoch 1897, Train Loss: 4.493773076186577, Train Raw Loss: 3.713810542681151, Validation Loss: 5.367712497711182\n",
      "Epoch 1898, Train Loss: 4.511665621855193, Train Raw Loss: 3.732120869681239, Validation Loss: 5.325747489929199\n",
      "Epoch 1899, Train Loss: 4.492897348726789, Train Raw Loss: 3.716418691724539, Validation Loss: 5.36151123046875\n",
      "Epoch 1900, Train Loss: 4.494094348698854, Train Raw Loss: 3.7163992003848154, Validation Loss: 5.375876426696777\n",
      "Epoch 1901, Train Loss: 4.498861002963451, Train Raw Loss: 3.72121237003141, Validation Loss: 5.394292831420898\n",
      "Epoch 1902, Train Loss: 4.517655643862155, Train Raw Loss: 3.7362652686734994, Validation Loss: 5.354586124420166\n",
      "Epoch 1903, Train Loss: 4.50303787096507, Train Raw Loss: 3.7219835858792067, Validation Loss: 5.344378471374512\n",
      "Epoch 1904, Train Loss: 4.493553264066577, Train Raw Loss: 3.71829685167306, Validation Loss: 5.37779426574707\n",
      "Epoch 1905, Train Loss: 4.506824124273327, Train Raw Loss: 3.726927642317282, Validation Loss: 5.355240345001221\n",
      "Epoch 1906, Train Loss: 4.505194475087855, Train Raw Loss: 3.7252382235394585, Validation Loss: 5.334507942199707\n",
      "Epoch 1907, Train Loss: 4.49414464044902, Train Raw Loss: 3.7209172579563328, Validation Loss: 5.349653244018555\n",
      "Epoch 1908, Train Loss: 4.486839342945152, Train Raw Loss: 3.710310100018978, Validation Loss: 5.345709323883057\n",
      "Epoch 1909, Train Loss: 4.491084184290634, Train Raw Loss: 3.709864266961813, Validation Loss: 5.372293472290039\n",
      "Epoch 1910, Train Loss: 4.5074556738966045, Train Raw Loss: 3.7319738337977064, Validation Loss: 5.338496208190918\n",
      "Epoch 1911, Train Loss: 4.486169808233778, Train Raw Loss: 3.7116462932692635, Validation Loss: 5.377712726593018\n",
      "Epoch 1912, Train Loss: 4.493293229490519, Train Raw Loss: 3.71631402903133, Validation Loss: 5.358159065246582\n",
      "Epoch 1913, Train Loss: 4.508851412021452, Train Raw Loss: 3.7277750155578055, Validation Loss: 5.359471321105957\n",
      "Epoch 1914, Train Loss: 4.504641095217731, Train Raw Loss: 3.728813401402699, Validation Loss: 5.3690996170043945\n",
      "Epoch 1915, Train Loss: 4.504460289205114, Train Raw Loss: 3.728387639236947, Validation Loss: 5.362570285797119\n",
      "Epoch 1916, Train Loss: 4.4934673236476055, Train Raw Loss: 3.7169155183765623, Validation Loss: 5.351171970367432\n",
      "Epoch 1917, Train Loss: 4.498240077247222, Train Raw Loss: 3.7226073193674285, Validation Loss: 5.378951549530029\n",
      "Epoch 1918, Train Loss: 4.507900231372979, Train Raw Loss: 3.729973181295726, Validation Loss: 5.338019371032715\n",
      "Epoch 1919, Train Loss: 4.497381868461768, Train Raw Loss: 3.7227018803772, Validation Loss: 5.33411169052124\n",
      "Epoch 1920, Train Loss: 4.503254341002967, Train Raw Loss: 3.7274415949980417, Validation Loss: 5.3533935546875\n",
      "Epoch 1921, Train Loss: 4.500561711357699, Train Raw Loss: 3.72663499183125, Validation Loss: 5.373239517211914\n",
      "Epoch 1922, Train Loss: 4.509727183646626, Train Raw Loss: 3.7301814991980793, Validation Loss: 5.380904197692871\n",
      "Epoch 1923, Train Loss: 4.506560363661912, Train Raw Loss: 3.730909949541092, Validation Loss: 5.371561050415039\n",
      "Epoch 1924, Train Loss: 4.507324812726842, Train Raw Loss: 3.7307346651951474, Validation Loss: 5.387400150299072\n",
      "Epoch 1925, Train Loss: 4.491963611129258, Train Raw Loss: 3.7142765178448625, Validation Loss: 5.372805595397949\n",
      "Epoch 1926, Train Loss: 4.5025519684371025, Train Raw Loss: 3.7273730162531136, Validation Loss: 5.367920398712158\n",
      "Epoch 1927, Train Loss: 4.499503256670303, Train Raw Loss: 3.7213327329191896, Validation Loss: 5.359670162200928\n",
      "Epoch 1928, Train Loss: 4.488415425684717, Train Raw Loss: 3.716300813129379, Validation Loss: 5.362424850463867\n",
      "Epoch 1929, Train Loss: 4.489505208532015, Train Raw Loss: 3.7164404079318047, Validation Loss: 5.347762584686279\n",
      "Epoch 1930, Train Loss: 4.48877988619109, Train Raw Loss: 3.715471165916986, Validation Loss: 5.356569766998291\n",
      "Epoch 1931, Train Loss: 4.507782507439455, Train Raw Loss: 3.7320906828675007, Validation Loss: 5.3503007888793945\n",
      "Epoch 1932, Train Loss: 4.492627487455805, Train Raw Loss: 3.7193587413678566, Validation Loss: 5.336714267730713\n",
      "Epoch 1933, Train Loss: 4.495331305058466, Train Raw Loss: 3.7220101105670134, Validation Loss: 5.36931037902832\n",
      "Epoch 1934, Train Loss: 4.494585061528617, Train Raw Loss: 3.719076336878869, Validation Loss: 5.352310657501221\n",
      "Epoch 1935, Train Loss: 4.4887973611967436, Train Raw Loss: 3.7171304165075223, Validation Loss: 5.338624954223633\n",
      "Epoch 1936, Train Loss: 4.4786587458517815, Train Raw Loss: 3.7065907527589137, Validation Loss: 5.338319301605225\n",
      "Epoch 1937, Train Loss: 4.4927787388364475, Train Raw Loss: 3.720351951610711, Validation Loss: 5.337618350982666\n",
      "Epoch 1938, Train Loss: 4.498865181207657, Train Raw Loss: 3.7286227045787705, Validation Loss: 5.337146282196045\n",
      "Epoch 1939, Train Loss: 4.490644445353084, Train Raw Loss: 3.7209868946423135, Validation Loss: 5.338912487030029\n",
      "Epoch 1940, Train Loss: 4.507187645261486, Train Raw Loss: 3.733431033500367, Validation Loss: 5.328681945800781\n",
      "Epoch 1941, Train Loss: 4.489981037626664, Train Raw Loss: 3.719093624005715, Validation Loss: 5.322970867156982\n",
      "Epoch 1942, Train Loss: 4.474842699575755, Train Raw Loss: 3.7053437903109523, Validation Loss: 5.34188175201416\n",
      "Epoch 1943, Train Loss: 4.478122854563925, Train Raw Loss: 3.708179714365138, Validation Loss: 5.404685020446777\n",
      "Epoch 1944, Train Loss: 4.493860022475322, Train Raw Loss: 3.7227290434969795, Validation Loss: 5.347940444946289\n",
      "Epoch 1945, Train Loss: 4.48416391118533, Train Raw Loss: 3.712551561784413, Validation Loss: 5.315676689147949\n",
      "Epoch 1946, Train Loss: 4.484728066540427, Train Raw Loss: 3.7129624360137514, Validation Loss: 5.319502830505371\n",
      "Epoch 1947, Train Loss: 4.490213960616125, Train Raw Loss: 3.720514242682192, Validation Loss: 5.309867858886719\n",
      "Epoch 1948, Train Loss: 4.463310054524078, Train Raw Loss: 3.698909287278851, Validation Loss: 5.371969699859619\n",
      "Epoch 1949, Train Loss: 4.498075056903892, Train Raw Loss: 3.7264806887755793, Validation Loss: 5.352357864379883\n",
      "Epoch 1950, Train Loss: 4.497222877252433, Train Raw Loss: 3.727921904830469, Validation Loss: 5.372247219085693\n",
      "Epoch 1951, Train Loss: 4.474272985632221, Train Raw Loss: 3.7049070874849956, Validation Loss: 5.340339660644531\n",
      "Epoch 1952, Train Loss: 4.480110351741314, Train Raw Loss: 3.708225250326925, Validation Loss: 5.3380866050720215\n",
      "Epoch 1953, Train Loss: 4.491686229863101, Train Raw Loss: 3.722998492461112, Validation Loss: 5.331074237823486\n",
      "Epoch 1954, Train Loss: 4.488314887012044, Train Raw Loss: 3.718211708217859, Validation Loss: 5.3617353439331055\n",
      "Epoch 1955, Train Loss: 4.4834937527775764, Train Raw Loss: 3.714768786438637, Validation Loss: 5.351309299468994\n",
      "Epoch 1956, Train Loss: 4.495738197904494, Train Raw Loss: 3.724391245552235, Validation Loss: 5.345195770263672\n",
      "Epoch 1957, Train Loss: 4.487274517118931, Train Raw Loss: 3.7185760616428323, Validation Loss: 5.34404993057251\n",
      "Epoch 1958, Train Loss: 4.48099318780005, Train Raw Loss: 3.7081817743471928, Validation Loss: 5.357369422912598\n",
      "Epoch 1959, Train Loss: 4.4936342196746, Train Raw Loss: 3.723449086604847, Validation Loss: 5.337441444396973\n",
      "Epoch 1960, Train Loss: 4.497669404620925, Train Raw Loss: 3.725848845268289, Validation Loss: 5.329652309417725\n",
      "Epoch 1961, Train Loss: 4.485077397980624, Train Raw Loss: 3.7143552161339257, Validation Loss: 5.376531600952148\n",
      "Epoch 1962, Train Loss: 4.498038698567284, Train Raw Loss: 3.729784090816975, Validation Loss: 5.332525730133057\n",
      "Epoch 1963, Train Loss: 4.492103011906147, Train Raw Loss: 3.7208679085390437, Validation Loss: 5.322631359100342\n",
      "Epoch 1964, Train Loss: 4.482950434419844, Train Raw Loss: 3.7174252977180813, Validation Loss: 5.356696128845215\n",
      "Epoch 1965, Train Loss: 4.480375179358655, Train Raw Loss: 3.7102246459987427, Validation Loss: 5.359080791473389\n",
      "Epoch 1966, Train Loss: 4.4930223449236815, Train Raw Loss: 3.7265208420654137, Validation Loss: 5.354928970336914\n",
      "Epoch 1967, Train Loss: 4.485515594813559, Train Raw Loss: 3.7187727401653925, Validation Loss: 5.346649169921875\n",
      "Epoch 1968, Train Loss: 4.477511798714598, Train Raw Loss: 3.7062521600888836, Validation Loss: 5.365482330322266\n",
      "Epoch 1969, Train Loss: 4.497112341556284, Train Raw Loss: 3.729001228304373, Validation Loss: 5.3488383293151855\n",
      "Epoch 1970, Train Loss: 4.488822902449304, Train Raw Loss: 3.7198797749976316, Validation Loss: 5.316690444946289\n",
      "Epoch 1971, Train Loss: 4.474592343552245, Train Raw Loss: 3.7095792125082676, Validation Loss: 5.349259853363037\n",
      "Epoch 1972, Train Loss: 4.50208932471772, Train Raw Loss: 3.734765020592345, Validation Loss: 5.332574367523193\n",
      "Epoch 1973, Train Loss: 4.485171017133528, Train Raw Loss: 3.7209571830721364, Validation Loss: 5.351380825042725\n",
      "Epoch 1974, Train Loss: 4.487331183627248, Train Raw Loss: 3.7179711737566525, Validation Loss: 5.343008518218994\n",
      "Epoch 1975, Train Loss: 4.483159234498938, Train Raw Loss: 3.7152474171585506, Validation Loss: 5.3233819007873535\n",
      "Epoch 1976, Train Loss: 4.485267687299185, Train Raw Loss: 3.7179700133287246, Validation Loss: 5.348180770874023\n",
      "Epoch 1977, Train Loss: 4.480215582748254, Train Raw Loss: 3.7155316958824796, Validation Loss: 5.329020023345947\n",
      "Epoch 1978, Train Loss: 4.475770799111989, Train Raw Loss: 3.7110332871476808, Validation Loss: 5.345626354217529\n",
      "Epoch 1979, Train Loss: 4.487364234733913, Train Raw Loss: 3.7219691059448654, Validation Loss: 5.308536529541016\n",
      "Epoch 1980, Train Loss: 4.472133699266447, Train Raw Loss: 3.7096921817296082, Validation Loss: 5.3540167808532715\n",
      "Epoch 1981, Train Loss: 4.477104178484943, Train Raw Loss: 3.710859121340844, Validation Loss: 5.343562126159668\n",
      "Epoch 1982, Train Loss: 4.489566573955947, Train Raw Loss: 3.7202952549689345, Validation Loss: 5.336288928985596\n",
      "Epoch 1983, Train Loss: 4.4854090957178006, Train Raw Loss: 3.721556962778171, Validation Loss: 5.345672130584717\n",
      "Epoch 1984, Train Loss: 4.481038104328844, Train Raw Loss: 3.715206510780586, Validation Loss: 5.336503982543945\n",
      "Epoch 1985, Train Loss: 4.490803456140889, Train Raw Loss: 3.727831444868611, Validation Loss: 5.3214311599731445\n",
      "Epoch 1986, Train Loss: 4.47713644579053, Train Raw Loss: 3.71262042514152, Validation Loss: 5.324972152709961\n",
      "Epoch 1987, Train Loss: 4.487660436083873, Train Raw Loss: 3.7207294929358694, Validation Loss: 5.34659481048584\n",
      "Epoch 1988, Train Loss: 4.4565465071549015, Train Raw Loss: 3.6925510738458898, Validation Loss: 5.354538440704346\n",
      "Epoch 1989, Train Loss: 4.473433182429936, Train Raw Loss: 3.70371570934852, Validation Loss: 5.350170612335205\n",
      "Epoch 1990, Train Loss: 4.482751043596202, Train Raw Loss: 3.718234790902999, Validation Loss: 5.334472179412842\n",
      "Epoch 1991, Train Loss: 4.482786202761862, Train Raw Loss: 3.7159291877514784, Validation Loss: 5.317664623260498\n",
      "Epoch 1992, Train Loss: 4.472753586744269, Train Raw Loss: 3.7127546668466596, Validation Loss: 5.359335899353027\n",
      "Epoch 1993, Train Loss: 4.493820912308163, Train Raw Loss: 3.7257631783270173, Validation Loss: 5.324925899505615\n",
      "Epoch 1994, Train Loss: 4.481916548435887, Train Raw Loss: 3.7180183614707656, Validation Loss: 5.358800411224365\n",
      "Epoch 1995, Train Loss: 4.4914543479681015, Train Raw Loss: 3.724912675656378, Validation Loss: 5.326594829559326\n",
      "Epoch 1996, Train Loss: 4.483378005690045, Train Raw Loss: 3.719301397808724, Validation Loss: 5.328167915344238\n",
      "Epoch 1997, Train Loss: 4.474520031445556, Train Raw Loss: 3.711732906724016, Validation Loss: 5.328111171722412\n",
      "Epoch 1998, Train Loss: 4.481228509172797, Train Raw Loss: 3.717473665707641, Validation Loss: 5.360287666320801\n",
      "Epoch 1999, Train Loss: 4.471954793069098, Train Raw Loss: 3.7074336363209617, Validation Loss: 5.3520121574401855\n",
      "Epoch 2000, Train Loss: 4.467561967174212, Train Raw Loss: 3.709593112539086, Validation Loss: 5.349123477935791\n",
      "Epoch 2001, Train Loss: 4.474220710371932, Train Raw Loss: 3.7086170280559196, Validation Loss: 5.337245941162109\n",
      "Epoch 2002, Train Loss: 4.477932734083798, Train Raw Loss: 3.7146968241780995, Validation Loss: 5.326847076416016\n",
      "Epoch 2003, Train Loss: 4.478894965143668, Train Raw Loss: 3.7153686721291805, Validation Loss: 5.343332767486572\n",
      "Epoch 2004, Train Loss: 4.488472311033143, Train Raw Loss: 3.7254712951266105, Validation Loss: 5.330544471740723\n",
      "Epoch 2005, Train Loss: 4.4776403056250675, Train Raw Loss: 3.7102906046642197, Validation Loss: 5.305355548858643\n",
      "Epoch 2006, Train Loss: 4.461636499274108, Train Raw Loss: 3.7042073220842413, Validation Loss: 5.366208553314209\n",
      "Epoch 2007, Train Loss: 4.479012244401707, Train Raw Loss: 3.7139077126358946, Validation Loss: 5.328954219818115\n",
      "Epoch 2008, Train Loss: 4.460869934658209, Train Raw Loss: 3.699586289479501, Validation Loss: 5.341198444366455\n",
      "Epoch 2009, Train Loss: 4.4733607213530275, Train Raw Loss: 3.7048867211987573, Validation Loss: 5.340756416320801\n",
      "Epoch 2010, Train Loss: 4.470747577233447, Train Raw Loss: 3.709461273873846, Validation Loss: 5.3372578620910645\n",
      "Epoch 2011, Train Loss: 4.474789987049169, Train Raw Loss: 3.7131185591841738, Validation Loss: 5.340652942657471\n",
      "Epoch 2012, Train Loss: 4.4781581060753926, Train Raw Loss: 3.7177707153062025, Validation Loss: 5.329850673675537\n",
      "Epoch 2013, Train Loss: 4.475457627160681, Train Raw Loss: 3.7159931537177826, Validation Loss: 5.316978931427002\n",
      "Epoch 2014, Train Loss: 4.471554220426413, Train Raw Loss: 3.707999515905976, Validation Loss: 5.334559917449951\n",
      "Epoch 2015, Train Loss: 4.466891353039278, Train Raw Loss: 3.7085594551016885, Validation Loss: 5.33687686920166\n",
      "Epoch 2016, Train Loss: 4.479299810487363, Train Raw Loss: 3.715211930581265, Validation Loss: 5.322380065917969\n",
      "Epoch 2017, Train Loss: 4.47249277577632, Train Raw Loss: 3.7138237251175776, Validation Loss: 5.33639669418335\n",
      "Epoch 2018, Train Loss: 4.476608861237764, Train Raw Loss: 3.7168317385638754, Validation Loss: 5.32481575012207\n",
      "Epoch 2019, Train Loss: 4.470626002012027, Train Raw Loss: 3.711895574753483, Validation Loss: 5.342593193054199\n",
      "Epoch 2020, Train Loss: 4.474073932237095, Train Raw Loss: 3.710325944961773, Validation Loss: 5.33526086807251\n",
      "Epoch 2021, Train Loss: 4.473685494065284, Train Raw Loss: 3.7116556625813244, Validation Loss: 5.342873573303223\n",
      "Epoch 2022, Train Loss: 4.470598310563299, Train Raw Loss: 3.7110855803721483, Validation Loss: 5.33933162689209\n",
      "Epoch 2023, Train Loss: 4.468695843219757, Train Raw Loss: 3.706432690595587, Validation Loss: 5.320919513702393\n",
      "Epoch 2024, Train Loss: 4.470616224987639, Train Raw Loss: 3.709745446054472, Validation Loss: 5.323030471801758\n",
      "Epoch 2025, Train Loss: 4.463347544396917, Train Raw Loss: 3.7015967866612804, Validation Loss: 5.285065174102783\n",
      "Epoch 2026, Train Loss: 4.466138271780478, Train Raw Loss: 3.705641761629118, Validation Loss: 5.327079772949219\n",
      "Epoch 2027, Train Loss: 4.455362502733866, Train Raw Loss: 3.697459546683563, Validation Loss: 5.340663433074951\n",
      "Epoch 2028, Train Loss: 4.467017280144824, Train Raw Loss: 3.70634998795059, Validation Loss: 5.341624736785889\n",
      "Epoch 2029, Train Loss: 4.488934504447712, Train Raw Loss: 3.725719864956207, Validation Loss: 5.301582336425781\n",
      "Epoch 2030, Train Loss: 4.461118672788143, Train Raw Loss: 3.703523898911145, Validation Loss: 5.317698001861572\n",
      "Epoch 2031, Train Loss: 4.468031733607252, Train Raw Loss: 3.710517773694462, Validation Loss: 5.336767196655273\n",
      "Epoch 2032, Train Loss: 4.474512814771798, Train Raw Loss: 3.7178703947613636, Validation Loss: 5.33696985244751\n",
      "Epoch 2033, Train Loss: 4.47433061103026, Train Raw Loss: 3.7137237038256394, Validation Loss: 5.32692813873291\n",
      "Epoch 2034, Train Loss: 4.465208212534587, Train Raw Loss: 3.707037352770567, Validation Loss: 5.3561906814575195\n",
      "Epoch 2035, Train Loss: 4.478701309570008, Train Raw Loss: 3.7173181445027392, Validation Loss: 5.327637195587158\n",
      "Epoch 2036, Train Loss: 4.466538638331824, Train Raw Loss: 3.707025510817766, Validation Loss: 5.32236385345459\n",
      "Epoch 2037, Train Loss: 4.461892986007863, Train Raw Loss: 3.7052407469186517, Validation Loss: 5.348978519439697\n",
      "Epoch 2038, Train Loss: 4.466322674312526, Train Raw Loss: 3.7096191982428235, Validation Loss: 5.320794105529785\n",
      "Epoch 2039, Train Loss: 4.463126563818919, Train Raw Loss: 3.7058875066538652, Validation Loss: 5.326939105987549\n",
      "Epoch 2040, Train Loss: 4.4688329591105385, Train Raw Loss: 3.711318279471662, Validation Loss: 5.337498188018799\n",
      "Epoch 2041, Train Loss: 4.475082824710342, Train Raw Loss: 3.714805809325642, Validation Loss: 5.309348106384277\n",
      "Epoch 2042, Train Loss: 4.458009233987994, Train Raw Loss: 3.702121768809027, Validation Loss: 5.326845645904541\n",
      "Epoch 2043, Train Loss: 4.4707180513276, Train Raw Loss: 3.7101205608704024, Validation Loss: 5.299013614654541\n",
      "Epoch 2044, Train Loss: 4.462223796463675, Train Raw Loss: 3.7060958618091213, Validation Loss: 5.301258087158203\n",
      "Epoch 2045, Train Loss: 4.460740710960494, Train Raw Loss: 3.7062748103506036, Validation Loss: 5.3369832038879395\n",
      "Epoch 2046, Train Loss: 4.481151899695396, Train Raw Loss: 3.723743999438981, Validation Loss: 5.299988269805908\n",
      "Epoch 2047, Train Loss: 4.466052745365435, Train Raw Loss: 3.7095748298698004, Validation Loss: 5.310628890991211\n",
      "Epoch 2048, Train Loss: 4.439094315841794, Train Raw Loss: 3.683958961400721, Validation Loss: 5.344925403594971\n",
      "Epoch 2049, Train Loss: 4.463569018700056, Train Raw Loss: 3.7064077966329125, Validation Loss: 5.336990833282471\n",
      "Epoch 2050, Train Loss: 4.461651927895016, Train Raw Loss: 3.704286257094807, Validation Loss: 5.307299613952637\n",
      "Epoch 2051, Train Loss: 4.475268440735009, Train Raw Loss: 3.7201258453643984, Validation Loss: 5.308986186981201\n",
      "Epoch 2052, Train Loss: 4.474535291103853, Train Raw Loss: 3.7190845588429107, Validation Loss: 5.316373825073242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     54\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 55\u001b[0m     \u001b[43mtemp_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Monitor gradients before clipping and stepping\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     all_gradients\u001b[38;5;241m.\u001b[39mappend(store_gradients(model))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_gradients = []\n",
    "raw_losses = []\n",
    "for epoch in range(3000):  # epochs\n",
    "    model.train()  # Set the model to training mode\n",
    "    avg_loss = 0\n",
    "    temp_loss = 0\n",
    "    temp_raw_loss = 0\n",
    "    sample_count = 0\n",
    "\n",
    "    avg_raw_loss = 0\n",
    "    raw_gradients = []\n",
    "    for t in random.sample(train_data, len(train_data)):  # sample through all data in random order each epoch\n",
    "        # Get reconstruction loss to help ground abstract state\n",
    "        decoded_values, all_policies = model(t[0])\n",
    "        decode_loss = F.mse_loss(decoded_values[0], t[0], reduction='sum')\n",
    "\n",
    "        # Get transition probabilities for each state\n",
    "        first_policy = all_policies[0]\n",
    "        second_policy = all_policies[1].view(num_actions, -1)\n",
    "        third_policy = all_policies[2].view(num_actions, num_actions, -1)\n",
    "        fourth_policy = all_policies[3].view(num_actions, num_actions, num_actions, -1)\n",
    "\n",
    "        # These should all add to 1 (in testing there seems to be some small rounding error)\n",
    "        second_layer_probs = first_policy * second_policy\n",
    "        third_layer_probs = second_layer_probs * third_policy\n",
    "        fourth_layer_probs = third_layer_probs * fourth_policy\n",
    "\n",
    "        # Flatten transition probabilities to then weigh with loss of each predicted state at each layer\n",
    "        first = torch.flatten(first_policy).view(num_actions, 1, 1, 1)\n",
    "        second = torch.flatten(second_layer_probs).view(num_actions**2, 1, 1, 1)\n",
    "        third = torch.flatten(third_layer_probs).view(num_actions**3, 1, 1, 1)\n",
    "        fourth = torch.flatten(fourth_layer_probs).view(num_actions**4, 1, 1, 1)\n",
    "\n",
    "        first_loss = (F.mse_loss(decoded_values[1], t[1], reduction='none') * first).sum()\n",
    "        second_loss = (F.mse_loss(decoded_values[2], t[2], reduction='none') * second).sum()\n",
    "        third_loss = (F.mse_loss(decoded_values[3], t[3], reduction='none') * third).sum()\n",
    "        fourth_loss = (F.mse_loss(decoded_values[4], t[4], reduction='none') * fourth).sum()\n",
    "\n",
    "        # For experimenting with different weights on different layers\n",
    "        raw_loss = (first_loss + second_loss + third_loss + fourth_loss).detach().item()\n",
    "        raw_losses.append(raw_loss)\n",
    "        l2w, l3w, l4w = 1, 1, 1\n",
    "        total_loss = first_loss + second_loss * l2w + third_loss * l3w + fourth_loss * l4w + decode_loss\n",
    "\n",
    "        # break if total loss is nan\n",
    "        if torch.isnan(total_loss):\n",
    "            raise ValueError(\"NAN LOSS\")\n",
    "\n",
    "        temp_loss += total_loss\n",
    "        temp_raw_loss += raw_loss\n",
    "        sample_count += 1\n",
    "\n",
    "        if sample_count % 1 == 0:\n",
    "            optimizer.zero_grad()\n",
    "            temp_loss.backward()\n",
    "\n",
    "            # Monitor gradients before clipping and stepping\n",
    "            all_gradients.append(store_gradients(model))\n",
    "\n",
    "            # Uncomment if you want to use gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += temp_loss.item()\n",
    "            avg_raw_loss += temp_raw_loss\n",
    "            temp_loss = 0\n",
    "            temp_raw_loss = 0\n",
    "\n",
    "    # To handle the case where the number of samples is not a multiple of 1\n",
    "    if sample_count % 1 != 0:\n",
    "        optimizer.zero_grad()\n",
    "        temp_loss.backward()\n",
    "        \n",
    "        # Monitor gradients before clipping and stepping\n",
    "        all_gradients.append(store_gradients(model))\n",
    "\n",
    "        # Uncomment if you want to use gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "        optimizer.step()\n",
    "        avg_loss += temp_loss.item()\n",
    "        avg_raw_loss += temp_raw_loss\n",
    "\n",
    "    avg_train_loss = avg_loss / len(train_data)\n",
    "    avg_train_raw_loss = avg_raw_loss / len(train_data)\n",
    "\n",
    "    # Perform validation\n",
    "    avg_valid_loss = validate(model, valid_data)\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss}, Train Raw Loss: {avg_train_raw_loss}, Validation Loss: {avg_valid_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['transition_fun', 0.7972447276115417],\n",
       " ['decoder.fc.weight', 1.0146783590316772],\n",
       " ['decoder.fc.bias', 1.225597858428955],\n",
       " ['decoder.deconv1.weight', 4.208570957183838],\n",
       " ['decoder.deconv1.bias', 0.8115442991256714],\n",
       " ['decoder.deconv2.weight', 0.8129922151565552],\n",
       " ['decoder.deconv2.bias', 0.6843503713607788],\n",
       " ['encoder.cnn_encoder.conv1.weight', 0.5368334054946899],\n",
       " ['encoder.cnn_encoder.conv1.bias', 0.45351430773735046],\n",
       " ['encoder.cnn_encoder.conv2.weight', 1.2930923700332642],\n",
       " ['encoder.cnn_encoder.conv2.bias', 0.7256703972816467],\n",
       " ['encoder.linear.weight', 2.945633888244629],\n",
       " ['encoder.linear.bias', 1.5791183710098267]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.46, 5.3 at 2000\n",
    "all_gradients[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d,q = model(valid_data[0][0]) ##Compare plateau in loss with just 1 action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_path(decoded, all_policies):\n",
    "    best_first_action = all_policies[0].argmax().item()\n",
    "    best_second_action = all_policies[1].view(num_actions,-1)[best_first_action].argmax().item() \n",
    "    best_third_action = all_policies[2].view(num_actions,num_actions,-1)[best_first_action][best_second_action].argmax().item()\n",
    "    best_fourth_action = all_policies[3].view(num_actions,num_actions,num_actions,-1)[best_first_action][best_second_action][best_third_action].argmax().item()\n",
    "    print(best_first_action,best_second_action,best_third_action,best_fourth_action)\n",
    "    first = decoded[0]\n",
    "    second = decoded[1][best_first_action].unsqueeze(0)\n",
    "    third = decoded[2][best_first_action**2 + best_second_action].unsqueeze(0)\n",
    "    fourth = decoded[3][best_first_action**3 + best_second_action**2 + best_third_action].unsqueeze(0)\n",
    "    fifth = decoded[4][best_first_action**4 + best_second_action**3 + best_third_action**2 + best_fourth_action].unsqueeze(0)\n",
    "    return [first,second,third,fourth,fifth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1 1\n",
      "torch.Size([1, 1, 20, 20])\n",
      "torch.Size([1, 1, 20, 20])\n",
      "torch.Size([1, 1, 20, 20])\n",
      "torch.Size([1, 1, 20, 20])\n",
      "torch.Size([1, 1, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "i = get_best_path(d,q)\n",
    "for a in i:\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1 1\n",
      "0 0 0 0\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "0 0 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "1 1 1 1\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "0 0 0 0\n",
      "1 0 1 1\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "1 1 1 1\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "1 0 1 1\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "1 0 0 0\n",
      "1 0 1 1\n",
      "0 0 1 1\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 1 1\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 1 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "1 0 1 1\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 0 0\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 0 0\n",
      "0 0 1 1\n",
      "0 0 0 0\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "1 0 0 0\n",
      "0 0 1 1\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "1 0 1 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 1 1\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 1 1\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "1 1 1 1\n",
      "0 0 1 1\n",
      "0 0 0 0\n",
      "1 0 1 1\n",
      "1 1 1 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "1 1 1 1\n",
      "0 0 1 1\n",
      "0 0 1 1\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 1 1\n",
      "1 0 0 0\n",
      "0 0 1 1\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "1 1 1 1\n",
      "1 1 1 1\n",
      "0 0 0 1\n"
     ]
    }
   ],
   "source": [
    "#try backward in succession?\n",
    "for i in valid_data:\n",
    "    a,b = model(i[0])\n",
    "    get_best_path(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1 1\n",
      "Start\n",
      "tensor([[-0.,  0., -0., -0.,  0.,  0., -0., -0.,  0., -0.],\n",
      "        [-0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0., -0.],\n",
      "        [-0., -0.,  0.,  0., -0.,  0.,  0.,  0.,  0., -0.],\n",
      "        [ 0., -0.,  0., -0., -0.,  0.,  0.,  1.,  1.,  0.],\n",
      "        [ 0.,  0., -0., -0., -1., -1.,  0.,  1.,  1.,  0.],\n",
      "        [-0.,  0.,  0., -0., -1., -1.,  0., -0.,  0., -0.],\n",
      "        [-0.,  0.,  0., -0., -0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0.],\n",
      "        [ 0.,  0., -0.,  0., -0., -0., -0.,  0., -0.,  0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0.]],\n",
      "       grad_fn=<RoundBackward0>)\n",
      "\n",
      "\n",
      "Action 1\n",
      "tensor([[ 0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0., -0.],\n",
      "        [-0.,  0.,  0., -0.,  0., -0.,  0.,  0., -0., -0.],\n",
      "        [-0.,  0.,  0., -0.,  0.,  0.,  0.,  0., -0., -0.],\n",
      "        [-0., -0., -0., -0.,  0.,  0.,  0.,  1.,  0., -0.],\n",
      "        [ 0.,  0.,  0., -0., -1., -1.,  0.,  1.,  0., -0.],\n",
      "        [-0., -0., -0., -0., -1., -1.,  0.,  0.,  0., -0.],\n",
      "        [ 0.,  0., -0., -0., -0.,  0.,  0.,  0., -0.,  0.],\n",
      "        [ 0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0.,  0.],\n",
      "        [-0., -0.,  0., -0., -0., -0., -0.,  0.,  0.,  0.]],\n",
      "       grad_fn=<RoundBackward0>)\n",
      "\n",
      "\n",
      "Action 2\n",
      "tensor([[-0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., -0.,  0., -0., -0., -0., -0.,  0.,  0.],\n",
      "        [ 0.,  0., -0., -0.,  0.,  0.,  0.,  0., -0.,  0.],\n",
      "        [-0., -0., -0., -0.,  0.,  0.,  0., -0., -0., -0.],\n",
      "        [ 0.,  0., -0., -0., -1., -1.,  1.,  0., -0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0., -1., -1.,  1.,  0., -0., -0.],\n",
      "        [ 0.,  0.,  0., -0.,  0.,  0., -0., -0.,  0.,  0.],\n",
      "        [ 0.,  0., -0., -0., -0., -0., -0., -0.,  0.,  0.],\n",
      "        [-0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0., -0.],\n",
      "        [-0., -0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
      "       grad_fn=<RoundBackward0>)\n",
      "\n",
      "\n",
      "Action 3\n",
      "tensor([[ 0.,  0.,  0., -0., -0., -0., -0.,  0.,  0.,  0.],\n",
      "        [-0., -0., -0.,  0., -0., -0.,  0., -0., -0., -0.],\n",
      "        [ 0., -0., -0.,  0.,  0.,  0.,  0.,  0., -0., -0.],\n",
      "        [-0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.],\n",
      "        [ 0.,  0.,  0.,  0., -1., -1.,  1.,  0., -0.,  0.],\n",
      "        [ 0.,  0., -0.,  0., -1., -1.,  1.,  0., -0.,  0.],\n",
      "        [ 0.,  0., -0.,  0., -0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [-0.,  0., -0., -0.,  0.,  0.,  0., -0.,  0.,  0.],\n",
      "        [-0.,  0., -0., -0.,  0.,  0.,  0.,  0.,  0., -0.],\n",
      "        [-0.,  0.,  0., -0., -0.,  0.,  0., -0., -0., -0.]],\n",
      "       grad_fn=<RoundBackward0>)\n",
      "\n",
      "\n",
      "Action\n",
      "tensor([[ 0.,  0.,  0.,  0., -0., -0.,  0., -0., -0.,  0.],\n",
      "        [-0., -0.,  0., -0., -0.,  0.,  0., -0., -0.,  0.],\n",
      "        [ 0., -0.,  0., -0., -0., -0.,  0.,  0., -0.,  0.],\n",
      "        [-0., -0.,  0., -0., -0., -0., -0.,  0.,  0., -0.],\n",
      "        [-0., -0., -0., -0., -1., -1.,  0., -0., -0.,  0.],\n",
      "        [-0.,  0.,  0.,  0., -1., -1.,  0., -0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., -0., -0.,  0.,  0.],\n",
      "        [-0., -0., -0., -0., -0.,  0., -0., -0.,  0., -0.],\n",
      "        [ 0.,  0., -0., -0., -0., -0., -0.,  0.,  0.,  0.],\n",
      "        [-0.,  0.,  0., -0., -0., -0., -0., -0.,  0.,  0.]],\n",
      "       grad_fn=<RoundBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def viewer(tensor):\n",
    "    tensor = tensor.squeeze(0).squeeze(0)\n",
    "    zoomed_tensor = tensor[5:-5, 5:-5]\n",
    "    return torch.round(zoomed_tensor)\n",
    "\n",
    "def print_movie(tensor_list):\n",
    "    for tensor,name in zip(tensor_list,[\"Start\",\"Action 1\",\"Action 2\",\"Action 3\",\"Action\"]):\n",
    "        print(name)\n",
    "        print(viewer(tensor))\n",
    "        print(\"\\n\")\n",
    "\n",
    "print_movie(get_best_path(d,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View Action Weights (This hasn't been informative yet)\n",
    "dec, all_policies = model(train_data[0][0]) \n",
    "# dot = make_dot((dec[0],dec[1],dec[2],dec[3],dec[4],all_policies[0],all_policies[1],all_policies[2],all_policies[3]),params=dict(model.named_parameters()))\n",
    "# dot.render('model', format='png')\n",
    "print(f\"Action Weight Sums { torch.round(model.transition_fun.data,decimals=3).sum(dim=0).sum(dim=0)}\")  #might be summing the wrong way, or just not interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_first_action = all_policies[0].argmax()\n",
    "best_second_action = all_policies[1].view(num_actions,-1)[best_first_action].argmax() \n",
    "best_third_action = all_policies[2].view(num_actions,num_actions,-1)[best_first_action][best_second_action].argmax()\n",
    "best_fourth_action = all_policies[3].view(num_actions,num_actions,num_actions,-1)[best_first_action][best_second_action][best_third_action].argmax() \n",
    "# print(torch.round(all_q[0],decimals=3).detach(), f\"Argmax {all_q[0].argmax().item()}\")\n",
    "# print(torch.round(all_q[1],decimals=3).view(4,-1).detach(),f\"Argmax {all_q[1].view(4,-1)[1].argmax().item()}\")\n",
    "# print(torch.round(all_q[2],decimals=3).view(4,4,-1)[0].detach(),f\"Argmax {all_q[2].view(4,4,-1)[1][0].argmax().item()}\")\n",
    "# print(torch.round(all_q[3],decimals=3).view(4,4,4,-1)[0][0].detach(),f\"Argmax {all_q[3].view(4,4,4,-1)[1][1][0].argmax().item()}\")\n",
    "print(f\"Best Actions: {best_first_action.item()} {best_second_action.item()} {best_third_action.item()} {best_fourth_action.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View Action Weights (This hasn't been informative yet)\n",
    "for i in range(500):\n",
    "    dec, all_policies = model(train_data[i][0]) \n",
    "\n",
    "    best_first_action = all_policies[0].argmax()\n",
    "    best_second_action = all_policies[1].view(4,-1)[best_first_action].argmax() \n",
    "    best_third_action = all_policies[2].view(4,4,-1)[best_first_action][best_second_action].argmax()\n",
    "    best_fourth_action = all_policies[3].view(4,4,4,-1)[best_first_action][best_second_action][best_third_action].argmax() \n",
    "\n",
    "    print(f\"Best Actions: {best_first_action.item()} {best_second_action.item()} {best_third_action.item()} {best_fourth_action.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
