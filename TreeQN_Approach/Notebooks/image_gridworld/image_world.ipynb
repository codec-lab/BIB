{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from treeQN.treeqn_traj_simplest import TreeQN\n",
    "import image_world\n",
    "import random\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = image_world.get_data(size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0025)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(20,20)\n",
    "b = torch.zeros(20,20)\n",
    "b[0][0] = 1\n",
    "F.mse_loss(a,b) #Target Loss should be at least less than this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition Transition\n"
     ]
    }
   ],
   "source": [
    "input_shape = train_data[0][0].shape# minimum size #train_data[0][0].shape\n",
    "num_actions = 4\n",
    "tree_depth = 4\n",
    "embedding_dim = 64\n",
    "gamma = 1 \n",
    "decode_dropout = 0\n",
    "t1 =False#True is Einsum. False +dx \n",
    "model = TreeQN(input_shape=input_shape, num_actions=num_actions, tree_depth=tree_depth, embedding_dim=embedding_dim, gamma=gamma,decode_dropout=decode_dropout,t1=t1)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optional: Pretrain Autoencoder</h1>\n",
    "(Doesn't seem necessary, useful for testing if decoding Z is possible though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_world.train_autoencoder(model,optimizer,train_data,valid_data,epochs=100,lambda_reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #freeze encoder and decoder\n",
    "# for param in model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.decoder.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_autoencode ability\n",
    "# def test_autoencoder(model,valid_data):\n",
    "#     with torch.no_grad():\n",
    "#         test_sample = random.choice(valid_data)\n",
    "#         encoding = model.encoder(test_sample[0])\n",
    "#         decoding = model.decoder(encoding)\n",
    "#         print('Original:\\n', test_sample[0].numpy()[0][0][4:-4, 4:-4])\n",
    "#         print('Reconstructed:\\n', np.round(decoding.numpy()[0][0][4:-4, 4:-4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/Desktop/TreeQN/BIB/TreeQN_Approach/Notebooks/image_gridworld/image_world.py:129: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([4, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  first_loss = (F.mse_loss(decoded_values[1], t[1], reduction='none') * first).sum().item()\n",
      "/home/mike/Desktop/TreeQN/BIB/TreeQN_Approach/Notebooks/image_gridworld/image_world.py:130: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([16, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  second_loss = (F.mse_loss(decoded_values[2], t[2], reduction='none') * second).sum().item()\n",
      "/home/mike/Desktop/TreeQN/BIB/TreeQN_Approach/Notebooks/image_gridworld/image_world.py:131: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([64, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  third_loss = (F.mse_loss(decoded_values[3], t[3], reduction='none') * third).sum().item()\n",
      "/home/mike/Desktop/TreeQN/BIB/TreeQN_Approach/Notebooks/image_gridworld/image_world.py:132: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([256, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  fourth_loss = (F.mse_loss(decoded_values[4], t[4], reduction='none') * fourth).sum().item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss Weighted: 7.272695064544678, | Validation Unweighted Avg: 0.15447017550468445\n",
      "Epoch 1, Total Loss: 1.021253763274713, DLoss: 1.0046975414861332, A1: 0.0034557934427125888, A2: 0.004056082857476378, A3: 0.004442845801399513, A4: 0.004601503271524879\n",
      "Epoch 2, Total Loss: 0.3693698264150457, DLoss: 0.35293838005004957, A1: 0.0034333888557739555, A2: 0.003966094620144842, A3: 0.004424838287840513, A4: 0.004607125037265095\n",
      "Epoch 3, Total Loss: 0.06195463539016518, DLoss: 0.04547949661077424, A1: 0.0035973065405745398, A2: 0.003925986087415368, A3: 0.004386147867295552, A4: 0.004565698431212116\n",
      "Epoch 4, Total Loss: 0.029470828302543273, DLoss: 0.013267396898432211, A1: 0.0035939741668037394, A2: 0.0038410434531132606, A3: 0.00428268563806672, A4: 0.004485728054053404\n",
      "Epoch 5, Total Loss: 0.023157147030261428, DLoss: 0.007292939272222363, A1: 0.003546910475812514, A2: 0.003745620743244548, A3: 0.004178395611234009, A4: 0.004393280828794972\n",
      "Epoch 6, Total Loss: 0.027039371761070057, DLoss: 0.011523000863169066, A1: 0.003494738814928992, A2: 0.0036527196612124416, A3: 0.00407662389122627, A4: 0.004292288634248755\n",
      "Epoch 7, Total Loss: 0.031008049346167934, DLoss: 0.015815599512478167, A1: 0.0034609957746314732, A2: 0.003569864768492566, A3: 0.003972600355998359, A4: 0.00418898888905956\n",
      "Epoch 8, Total Loss: 0.020508713609623638, DLoss: 0.005677564998834648, A1: 0.0034146620045331392, A2: 0.0034721570618619973, A3: 0.0038664198702793905, A4: 0.004077909676231105\n",
      "Epoch 9, Total Loss: 0.029388012999499388, DLoss: 0.014847735218195752, A1: 0.0033924487758089194, A2: 0.0034031005756167527, A3: 0.003770628147123551, A4: 0.003974100288046016\n",
      "Epoch 10, Total Loss: 0.021716158676215194, DLoss: 0.0075563425018282775, A1: 0.0033332017006945202, A2: 0.003308249876665121, A3: 0.0036636238028718666, A4: 0.003854740813205188\n",
      "Epoch 11, Validation Loss Weighted: 5.397978782653809, | Validation Unweighted Avg: 0.003605807665735483\n",
      "Epoch 11, Total Loss: 0.016915645336054944, DLoss: 0.0031856478500148196, A1: 0.003275951657401906, A2: 0.0032067139717665586, A3: 0.003536714461039413, A4: 0.0037106173949062148\n",
      "Epoch 12, Total Loss: 0.01871237951669503, DLoss: 0.005429014657751065, A1: 0.003219455802305178, A2: 0.0031026567702858963, A3: 0.0034102416459724986, A4: 0.003551010649905286\n",
      "Epoch 13, Total Loss: 0.02337378915399313, DLoss: 0.010524099234978414, A1: 0.003172342748042535, A2: 0.0030045004572126677, A3: 0.003285459517924623, A4: 0.003387387110640041\n",
      "Epoch 14, Total Loss: 0.03739364890551025, DLoss: 0.024738550918515432, A1: 0.0031729372980242427, A2: 0.0029737971363250505, A3: 0.003220821914940395, A4: 0.003287541675804691\n",
      "Epoch 15, Total Loss: 0.02017288257960569, DLoss: 0.007909943344896997, A1: 0.003122852916236628, A2: 0.0028890832591886547, A3: 0.0031116321418349717, A4: 0.0031393709603104404\n",
      "Epoch 16, Total Loss: 0.017977385087446732, DLoss: 0.006138839228184555, A1: 0.00307610913137482, A2: 0.002797321310605515, A3: 0.0029921758602457966, A4: 0.0029729396020146933\n",
      "Epoch 17, Total Loss: 0.017694889500059863, DLoss: 0.006281065958848393, A1: 0.0030326353597708725, A2: 0.0027083293661813845, A3: 0.002873183433389799, A4: 0.002799675332128324\n",
      "Epoch 18, Total Loss: 0.023711622243916448, DLoss: 0.012559663221790371, A1: 0.003012024621817876, A2: 0.0026586348222653296, A3: 0.002798159106168896, A4: 0.002683140469757332\n",
      "Epoch 19, Total Loss: 0.023044543992727994, DLoss: 0.0121571327612566, A1: 0.0029706840646791864, A2: 0.0026125631644390523, A3: 0.0027339809680018913, A4: 0.0025701829951933837\n",
      "Epoch 20, Total Loss: 0.017505110487003216, DLoss: 0.006900774024490437, A1: 0.0029249729906124147, A2: 0.002556052271657708, A3: 0.00266068680529398, A4: 0.002462624392302876\n",
      "Epoch 21, Validation Loss Weighted: 4.163536548614502, | Validation Unweighted Avg: 0.0029517323710024357\n",
      "Epoch 21, Total Loss: 0.015318878291344101, DLoss: 0.004960264022123407, A1: 0.002882246783172542, A2: 0.0025093514810908923, A3: 0.0026015726288526574, A4: 0.002365443402562629\n",
      "Epoch 22, Total Loss: 0.017839141469448806, DLoss: 0.007674229219661687, A1: 0.0028418806042860853, A2: 0.002472699989183721, A3: 0.0025567725607702, A4: 0.0022935590907846664\n",
      "Epoch 23, Total Loss: 0.0170518156606704, DLoss: 0.007028451185694642, A1: 0.0028106003999710085, A2: 0.002448647062886845, A3: 0.0025234477520412343, A4: 0.0022406692584891886\n",
      "Epoch 24, Total Loss: 0.014962738811631094, DLoss: 0.005058527819346637, A1: 0.002792475363027982, A2: 0.002425711653830314, A3: 0.002491295158820735, A4: 0.0021947288245428354\n",
      "Epoch 25, Total Loss: 0.01975799324837598, DLoss: 0.009954270847480405, A1: 0.0027632443244907665, A2: 0.0024125407767397436, A3: 0.002466521291485564, A4: 0.002161416089141064\n",
      "Epoch 26, Total Loss: 0.024589623379605737, DLoss: 0.014813187819990244, A1: 0.0027574482109312985, A2: 0.002410497412678193, A3: 0.0024630971073003657, A4: 0.002145392768381333\n",
      "Epoch 27, Total Loss: 0.016527485915205694, DLoss: 0.006875659198108637, A1: 0.0027208144554275683, A2: 0.0023841323955407874, A3: 0.0024349224277433346, A4: 0.002111957420129329\n",
      "Epoch 28, Total Loss: 0.02288251795636659, DLoss: 0.013297338283155113, A1: 0.0027033185173588044, A2: 0.0023737444066103887, A3: 0.002422380215615373, A4: 0.0020857364510778677\n",
      "Epoch 29, Total Loss: 0.013622124785218726, DLoss: 0.004116887990014881, A1: 0.002683637698646635, A2: 0.0023530262321318413, A3: 0.002405005298681896, A4: 0.002063567526850172\n",
      "Epoch 30, Total Loss: 0.015890913515944374, DLoss: 0.006463014369364828, A1: 0.002678533212747425, A2: 0.002332147190199149, A3: 0.002383570983709598, A4: 0.0020336477980229324\n",
      "Epoch 31, Validation Loss Weighted: 3.8747053146362305, | Validation Unweighted Avg: 0.005583794787526131\n",
      "Epoch 31, Total Loss: 0.03011490914293311, DLoss: 0.020655810204334556, A1: 0.0026853833770887417, A2: 0.0023428592037155545, A3: 0.0023940699344331567, A4: 0.0020367862794294276\n",
      "Epoch 32, Total Loss: 0.0156321471049027, DLoss: 0.006247191414612726, A1: 0.0026802586380984973, A2: 0.002322221327234398, A3: 0.0023855607109990987, A4: 0.001996915064625103\n",
      "Epoch 33, Total Loss: 0.011413261645727537, DLoss: 0.002100386909098068, A1: 0.00266061018707908, A2: 0.0023210299561138856, A3: 0.0023670701902698387, A4: 0.0019641643654639746\n",
      "Epoch 34, Total Loss: 0.012196457148952917, DLoss: 0.002970658887484619, A1: 0.0026409375735304574, A2: 0.0023062764666974545, A3: 0.002346773527096957, A4: 0.001931810713986951\n",
      "Epoch 35, Total Loss: 0.02631494974378835, DLoss: 0.017090298608342314, A1: 0.0026382378948090427, A2: 0.002316116744292561, A3: 0.002347108234905384, A4: 0.0019231883581431414\n",
      "Epoch 36, Total Loss: 0.019412707571278918, DLoss: 0.010214230285915123, A1: 0.002632843900937587, A2: 0.0023142288587140766, A3: 0.0023456237628124653, A4: 0.0019057808205781674\n",
      "Epoch 37, Total Loss: 0.01205047531561418, DLoss: 0.0029796188633719627, A1: 0.002590789036317305, A2: 0.0022870283679698, A3: 0.0023227050360715524, A4: 0.0018703340214084494\n",
      "Epoch 38, Total Loss: 0.014356422047554092, DLoss: 0.005356930422765965, A1: 0.002564501145388931, A2: 0.0022740053589752115, A3: 0.00231001053259454, A4: 0.001850974569837986\n",
      "Epoch 39, Total Loss: 0.013824311609972608, DLoss: 0.004880121824680827, A1: 0.002554325010119514, A2: 0.0022666299207644027, A3: 0.002299814152700657, A4: 0.0018234206977385012\n",
      "Epoch 40, Total Loss: 0.020000772703100335, DLoss: 0.011062794630098242, A1: 0.0025442073015834797, A2: 0.0022781003001992677, A3: 0.0023030627084980634, A4: 0.001812607732559131\n",
      "Epoch 41, Validation Loss Weighted: 3.6007978916168213, | Validation Unweighted Avg: 0.002468633931130171\n",
      "Epoch 41, Total Loss: 0.015695830026048827, DLoss: 0.0068033088798983956, A1: 0.0025331281749954955, A2: 0.0022744599952023814, A3: 0.0022950075746683233, A4: 0.0017899254072372885\n",
      "Epoch 42, Total Loss: 0.010340549682521008, DLoss: 0.001566942196256026, A1: 0.002499089028093625, A2: 0.0022518601808273657, A3: 0.00227613949153403, A4: 0.0017465187712911179\n",
      "Epoch 43, Total Loss: 0.013789863321422176, DLoss: 0.005107157558615489, A1: 0.0024756983875043014, A2: 0.0022351624274795706, A3: 0.0022647761898538605, A4: 0.0017070687614084983\n",
      "Epoch 44, Total Loss: 0.018379420317201453, DLoss: 0.009738050747281787, A1: 0.0024653283148919317, A2: 0.002224745764397085, A3: 0.002255625041222877, A4: 0.0016956705070862717\n",
      "Epoch 45, Total Loss: 0.014932644481516696, DLoss: 0.00635940749859649, A1: 0.002449592951134863, A2: 0.002203901521649889, A3: 0.002240321099948646, A4: 0.0016794213723518292\n",
      "Epoch 46, Total Loss: 0.014209729176945985, DLoss: 0.00570483296839732, A1: 0.0024330191356553275, A2: 0.002190947050588544, A3: 0.002230042463634163, A4: 0.0016508875689892606\n",
      "Epoch 47, Total Loss: 0.020990889335305175, DLoss: 0.012508942888499323, A1: 0.0024288380592638117, A2: 0.0021858756271698936, A3: 0.002226360799448395, A4: 0.001640871971506964\n",
      "Epoch 48, Total Loss: 0.030827848719094286, DLoss: 0.022264907422570767, A1: 0.0024544912824322556, A2: 0.0022010611899366435, A3: 0.0022452382381412794, A4: 0.0016621504883832213\n",
      "Epoch 49, Total Loss: 0.00958856777711348, DLoss: 0.0012326856386855733, A1: 0.002407579438295215, A2: 0.0021540627962994305, A3: 0.002214097483506934, A4: 0.0015801424245265397\n",
      "Epoch 50, Total Loss: 0.008622213586402888, DLoss: 0.00037273538815828613, A1: 0.0023825459468009118, A2: 0.0021275345638225025, A3: 0.002193800657352602, A4: 0.0015455970339561728\n",
      "Epoch 51, Validation Loss Weighted: 3.351465940475464, | Validation Unweighted Avg: 0.0022030777763575315\n",
      "Epoch 51, Total Loss: 0.009204948600381613, DLoss: 0.00100341564585423, A1: 0.002376064372418279, A2: 0.0021071607792030344, A3: 0.002180240362014791, A4: 0.001538067425082607\n",
      "Epoch 52, Total Loss: 0.019765880771658636, DLoss: 0.01154992532299895, A1: 0.002376734283329411, A2: 0.0021072625806978483, A3: 0.0021833775427446446, A4: 0.0015485810160912065\n",
      "Epoch 53, Total Loss: 0.023477932656268505, DLoss: 0.015241228112294763, A1: 0.0023887438562021335, A2: 0.0021162152644881808, A3: 0.002185758141885427, A4: 0.0015459872012301772\n",
      "Epoch 54, Total Loss: 0.010814961962486533, DLoss: 0.0027151678172892637, A1: 0.002354859924790534, A2: 0.0020827843262602322, A3: 0.0021690406095744535, A4: 0.001493109295684421\n",
      "Epoch 55, Total Loss: 0.009737786587158388, DLoss: 0.0017270389582170703, A1: 0.002343835773750801, A2: 0.0020603128597775303, A3: 0.002150604746367952, A4: 0.0014559942517239092\n",
      "Epoch 56, Total Loss: 0.018610257375985383, DLoss: 0.010611567448094402, A1: 0.0023402369644662194, A2: 0.002057862353765152, A3: 0.002158287802541798, A4: 0.0014423028311946176\n",
      "Epoch 57, Total Loss: 0.01632855935868892, DLoss: 0.008381821838239293, A1: 0.002329913075928661, A2: 0.002043418403693729, A3: 0.0021494879636024546, A4: 0.001423918029600331\n",
      "Epoch 58, Total Loss: 0.012206485056826337, DLoss: 0.004334008524099111, A1: 0.002323413446058773, A2: 0.0020252288165713912, A3: 0.002133650715801526, A4: 0.0013901835082585669\n",
      "Epoch 59, Total Loss: 0.01358368164351718, DLoss: 0.00578832520881075, A1: 0.002306350648657165, A2: 0.0020041719486471265, A3: 0.002125389494573359, A4: 0.0013594443155770105\n",
      "Epoch 60, Total Loss: 0.009953054135919294, DLoss: 0.002253020146070577, A1: 0.002288353965956379, A2: 0.0019853085882707753, A3: 0.002101995464710688, A4: 0.001324376030507582\n",
      "Epoch 61, Validation Loss Weighted: 3.1226871013641357, | Validation Unweighted Avg: 0.0020280044991523027\n",
      "Epoch 61, Total Loss: 0.008995938633399253, DLoss: 0.0013901465411022813, A1: 0.002270707182734358, A2: 0.0019671109557913785, A3: 0.0020848838130901144, A4: 0.0012830901404165408\n",
      "Epoch 62, Total Loss: 0.016862056298520077, DLoss: 0.009300602737560192, A1: 0.002259705608329651, A2: 0.0019558765073518524, A3: 0.0020774593571497297, A4: 0.0012684120386521416\n",
      "Epoch 63, Total Loss: 0.014622071997093205, DLoss: 0.007083006149729375, A1: 0.0022590592990375377, A2: 0.0019366419630717825, A3: 0.0020832714815200728, A4: 0.0012600931325736878\n",
      "Epoch 64, Total Loss: 0.00964717244390737, DLoss: 0.0022212662191701715, A1: 0.0022326499223709107, A2: 0.0019107809782409194, A3: 0.002056027973197739, A4: 0.0012264473669347353\n",
      "Epoch 65, Total Loss: 0.011964578630233353, DLoss: 0.004608176404673776, A1: 0.002217460841215639, A2: 0.0018875753065169026, A3: 0.0020389778989324853, A4: 0.0012123880614209074\n",
      "Epoch 66, Total Loss: 0.013150611985474825, DLoss: 0.005829224990024654, A1: 0.002214348278093067, A2: 0.0018708188557701017, A3: 0.002041588025696745, A4: 0.0011946318403881213\n",
      "Epoch 67, Total Loss: 0.012183545476926322, DLoss: 0.004936349607305601, A1: 0.0021990213908297434, A2: 0.0018590300534428522, A3: 0.0020216483860911634, A4: 0.001167496033965356\n",
      "Epoch 68, Total Loss: 0.010451418341306801, DLoss: 0.0032682525167315774, A1: 0.0021821525805121797, A2: 0.0018415153177391568, A3: 0.0020077435828914697, A4: 0.0011517543583812024\n",
      "Epoch 69, Total Loss: 0.010989533409222284, DLoss: 0.0038809896778812716, A1: 0.002165929423856803, A2: 0.001825202631615949, A3: 0.001991095070049844, A4: 0.001126316572613591\n",
      "Epoch 70, Total Loss: 0.013705948105251248, DLoss: 0.006618702218772589, A1: 0.002164285687666217, A2: 0.0018218004729450596, A3: 0.0019850280035329476, A4: 0.0011161316891296089\n",
      "Epoch 71, Validation Loss Weighted: 2.9342687129974365, | Validation Unweighted Avg: 0.0017263705376535654\n",
      "Epoch 71, Total Loss: 0.01353887221775949, DLoss: 0.006500682429230602, A1: 0.0021528743720740416, A2: 0.0018089484312275256, A3: 0.001983212919334288, A4: 0.0010931540535900487\n",
      "Epoch 72, Total Loss: 0.00861611325420778, DLoss: 0.0016878166861054276, A1: 0.002128803674978289, A2: 0.0017809349802767182, A3: 0.0019493459940845654, A4: 0.00106921188602097\n",
      "Epoch 73, Total Loss: 0.009068005261096088, DLoss: 0.002232436677282253, A1: 0.0021098944649565964, A2: 0.001761898415597071, A3: 0.0019235081706111404, A4: 0.0010402674923005344\n",
      "Epoch 74, Total Loss: 0.014716419008221815, DLoss: 0.00788383786963426, A1: 0.0021120119596492837, A2: 0.0017642286680215461, A3: 0.0019194883230904286, A4: 0.0010368521686442281\n",
      "Epoch 75, Total Loss: 0.013079920315861025, DLoss: 0.006271471568536733, A1: 0.0021144658321811055, A2: 0.0017475146072683857, A3: 0.0019178252323234285, A4: 0.0010286430967177942\n",
      "Epoch 76, Total Loss: 0.009453094502995637, DLoss: 0.0027567135367214425, A1: 0.002089289105920629, A2: 0.001718633846295151, A3: 0.0018856206247371368, A4: 0.0010028373747693628\n",
      "Epoch 77, Total Loss: 0.010058453736234117, DLoss: 0.0034220705556270497, A1: 0.002072653993011706, A2: 0.0017080232834401118, A3: 0.0018719579908065497, A4: 0.0009837479347797025\n",
      "Epoch 78, Total Loss: 0.008787065562368794, DLoss: 0.002235888751535888, A1: 0.002055740285537798, A2: 0.0016856167108265005, A3: 0.0018498625394634226, A4: 0.0009599572814874012\n",
      "Epoch 79, Total Loss: 0.009897921469316564, DLoss: 0.0034028079826384783, A1: 0.002040560979565436, A2: 0.0016688530812230468, A3: 0.001834175461077724, A4: 0.000951523984390819\n",
      "Epoch 80, Total Loss: 0.010970032644796778, DLoss: 0.0045064320015213025, A1: 0.002038297016406432, A2: 0.0016684582343176853, A3: 0.0018148979140360925, A4: 0.0009419474357855506\n",
      "Epoch 81, Validation Loss Weighted: 2.7869505882263184, | Validation Unweighted Avg: 0.0031887523364275694\n",
      "Epoch 81, Total Loss: 0.010773635540284555, DLoss: 0.004367423283904579, A1: 0.0020203402514611794, A2: 0.0016634263270217079, A3: 0.0018112681679088962, A4: 0.0009111775109142235\n",
      "Epoch 82, Total Loss: 0.014823847234418446, DLoss: 0.008366457346974956, A1: 0.002035305259579962, A2: 0.0016829728619830514, A3: 0.0017957610829563981, A4: 0.0009433506661893757\n",
      "Epoch 83, Total Loss: 0.008121900471054357, DLoss: 0.0017697403266290414, A1: 0.0020218609043778004, A2: 0.0016605600156419148, A3: 0.0017719106904802505, A4: 0.0008978285534381443\n",
      "Epoch 84, Total Loss: 0.006952856721314178, DLoss: 0.0007167918222297555, A1: 0.0019724826942282645, A2: 0.0016368132761933587, A3: 0.0017390702919907529, A4: 0.0008876986368374094\n",
      "Epoch 85, Total Loss: 0.010901568201370537, DLoss: 0.004663743833556179, A1: 0.002005065799775449, A2: 0.0016274774695788934, A3: 0.0017266060016647151, A4: 0.0008786751265605827\n",
      "Epoch 86, Total Loss: 0.021044380509887228, DLoss: 0.01473955101153644, A1: 0.002015298255190084, A2: 0.00162630317394029, A3: 0.0017589617721651765, A4: 0.0009042662399058992\n",
      "Epoch 87, Total Loss: 0.00962465875252912, DLoss: 0.0033683209625781853, A1: 0.0020005665312055497, A2: 0.0016154687785522334, A3: 0.0017452122910816053, A4: 0.0008950901386428582\n",
      "Epoch 88, Total Loss: 0.0064474542529999535, DLoss: 0.0002838830637880495, A1: 0.00196408717893064, A2: 0.0015865307305251587, A3: 0.0017363407265458424, A4: 0.000876612556219863\n",
      "Epoch 89, Total Loss: 0.00644253003410995, DLoss: 0.00040113982436609645, A1: 0.0019277703466782853, A2: 0.0015691803327751008, A3: 0.0016797717139442367, A4: 0.0008646678087560841\n",
      "Epoch 90, Total Loss: 0.008969202693763443, DLoss: 0.0029906750911984337, A1: 0.0019088422559434548, A2: 0.001559183070970572, A3: 0.0016543826035393234, A4: 0.000856119696122319\n",
      "Epoch 91, Validation Loss Weighted: 2.577944278717041, | Validation Unweighted Avg: 0.003328586695715785\n",
      "Epoch 91, Total Loss: 0.01099836776689203, DLoss: 0.005065048040705733, A1: 0.0018889265177263455, A2: 0.0015495602683090097, A3: 0.0016325583062346348, A4: 0.0008622745987932748\n",
      "Epoch 92, Total Loss: 0.02111074307996949, DLoss: 0.015062481413637712, A1: 0.0019166615714890543, A2: 0.0015671099985907363, A3: 0.0016841956393115899, A4: 0.0008802944237355735\n",
      "Epoch 93, Total Loss: 0.008876503617714413, DLoss: 0.002979729222318962, A1: 0.001905637416480617, A2: 0.0015373355855858378, A3: 0.0016198577999603004, A4: 0.0008339436114263001\n",
      "Epoch 94, Total Loss: 0.005881156183948571, DLoss: 0.00015597172215910343, A1: 0.001857652190797539, A2: 0.0015099041028985415, A3: 0.0015544037911405955, A4: 0.0008032243540748657\n",
      "Epoch 95, Total Loss: 0.00585670720373111, DLoss: 0.00021379406639839132, A1: 0.0018232725277564233, A2: 0.0014898708409005353, A3: 0.0015304066031769088, A4: 0.0007993631596698172\n",
      "Epoch 96, Total Loss: 0.007733554211021824, DLoss: 0.0021707873695380507, A1: 0.0017908775274091484, A2: 0.0014799731983326967, A3: 0.0015028425862758674, A4: 0.0007890735423478129\n",
      "Epoch 97, Total Loss: 0.026473238607021894, DLoss: 0.020643563555892218, A1: 0.001858278604711152, A2: 0.0015524543651157397, A3: 0.001569125960469881, A4: 0.0008498162102610381\n",
      "Epoch 98, Total Loss: 0.009464729664085264, DLoss: 0.0037787214491386708, A1: 0.001830761357930235, A2: 0.001475882064683405, A3: 0.0015669772822134705, A4: 0.0008123875279125059\n",
      "Epoch 99, Total Loss: 0.0065056760647249495, DLoss: 0.0009620192838404116, A1: 0.0017963046599602836, A2: 0.0014557785367122216, A3: 0.001501889888525263, A4: 0.0007896836991263131\n",
      "Epoch 100, Total Loss: 0.006704170917245475, DLoss: 0.0012523152164497086, A1: 0.0017623942382422023, A2: 0.0014445672982467592, A3: 0.0014656710335244, A4: 0.0007792231365204895\n",
      "Epoch 101, Validation Loss Weighted: 2.364936351776123, | Validation Unweighted Avg: 0.0011903374688699841\n",
      "Epoch 101, Total Loss: 0.0063700031430926176, DLoss: 0.0009658983453680941, A1: 0.001731234774342738, A2: 0.0014309616730977062, A3: 0.001464942750912583, A4: 0.0007769655912521888\n",
      "Epoch 102, Total Loss: 0.0069576575627169485, DLoss: 0.0016228151480886894, A1: 0.0017045789443231611, A2: 0.0014201514741051307, A3: 0.0014417174247104082, A4: 0.0007683945868021427\n",
      "Epoch 103, Total Loss: 0.009988794700687073, DLoss: 0.004710610616100232, A1: 0.001687029586305884, A2: 0.001416307519155618, A3: 0.0014143236640269275, A4: 0.0007605233393075071\n",
      "Epoch 104, Total Loss: 0.012469295549883762, DLoss: 0.007101551615580155, A1: 0.0016849323368991133, A2: 0.0014998344758747739, A3: 0.0014097304120448163, A4: 0.0007732467126598667\n",
      "Epoch 105, Total Loss: 0.007112461275061254, DLoss: 0.0018250832439992916, A1: 0.0016579138140183534, A2: 0.0015143918374881402, A3: 0.0013821415076646108, A4: 0.0007329308740405726\n",
      "Epoch 106, Total Loss: 0.005655449943680485, DLoss: 0.00036161533275431295, A1: 0.0016269373475965535, A2: 0.0015901323819310744, A3: 0.0013453914035132832, A4: 0.0007313734519729307\n",
      "Epoch 107, Total Loss: 0.007239935149184682, DLoss: 0.0020216081436212167, A1: 0.0016044591818089513, A2: 0.0015496189252272334, A3: 0.0013345101110065695, A4: 0.0007297387723652223\n",
      "Epoch 108, Total Loss: 0.02330412771341137, DLoss: 0.017987769221839367, A1: 0.001642895963397512, A2: 0.0015316822547571396, A3: 0.0013804851843319326, A4: 0.0007612952755483672\n",
      "Epoch 109, Total Loss: 0.015147661646849222, DLoss: 0.00993018373026809, A1: 0.0016655419097250242, A2: 0.001415571891067719, A3: 0.0013870605630059304, A4: 0.0007493034804528262\n",
      "Epoch 110, Total Loss: 0.005252213166518645, DLoss: 0.0002511079529457757, A1: 0.0016058996924336745, A2: 0.0013627232284555, A3: 0.0013140294300021857, A4: 0.0007184528444089334\n",
      "Epoch 111, Validation Loss Weighted: 2.179133653640747, | Validation Unweighted Avg: 0.001025727717205882\n",
      "Epoch 111, Total Loss: 0.005006200780520554, DLoss: 8.997053603459129e-05, A1: 0.001577139625971375, A2: 0.001350151024367237, A3: 0.0012740314144387163, A4: 0.0007149081938471433\n",
      "Epoch 112, Total Loss: 0.004995189800022424, DLoss: 0.00014710353905022865, A1: 0.0015431507759785744, A2: 0.001355599649964874, A3: 0.0012437261538500687, A4: 0.0007056096844693953\n",
      "Epoch 113, Total Loss: 0.00603725852296603, DLoss: 0.0010942959664548238, A1: 0.0015191445865969978, A2: 0.0014754587765888904, A3: 0.0012461225139717995, A4: 0.0007022366740784492\n",
      "Epoch 114, Total Loss: 0.008723198013930497, DLoss: 0.0038732604195750105, A1: 0.001501891253759492, A2: 0.0013968495322677138, A3: 0.001242033317314715, A4: 0.00070916352382152\n",
      "Epoch 115, Total Loss: 0.00894398485172794, DLoss: 0.004171389303916261, A1: 0.0014925947019946762, A2: 0.001361448855194877, A3: 0.0012278256369675298, A4: 0.0006907263478834383\n",
      "Epoch 116, Total Loss: 0.006293962013759566, DLoss: 0.0016109294203157664, A1: 0.0014634234208973464, A2: 0.0013257128320798405, A3: 0.001196139101507883, A4: 0.0006977572513940024\n",
      "Epoch 117, Total Loss: 0.01013733963587914, DLoss: 0.005417336903205565, A1: 0.0014515465102952227, A2: 0.0013654676721423376, A3: 0.0011953284392472018, A4: 0.0007076601590928145\n",
      "Epoch 118, Total Loss: 0.012194660716076297, DLoss: 0.007532257768087385, A1: 0.001469218862224476, A2: 0.001292901989654638, A3: 0.0012001483573509508, A4: 0.0007001337588669478\n",
      "Epoch 119, Total Loss: 0.005492824403600852, DLoss: 0.0009615936887712451, A1: 0.001429856269996063, A2: 0.001276519132112629, A3: 0.0011534312376170418, A4: 0.000671424068241322\n",
      "Epoch 120, Total Loss: 0.005018808150039563, DLoss: 0.0005419286568708643, A1: 0.0014098505212380339, A2: 0.0012711789288997269, A3: 0.001133536971610738, A4: 0.0006623130788449833\n",
      "Epoch 121, Validation Loss Weighted: 2.0285120010375977, | Validation Unweighted Avg: 0.0012579564936459064\n",
      "Epoch 121, Total Loss: 0.005065168347077402, DLoss: 0.0006451553705780745, A1: 0.0013818295775060753, A2: 0.001255774841212604, A3: 0.001110896305478739, A4: 0.0006715122464315341\n",
      "Epoch 122, Total Loss: 0.010319339199287986, DLoss: 0.005849907379359303, A1: 0.0013874534583010246, A2: 0.0012629998465854442, A3: 0.0011270574232846477, A4: 0.0006919210711533775\n",
      "Epoch 123, Total Loss: 0.008083476301346143, DLoss: 0.0036457868442415597, A1: 0.001384745215645208, A2: 0.0012473232615196719, A3: 0.0011125716792858755, A4: 0.0006930493038784004\n",
      "Epoch 124, Total Loss: 0.006823972113356417, DLoss: 0.002457130615617974, A1: 0.0013558683955290523, A2: 0.001238175801385792, A3: 0.0011009987655019027, A4: 0.000671798546748257\n",
      "Epoch 125, Total Loss: 0.005211038459145295, DLoss: 0.0008697372751901804, A1: 0.0013524519711526491, A2: 0.0012311727520434282, A3: 0.0010856419123394352, A4: 0.0006720345568200256\n",
      "Epoch 126, Total Loss: 0.005475451292824635, DLoss: 0.0011946312837112186, A1: 0.001328727247726585, A2: 0.00122443157389088, A3: 0.0010705298824508316, A4: 0.0006571312915680623\n",
      "Epoch 127, Total Loss: 0.006942285955036906, DLoss: 0.0026308230615210383, A1: 0.00132530750108179, A2: 0.0012229965952098032, A3: 0.001073848371214064, A4: 0.0006893104114004183\n",
      "Epoch 128, Total Loss: 0.010201879573816602, DLoss: 0.005931819825226822, A1: 0.0013193811483109708, A2: 0.0012063366851924168, A3: 0.0010667148434136337, A4: 0.0006776270728302982\n",
      "Epoch 129, Total Loss: 0.006965998418242882, DLoss: 0.0027190000480914025, A1: 0.0013188427124325905, A2: 0.0011923517854897495, A3: 0.0010609275399309065, A4: 0.0006748763110325936\n",
      "Epoch 130, Total Loss: 0.00493578124653802, DLoss: 0.000785106437913121, A1: 0.001284062962837528, A2: 0.001179533592899547, A3: 0.0010373453449905024, A4: 0.0006497328962309852\n",
      "Epoch 131, Validation Loss Weighted: 1.9013452529907227, | Validation Unweighted Avg: 0.001613118452951312\n",
      "Epoch 131, Total Loss: 0.005828869823414028, DLoss: 0.001670089767404451, A1: 0.001272511686230163, A2: 0.001173661416413035, A3: 0.001038496938616101, A4: 0.0006741100067880656\n",
      "Epoch 132, Total Loss: 0.012792099670993842, DLoss: 0.008515932853333652, A1: 0.0012980921621585202, A2: 0.001204039550744214, A3: 0.001069406081792708, A4: 0.0007046290049898248\n",
      "Epoch 133, Total Loss: 0.007057516059086827, DLoss: 0.0028786451161563905, A1: 0.0012860588558727282, A2: 0.0011817674653436618, A3: 0.0010446201714743935, A4: 0.0006664244317521066\n",
      "Epoch 134, Total Loss: 0.005131317421116612, DLoss: 0.0010325948687585134, A1: 0.001261064464349395, A2: 0.0011583833144620506, A3: 0.0010246794540349088, A4: 0.0006545953500459622\n",
      "Epoch 135, Total Loss: 0.005473333194888916, DLoss: 0.0013916882650267905, A1: 0.0012465173567761667, A2: 0.001163104017210902, A3: 0.0010173041508923317, A4: 0.0006547193808976524\n",
      "Epoch 136, Total Loss: 0.0052548884515999815, DLoss: 0.0012212464098080422, A1: 0.0012263656071643874, A2: 0.001153223548185038, A3: 0.0010038562643372147, A4: 0.0006501966304809188\n",
      "Epoch 137, Total Loss: 0.006062785395558669, DLoss: 0.002042366076652384, A1: 0.0012123488730139797, A2: 0.0011451353210180637, A3: 0.000998596791842746, A4: 0.000664338337612042\n",
      "Epoch 138, Total Loss: 0.01088823921798559, DLoss: 0.006851696846239395, A1: 0.0012232851419949756, A2: 0.001138851907952895, A3: 0.00100658395564427, A4: 0.0006678213209604357\n",
      "Epoch 139, Total Loss: 0.0073128049925964495, DLoss: 0.003288109999068018, A1: 0.0012119861772424139, A2: 0.0011458510905868024, A3: 0.0010040808644341517, A4: 0.0006627768721872682\n",
      "Epoch 140, Total Loss: 0.004329806858401174, DLoss: 0.0003689582852034205, A1: 0.0011914304145400159, A2: 0.001122705966471668, A3: 0.0009917040102664031, A4: 0.0006550081624399437\n",
      "Epoch 141, Validation Loss Weighted: 1.7786005735397339, | Validation Unweighted Avg: 0.0008671038085594773\n",
      "Epoch 141, Total Loss: 0.0041816349274251315, DLoss: 0.00022461385999783735, A1: 0.0011878668410976057, A2: 0.0011226054929383365, A3: 0.0009853891116935385, A4: 0.0006611596272539993\n",
      "Epoch 142, Total Loss: 0.006624965448017148, DLoss: 0.0026957340254077943, A1: 0.001177861508635529, A2: 0.0011201921690041094, A3: 0.0009846925409213079, A4: 0.0006464851900091162\n",
      "Epoch 143, Total Loss: 0.009627454825518229, DLoss: 0.0056500059696686965, A1: 0.0011891001303659075, A2: 0.0011193158729094483, A3: 0.0009928113421251922, A4: 0.0006762214944584147\n",
      "Epoch 144, Total Loss: 0.004778968512255233, DLoss: 0.0008939665261013115, A1: 0.0011706825704260898, A2: 0.001101773529130283, A3: 0.0009744487185095997, A4: 0.000638097188584652\n",
      "Epoch 145, Total Loss: 0.005645104116543238, DLoss: 0.0017525861059931975, A1: 0.0011586167460667309, A2: 0.001101606369576819, A3: 0.0009725819706793779, A4: 0.0006597128977690855\n",
      "Epoch 146, Total Loss: 0.005425455963334323, DLoss: 0.001570794212272052, A1: 0.0011459159116301717, A2: 0.0010952091579218342, A3: 0.0009700727989432132, A4: 0.0006434638437233596\n",
      "Epoch 147, Total Loss: 0.0062493594073616395, DLoss: 0.002391503667472121, A1: 0.001141529341359935, A2: 0.0010974250904672027, A3: 0.000971585743470562, A4: 0.0006473155294522506\n",
      "Epoch 148, Total Loss: 0.007708798108664765, DLoss: 0.00387245636123804, A1: 0.0011392997792261832, A2: 0.0010906575105598842, A3: 0.0009658228335783034, A4: 0.0006405616079725653\n",
      "Epoch 149, Total Loss: 0.012641761541123163, DLoss: 0.008567426768422592, A1: 0.001180804382932944, A2: 0.001194889061033874, A3: 0.001035714017300713, A4: 0.0006629273617198289\n",
      "Epoch 150, Total Loss: 0.005992205133423505, DLoss: 0.0020351221007744737, A1: 0.0011519415730204359, A2: 0.001146279461996032, A3: 0.0010099631760485037, A4: 0.000648898820277695\n",
      "Epoch 151, Validation Loss Weighted: 1.7525972127914429, | Validation Unweighted Avg: 0.0008061424014158547\n",
      "Epoch 151, Total Loss: 0.004118029549532697, DLoss: 0.00029444551747391496, A1: 0.0011243036534241664, A2: 0.001081396890855434, A3: 0.0009770060951185852, A4: 0.0006408774333067413\n",
      "Epoch 152, Total Loss: 0.004008885488821595, DLoss: 0.00021780294790997603, A1: 0.0011101033820523297, A2: 0.0010761634212278957, A3: 0.0009719705341426147, A4: 0.0006328451931618805\n",
      "Epoch 153, Total Loss: 0.00466021713522919, DLoss: 0.0008913759559535802, A1: 0.0011021129043993741, A2: 0.001069096643004741, A3: 0.000955296054698093, A4: 0.0006423355408845633\n",
      "Epoch 154, Total Loss: 0.006937886466303925, DLoss: 0.003153751434497694, A1: 0.0011033544483408067, A2: 0.0010658531794268294, A3: 0.0009592945384611085, A4: 0.0006556329022053184\n",
      "Epoch 155, Total Loss: 0.004699117211831353, DLoss: 0.0009567578641343227, A1: 0.001087634292707662, A2: 0.00106278273224234, A3: 0.0009492951286367977, A4: 0.0006426472161448064\n",
      "Epoch 156, Total Loss: 0.006481367450098994, DLoss: 0.0027070183115111748, A1: 0.0010870230069485842, A2: 0.0010617864827426357, A3: 0.0009579922350316007, A4: 0.0006675474034388819\n",
      "Epoch 157, Total Loss: 0.005314239179212811, DLoss: 0.0016192813089053908, A1: 0.0010804991594341117, A2: 0.0010527890957333014, A3: 0.0009382540344549026, A4: 0.0006234156086644692\n",
      "Epoch 158, Total Loss: 0.005543496893237303, DLoss: 0.001802065727447784, A1: 0.0010709538683096815, A2: 0.0010731499364985873, A3: 0.0009539681453299471, A4: 0.0006433592233902759\n",
      "Epoch 159, Total Loss: 0.007204394854091912, DLoss: 0.003488070464928486, A1: 0.0010663715133143177, A2: 0.0010660029371551488, A3: 0.0009431911958348461, A4: 0.000640758754517182\n",
      "Epoch 160, Total Loss: 0.008056295462301933, DLoss: 0.004311469954237426, A1: 0.0010882802410295963, A2: 0.0010723100359924006, A3: 0.0009425172538943694, A4: 0.0006417179789588607\n",
      "Epoch 161, Validation Loss Weighted: 1.7714393138885498, | Validation Unweighted Avg: 0.0011519609251990914\n",
      "Epoch 161, Total Loss: 0.005843226273612924, DLoss: 0.002113874970977618, A1: 0.0010800248475458251, A2: 0.0010713342221854625, A3: 0.0009414942956937012, A4: 0.0006364979519606674\n",
      "Epoch 162, Total Loss: 0.005974029306856789, DLoss: 0.0022636220159421845, A1: 0.0010618724975012645, A2: 0.0010604839133272683, A3: 0.0009422023846127559, A4: 0.0006458485091157358\n",
      "Epoch 163, Total Loss: 0.004005724423636407, DLoss: 0.0003327487044440535, A1: 0.0010545261400362836, A2: 0.0010556616217234642, A3: 0.0009311419085861417, A4: 0.0006316460658870876\n",
      "Epoch 164, Total Loss: 0.00434856173596927, DLoss: 0.0006628901541592891, A1: 0.0010509302194060927, A2: 0.0010559321876991651, A3: 0.0009353151630999822, A4: 0.0006434940052258753\n",
      "Epoch 165, Total Loss: 0.007138668871630216, DLoss: 0.003441614266590808, A1: 0.001058273477561174, A2: 0.001052260010303805, A3: 0.000934763763664457, A4: 0.0006517573381395116\n",
      "Epoch 166, Total Loss: 0.005553545763673769, DLoss: 0.0018857438944079067, A1: 0.0010525887124293314, A2: 0.0010451528153432512, A3: 0.0009364849955173585, A4: 0.0006335753365213102\n",
      "Epoch 167, Total Loss: 0.00582387890079355, DLoss: 0.002153466562793421, A1: 0.0010453224063398507, A2: 0.0010525417652057843, A3: 0.0009275789074308704, A4: 0.000644969269904488\n",
      "Epoch 168, Total Loss: 0.004695790952709186, DLoss: 0.0010545875388660616, A1: 0.0010287787717143974, A2: 0.0010444827496113166, A3: 0.0009296642806971662, A4: 0.0006382776263845617\n",
      "Epoch 169, Total Loss: 0.004599817010603147, DLoss: 0.0009837995595510373, A1: 0.0010264238284435124, A2: 0.001041499325872957, A3: 0.0009236071905516755, A4: 0.0006244871067503316\n",
      "Epoch 170, Total Loss: 0.007053548855632967, DLoss: 0.003411617605558025, A1: 0.0010467950323386917, A2: 0.0010426380725386314, A3: 0.0009301154348246531, A4: 0.0006223827117289396\n",
      "Epoch 171, Validation Loss Weighted: 1.7084381580352783, | Validation Unweighted Avg: 0.0012095804559066892\n",
      "Epoch 171, Total Loss: 0.006403147180654658, DLoss: 0.002721300009381428, A1: 0.0010434647387062962, A2: 0.001041060466179095, A3: 0.0009441704235624373, A4: 0.000653151555872517\n",
      "Epoch 172, Total Loss: 0.004918081765010191, DLoss: 0.001249791170035182, A1: 0.0010298928701641587, A2: 0.0010390157888237313, A3: 0.0009451000609756607, A4: 0.0006542818841146733\n",
      "Epoch 173, Total Loss: 0.004430281087380453, DLoss: 0.0008110733446466673, A1: 0.0010254141110305102, A2: 0.0010324955032047002, A3: 0.0009270663076594329, A4: 0.0006342318559952467\n",
      "Epoch 174, Total Loss: 0.006339925128916299, DLoss: 0.0026757245452410476, A1: 0.0010235674761299213, A2: 0.0010332596398339691, A3: 0.000938218297505955, A4: 0.000669155183541906\n",
      "Epoch 175, Total Loss: 0.008127352402450263, DLoss: 0.004439979093943045, A1: 0.001048643523641989, A2: 0.0010395633741030576, A3: 0.0009391641607428689, A4: 0.0006600022464143959\n",
      "Epoch 176, Total Loss: 0.005791820161076347, DLoss: 0.002173870548656189, A1: 0.0010188103776768416, A2: 0.0010293264072722443, A3: 0.0009269460104984649, A4: 0.0006428668140870287\n",
      "Epoch 177, Total Loss: 0.004893329775024375, DLoss: 0.001288482750433931, A1: 0.0010176837299695762, A2: 0.001024337399766492, A3: 0.0009237033607570827, A4: 0.0006391225546766774\n",
      "Epoch 178, Total Loss: 0.004720652724261692, DLoss: 0.0011575369736833224, A1: 0.0010091095487702907, A2: 0.0010235543001478867, A3: 0.0009068847284776315, A4: 0.0006235671590233819\n",
      "Epoch 179, Total Loss: 0.00381856516012869, DLoss: 0.0002537809145988748, A1: 0.0010047025018221493, A2: 0.0010203253805643312, A3: 0.0009128098825427018, A4: 0.0006269464568338828\n",
      "Epoch 180, Total Loss: 0.005139077473689791, DLoss: 0.001554707296930825, A1: 0.0010081184372872734, A2: 0.0010209408253848034, A3: 0.0009179889862397513, A4: 0.0006373219261976456\n",
      "Epoch 181, Validation Loss Weighted: 1.767017126083374, | Validation Unweighted Avg: 0.0020605698227882385\n",
      "Epoch 181, Total Loss: 0.010719918840649453, DLoss: 0.006990682649881241, A1: 0.0010586411253329557, A2: 0.0010600020504501696, A3: 0.0009474680289382708, A4: 0.0006631249883040171\n",
      "Epoch 182, Total Loss: 0.006529748664946634, DLoss: 0.00286807032918494, A1: 0.001043840985742215, A2: 0.001032974755170554, A3: 0.0009328642171071393, A4: 0.0006519984004047397\n",
      "Epoch 183, Total Loss: 0.003731880794326902, DLoss: 0.00017394807478426214, A1: 0.0010046773995203363, A2: 0.001028783595765245, A3: 0.0009130950836234578, A4: 0.0006113766390047782\n",
      "Epoch 184, Total Loss: 0.0036459686455517924, DLoss: 9.904165127326533e-05, A1: 0.0009939951183034943, A2: 0.0010214876295156666, A3: 0.0009127157539007219, A4: 0.0006187284916367472\n",
      "Epoch 185, Total Loss: 0.003759956448993762, DLoss: 0.00019844937213830152, A1: 0.0009861707585183798, A2: 0.0010219591534678329, A3: 0.0009173056215662324, A4: 0.0006360715455519477\n",
      "Epoch 186, Total Loss: 0.004255851420219353, DLoss: 0.0007062717950744131, A1: 0.0009864656085483148, A2: 0.0010140356636136195, A3: 0.0009176570975837835, A4: 0.0006314212654863447\n",
      "Epoch 187, Total Loss: 0.013010320686523549, DLoss: 0.009310639184100595, A1: 0.0010522734812432398, A2: 0.0010376032980027297, A3: 0.0009552283541159506, A4: 0.0006545763629632843\n",
      "Epoch 188, Total Loss: 0.0071421341457277195, DLoss: 0.0033760413089277625, A1: 0.0010951391937024627, A2: 0.0010390425548642021, A3: 0.0009705759836751069, A4: 0.0006613351235996974\n",
      "Epoch 189, Total Loss: 0.00384925599897873, DLoss: 0.000217002107266043, A1: 0.0010497710162615643, A2: 0.0010217260326664307, A3: 0.0009396073314085449, A4: 0.0006211495371272493\n",
      "Epoch 190, Total Loss: 0.0036636074518107556, DLoss: 8.154849061942182e-05, A1: 0.0010011443011452767, A2: 0.0010259447026369757, A3: 0.0009290713012515797, A4: 0.0006258986638716703\n",
      "Epoch 191, Validation Loss Weighted: 1.6040760278701782, | Validation Unweighted Avg: 0.0008200075244531035\n",
      "Epoch 191, Total Loss: 0.003829417529976292, DLoss: 0.0002539377610859943, A1: 0.0009973419020174003, A2: 0.001017605706519217, A3: 0.0009259012669520994, A4: 0.0006346308957828031\n",
      "Epoch 192, Total Loss: 0.005263527901181739, DLoss: 0.0016978780265410685, A1: 0.0009880460685797682, A2: 0.0010156000014748397, A3: 0.0009276693310766859, A4: 0.0006343344312881527\n",
      "Epoch 193, Total Loss: 0.007174552240890493, DLoss: 0.0036246024606622416, A1: 0.0009824441667826084, A2: 0.0010191138077309668, A3: 0.0009176796597495757, A4: 0.0006307121438939331\n",
      "Epoch 194, Total Loss: 0.007939085675223561, DLoss: 0.004347185296104395, A1: 0.0010029517201970025, A2: 0.0010297915806652535, A3: 0.0009269482898162096, A4: 0.0006322087644961853\n",
      "Epoch 195, Total Loss: 0.0038269983774212465, DLoss: 0.0002986280471470143, A1: 0.000985586434729736, A2: 0.001019063412898256, A3: 0.000910753810839279, A4: 0.0006129666476764131\n",
      "Epoch 196, Total Loss: 0.003620768631696013, DLoss: 9.721167972119556e-05, A1: 0.0009752424508157922, A2: 0.0010088984089634324, A3: 0.0009096260847689502, A4: 0.0006297899979348254\n",
      "Epoch 197, Total Loss: 0.004252118861603032, DLoss: 0.0007179208945506781, A1: 0.0009679177821536459, A2: 0.0010092362791593503, A3: 0.0009100503716746805, A4: 0.000646993526821792\n",
      "Epoch 198, Total Loss: 0.00549000302469887, DLoss: 0.001945629846225281, A1: 0.0009700347046997525, A2: 0.0010106393494631696, A3: 0.0009190465404034595, A4: 0.0006446525854946892\n",
      "Epoch 199, Total Loss: 0.005470285038675435, DLoss: 0.0019443661049246492, A1: 0.0009607572566396398, A2: 0.0010129595051611515, A3: 0.0009067383207696945, A4: 0.0006454638364960952\n",
      "Epoch 200, Total Loss: 0.006842053928994574, DLoss: 0.003345090180796846, A1: 0.0009664669044551291, A2: 0.001011597264745102, A3: 0.00090037104782823, A4: 0.0006185285197840461\n",
      "Epoch 201, Validation Loss Weighted: 1.5939902067184448, | Validation Unweighted Avg: 0.0008779481286183\n",
      "Epoch 201, Total Loss: 0.004433100190362893, DLoss: 0.000914000529039847, A1: 0.0009711911284739471, A2: 0.0010039423466588976, A3: 0.0009126281385761104, A4: 0.0006313380369895392\n",
      "Epoch 202, Total Loss: 0.005376506819315678, DLoss: 0.0018491676801865899, A1: 0.0009694816942018489, A2: 0.0010028528914931072, A3: 0.0009177200330255305, A4: 0.0006372845072829395\n",
      "Epoch 203, Total Loss: 0.005141283463176064, DLoss: 0.001631794857299114, A1: 0.0009679877454759249, A2: 0.0010087057084423247, A3: 0.0009087043071882694, A4: 0.0006240908624849081\n",
      "Epoch 204, Total Loss: 0.00483847061021317, DLoss: 0.00131518967007816, A1: 0.0009703005531636418, A2: 0.001003392903182678, A3: 0.0009170516106182318, A4: 0.0006325358456251706\n",
      "Epoch 205, Total Loss: 0.004430156453533776, DLoss: 0.0009576942795816153, A1: 0.0009596980093481926, A2: 0.0009976624169046938, A3: 0.0008987980171696108, A4: 0.0006163037216951621\n",
      "Epoch 206, Total Loss: 0.003990857933042042, DLoss: 0.0005081858127100118, A1: 0.000953736862790968, A2: 0.0009984612250842804, A3: 0.0009027628759543983, A4: 0.0006277111581766803\n",
      "Epoch 207, Total Loss: 0.005199470619448799, DLoss: 0.0016949667495447845, A1: 0.0009496759811430489, A2: 0.0009982809159447986, A3: 0.0009146444247727976, A4: 0.0006419025858246062\n",
      "Epoch 208, Total Loss: 0.007770722957634875, DLoss: 0.004264182447844756, A1: 0.0009631210635441015, A2: 0.0010058781579697427, A3: 0.0009094146411908614, A4: 0.0006281266434019595\n",
      "Epoch 209, Total Loss: 0.006685411422619258, DLoss: 0.0031873350160656795, A1: 0.0009616026363387805, A2: 0.001003764073878053, A3: 0.0009160160367256057, A4: 0.0006166936683216177\n",
      "Epoch 210, Total Loss: 0.003818429193316578, DLoss: 0.0003330328802390299, A1: 0.0009508759458929655, A2: 0.0009990442972511201, A3: 0.0009051345437207849, A4: 0.0006303415261796049\n",
      "Epoch 211, Validation Loss Weighted: 1.6047462224960327, | Validation Unweighted Avg: 0.0007582151447422802\n",
      "Epoch 211, Total Loss: 0.0036508283865259727, DLoss: 0.00020373272535090852, A1: 0.0009433593131027671, A2: 0.0009930652160826206, A3: 0.0008957205391900723, A4: 0.0006149506088315154\n",
      "Epoch 212, Total Loss: 0.003907852122673913, DLoss: 0.0004444083065457314, A1: 0.0009490647867179095, A2: 0.000990602437290859, A3: 0.0009018503636649588, A4: 0.0006219262540691323\n",
      "Epoch 213, Total Loss: 0.004763938409318639, DLoss: 0.0013145556034776912, A1: 0.0009443678783306744, A2: 0.0009915675461921968, A3: 0.0008960124238993475, A4: 0.0006174349449793226\n",
      "Epoch 214, Total Loss: 0.005331199340368833, DLoss: 0.0018464442874987568, A1: 0.0009419139290664382, A2: 0.0009992725381876384, A3: 0.0009120490198669855, A4: 0.0006315195778122209\n",
      "Epoch 215, Total Loss: 0.00717400413705036, DLoss: 0.0036665668375049296, A1: 0.000952064565473682, A2: 0.0009982481274040435, A3: 0.0009027312885874297, A4: 0.0006543932783932343\n",
      "Epoch 216, Total Loss: 0.004509539476897441, DLoss: 0.001049151520039066, A1: 0.0009443731527632983, A2: 0.0009903195031256474, A3: 0.0009012599593171961, A4: 0.0006244353614254743\n",
      "Epoch 217, Total Loss: 0.003756384440103747, DLoss: 0.00029699935127031166, A1: 0.0009427330450638361, A2: 0.0009926413974839802, A3: 0.0009014084001261257, A4: 0.0006226022594091774\n",
      "Epoch 218, Total Loss: 0.0036543723682288228, DLoss: 0.00020961321527217727, A1: 0.0009384578447307749, A2: 0.0009835274521471794, A3: 0.0009010045403928499, A4: 0.0006217692939613202\n",
      "Epoch 219, Total Loss: 0.004131655059807764, DLoss: 0.0006828103576481225, A1: 0.0009356503967203687, A2: 0.0009870655741410289, A3: 0.00089901309620886, A4: 0.0006271156364040176\n",
      "Epoch 220, Total Loss: 0.007484598666831681, DLoss: 0.003977145327040819, A1: 0.0009474235356306467, A2: 0.0009990644130167228, A3: 0.0009141510896204802, A4: 0.0006468143086956181\n",
      "Epoch 221, Validation Loss Weighted: 1.5777912139892578, | Validation Unweighted Avg: 0.0009695185581222177\n",
      "Epoch 221, Total Loss: 0.007516643193296411, DLoss: 0.004054976496924858, A1: 0.0009479816725863863, A2: 0.0009920753714761138, A3: 0.0009024757311139679, A4: 0.0006191339178506248\n",
      "Epoch 222, Total Loss: 0.004180382236055183, DLoss: 0.0007100824674291124, A1: 0.0009415681253565046, A2: 0.00099069496324608, A3: 0.0009052362559404256, A4: 0.000632800433380577\n",
      "Epoch 223, Total Loss: 0.003943481861907375, DLoss: 0.0005031903931839307, A1: 0.0009351292454043273, A2: 0.0009873008298861598, A3: 0.0008923743229760559, A4: 0.0006254870674266302\n",
      "Epoch 224, Total Loss: 0.003800338348611893, DLoss: 0.0003762376799634446, A1: 0.0009332124427146482, A2: 0.0009860335508702436, A3: 0.0008919248707487978, A4: 0.0006129297900687644\n",
      "Epoch 225, Total Loss: 0.004096872303092493, DLoss: 0.0006659066135894549, A1: 0.0009283301282597345, A2: 0.000986112856195508, A3: 0.0008958028203241239, A4: 0.0006207198774435793\n",
      "Epoch 226, Total Loss: 0.006301078472924102, DLoss: 0.002834937025710877, A1: 0.0009352880935032524, A2: 0.000994811994909965, A3: 0.0009145943866521769, A4: 0.0006214469375332886\n",
      "Epoch 227, Total Loss: 0.006140724603160792, DLoss: 0.0026840797342381185, A1: 0.0009322167275761538, A2: 0.0009930716530933833, A3: 0.0009042675119210551, A4: 0.0006270889896809835\n",
      "Epoch 228, Total Loss: 0.00379644918026646, DLoss: 0.00034225756188871774, A1: 0.0009333773863256118, A2: 0.0009839414418613822, A3: 0.0009029365274293674, A4: 0.0006339362808086496\n",
      "Epoch 229, Total Loss: 0.0034952064261729405, DLoss: 7.608678314351312e-05, A1: 0.000926775114666353, A2: 0.0009829398332261586, A3: 0.0008987865150960385, A4: 0.0006106181885239824\n",
      "Epoch 230, Total Loss: 0.003557963718545083, DLoss: 0.00014130090280973077, A1: 0.0009269338287719041, A2: 0.0009808558345267473, A3: 0.0008917738871704469, A4: 0.0006170992441308375\n",
      "Epoch 231, Validation Loss Weighted: 1.5821415185928345, | Validation Unweighted Avg: 0.001407864154316485\n",
      "Epoch 231, Total Loss: 0.005416737321303861, DLoss: 0.0019695409082156733, A1: 0.0009325013177658547, A2: 0.0009843427481841486, A3: 0.0008974800303886613, A4: 0.0006328723079977035\n",
      "Epoch 232, Total Loss: 0.007082612047485203, DLoss: 0.003617300735574893, A1: 0.0009377726577192582, A2: 0.0009862762125728544, A3: 0.0009066155594403178, A4: 0.0006346468525903633\n",
      "Epoch 233, Total Loss: 0.004758295343477089, DLoss: 0.0013456470992836826, A1: 0.0009279515902604048, A2: 0.0009776619195484024, A3: 0.0008920597940025552, A4: 0.0006149749329117846\n",
      "Epoch 234, Total Loss: 0.0036492354394299167, DLoss: 0.00023609188109813576, A1: 0.0009234246311031959, A2: 0.0009814299864749707, A3: 0.0008907213218869649, A4: 0.0006175676354401234\n",
      "Epoch 235, Total Loss: 0.00370747850783201, DLoss: 0.00028132985588937037, A1: 0.0009239665814427347, A2: 0.0009823569622312788, A3: 0.0008936637762107321, A4: 0.0006261613543467155\n",
      "Epoch 236, Total Loss: 0.004065088600923032, DLoss: 0.0006447729429882285, A1: 0.0009204962409967265, A2: 0.0009809617002784678, A3: 0.0008950203702335619, A4: 0.0006238373514365363\n",
      "Epoch 237, Total Loss: 0.008135793701777319, DLoss: 0.004669747200188629, A1: 0.0009332770043659945, A2: 0.0009876824324485576, A3: 0.0009006492693908347, A4: 0.0006444378015616663\n",
      "Epoch 238, Total Loss: 0.00549504300440524, DLoss: 0.0020438977942831118, A1: 0.0009352860450648719, A2: 0.0009867397496360684, A3: 0.0008985011675308256, A4: 0.0006306182347171588\n",
      "Epoch 239, Total Loss: 0.003547737673910557, DLoss: 0.00013885591028652428, A1: 0.0009241368294913792, A2: 0.0009751177483518735, A3: 0.0008913500371983736, A4: 0.0006182771343736187\n",
      "Epoch 240, Total Loss: 0.0035463972339130124, DLoss: 0.00013013716055535374, A1: 0.0009228856050413015, A2: 0.000976854096907945, A3: 0.0008914749854718386, A4: 0.0006250453905915327\n",
      "Epoch 241, Validation Loss Weighted: 1.6296734809875488, | Validation Unweighted Avg: 0.0008749003754928708\n",
      "Epoch 241, Total Loss: 0.0038084071570417357, DLoss: 0.0003966911805946438, A1: 0.0009234363619477584, A2: 0.0009799611800686266, A3: 0.0008875925389350992, A4: 0.000620725898482471\n",
      "Epoch 242, Total Loss: 0.005254189979049526, DLoss: 0.0018301870639334083, A1: 0.0009216654351596596, A2: 0.0009794306242879843, A3: 0.0008949824740160363, A4: 0.000627924389519566\n",
      "Epoch 243, Total Loss: 0.004880528361139692, DLoss: 0.0014673217703768222, A1: 0.0009232625051729635, A2: 0.000982101572220596, A3: 0.0008962469071754888, A4: 0.0006115956024793624\n",
      "Epoch 244, Total Loss: 0.00464343922788182, DLoss: 0.0012280881191889586, A1: 0.0009183455328742308, A2: 0.000977707156165376, A3: 0.0008921707370353199, A4: 0.0006271276944268968\n",
      "Epoch 245, Total Loss: 0.0038582089137476445, DLoss: 0.000450689140035882, A1: 0.00091905888604577, A2: 0.0009788044197462527, A3: 0.0008938439964925816, A4: 0.0006158124507754275\n",
      "Epoch 246, Total Loss: 0.0035781976452364524, DLoss: 0.0001779366045825141, A1: 0.000913917805808689, A2: 0.0009742914712122994, A3: 0.0008889271021685504, A4: 0.0006231246446366806\n",
      "Epoch 247, Total Loss: 0.003808039687878177, DLoss: 0.00040952475165712796, A1: 0.0009135255572924837, A2: 0.0009789324872558195, A3: 0.000892521557121158, A4: 0.0006135353307978552\n",
      "Epoch 248, Total Loss: 0.007579617296538676, DLoss: 0.0041254198021885135, A1: 0.0009246488323283715, A2: 0.000985425233495169, A3: 0.000912280206049135, A4: 0.0006318432467361907\n",
      "Epoch 249, Total Loss: 0.006907804678749844, DLoss: 0.003455702185866275, A1: 0.0009312249419715292, A2: 0.0009866399656816198, A3: 0.000907791582846006, A4: 0.0006264460077380301\n",
      "Epoch 250, Total Loss: 0.003723079117844463, DLoss: 0.0003280244634714274, A1: 0.0009156013262203157, A2: 0.0009733867313694976, A3: 0.0008921290142552607, A4: 0.0006139376030887434\n",
      "Epoch 251, Validation Loss Weighted: 1.5673794746398926, | Validation Unweighted Avg: 0.0007008969550952315\n",
      "Epoch 251, Total Loss: 0.0034904069387266647, DLoss: 0.00010203457258781286, A1: 0.0009123307678919445, A2: 0.0009718143184396798, A3: 0.0008910058961439724, A4: 0.0006132213751429431\n",
      "Epoch 252, Total Loss: 0.0035347797512341374, DLoss: 0.0001465565010304007, A1: 0.000911054325592886, A2: 0.0009725737948594377, A3: 0.0008900466131080975, A4: 0.0006145485050493252\n",
      "Epoch 253, Total Loss: 0.005048467996940334, DLoss: 0.0016224931723627378, A1: 0.0009154182792041287, A2: 0.0009811227701034526, A3: 0.000893874872731844, A4: 0.0006355589176440514\n",
      "Epoch 254, Total Loss: 0.005911732037797761, DLoss: 0.002508721297735147, A1: 0.0009267243562672692, A2: 0.0009674792845048201, A3: 0.0008877488528510598, A4: 0.0006210582369972558\n",
      "Epoch 255, Total Loss: 0.005420659096522617, DLoss: 0.0020354163909360066, A1: 0.000913563828622831, A2: 0.0009714073171794139, A3: 0.0008930449366268725, A4: 0.0006072266382695737\n",
      "Epoch 256, Total Loss: 0.00405493098958025, DLoss: 0.0006917205453481239, A1: 0.000907799716145002, A2: 0.0009665759097515547, A3: 0.0008863333464970227, A4: 0.0006025014666130862\n",
      "Epoch 257, Total Loss: 0.0035240452730332353, DLoss: 0.00013405947832077138, A1: 0.0009100432392270621, A2: 0.0009740653339039537, A3: 0.0008869602012013771, A4: 0.0006189170155618161\n",
      "Epoch 258, Total Loss: 0.0035345357298369475, DLoss: 0.0001391652984693792, A1: 0.0009093000777656099, A2: 0.0009746914729951392, A3: 0.0008923771154141749, A4: 0.0006190017659595138\n",
      "Epoch 259, Total Loss: 0.00417463480265393, DLoss: 0.0007784668995306922, A1: 0.0009123064432866489, A2: 0.0009738228968258641, A3: 0.000891649001143626, A4: 0.0006183895549363363\n",
      "Epoch 260, Total Loss: 0.0059269111550582405, DLoss: 0.0024824189997161737, A1: 0.0009246359308897147, A2: 0.0009860063860597538, A3: 0.0009046855099044974, A4: 0.0006291643224337597\n",
      "Epoch 261, Validation Loss Weighted: 1.6236292123794556, | Validation Unweighted Avg: 0.0007955334149301052\n",
      "Epoch 261, Total Loss: 0.005474149587561525, DLoss: 0.0020760518470607082, A1: 0.0009128805902140481, A2: 0.0009761896318733224, A3: 0.0008950361104118862, A4: 0.0006139913769650527\n",
      "Epoch 262, Total Loss: 0.0035873395505404914, DLoss: 0.00021351216631625323, A1: 0.0009091565507539847, A2: 0.000969363994609425, A3: 0.0008897949089094419, A4: 0.0006055119575442157\n",
      "Epoch 263, Total Loss: 0.0037775687661037822, DLoss: 0.0003864218000341101, A1: 0.0009152932957848944, A2: 0.0009713172822578351, A3: 0.0008852535566182434, A4: 0.0006192828336348943\n",
      "Epoch 264, Total Loss: 0.0037293669059264094, DLoss: 0.00032883718694475564, A1: 0.0009098504715625081, A2: 0.0009769009921145964, A3: 0.0008950806372633286, A4: 0.000618697582678413\n",
      "Epoch 265, Total Loss: 0.004023910778910663, DLoss: 0.0006215119931188052, A1: 0.0009170301234483056, A2: 0.0009676072308510894, A3: 0.0008895771819156422, A4: 0.0006281842453724741\n",
      "Epoch 266, Total Loss: 0.006371287184976444, DLoss: 0.002968742784551895, A1: 0.0009082413392992418, A2: 0.0009781775517653428, A3: 0.0008848631175376134, A4: 0.000631262413821787\n",
      "Epoch 267, Total Loss: 0.004758915510451929, DLoss: 0.0013418421611938603, A1: 0.0009152358223115119, A2: 0.0009794723220796764, A3: 0.0009013947689027191, A4: 0.0006209704367889075\n",
      "Epoch 268, Total Loss: 0.00408351205455388, DLoss: 0.0006941572374754204, A1: 0.0009102644334585752, A2: 0.0009658645058400659, A3: 0.000894221209668186, A4: 0.0006190046605297539\n",
      "Epoch 269, Total Loss: 0.0037437941729546185, DLoss: 0.00036941075354248327, A1: 0.0008995565783090404, A2: 0.0009691546435057965, A3: 0.0008906466489282717, A4: 0.0006150255318454418\n",
      "Epoch 270, Total Loss: 0.004349132816357517, DLoss: 0.0009476510263167173, A1: 0.0009088812647302224, A2: 0.0009748421451997108, A3: 0.0008932269933066808, A4: 0.0006245313933814858\n",
      "Epoch 271, Validation Loss Weighted: 1.6337240934371948, | Validation Unweighted Avg: 0.0007402921328321099\n",
      "Epoch 271, Total Loss: 0.00427946361570238, DLoss: 0.0009186801629105255, A1: 0.0009058754170134556, A2: 0.0009667760817137605, A3: 0.0008812692028393279, A4: 0.0006068627496895054\n",
      "Epoch 272, Total Loss: 0.003702776101735336, DLoss: 0.00033175807464596636, A1: 0.0009091875428959446, A2: 0.000969279490341664, A3: 0.000890343417540862, A4: 0.0006022075741487821\n",
      "Epoch 273, Total Loss: 0.006382947773884305, DLoss: 0.002968239269069851, A1: 0.0009162288722419279, A2: 0.0009803967219072134, A3: 0.0008907459011383749, A4: 0.0006273369898219079\n",
      "Epoch 274, Total Loss: 0.007804348339215556, DLoss: 0.004342899477209853, A1: 0.0009468239077085111, A2: 0.000984287989252731, A3: 0.0009105847440216828, A4: 0.0006197522586841263\n",
      "Epoch 275, Total Loss: 0.0036242067239403365, DLoss: 0.000250758547686432, A1: 0.0009118467844080608, A2: 0.0009708512423747545, A3: 0.000883672989437979, A4: 0.0006070771559238479\n",
      "Epoch 276, Total Loss: 0.003495300125392863, DLoss: 0.00011956450047033616, A1: 0.000908755812257699, A2: 0.0009729663314374193, A3: 0.0008783239116672287, A4: 0.0006156895517712903\n",
      "Epoch 277, Total Loss: 0.0034845924006566002, DLoss: 0.00012931887888705454, A1: 0.0009035433143294953, A2: 0.0009710895892753383, A3: 0.0008822237850546861, A4: 0.000598416833045948\n",
      "Epoch 278, Total Loss: 0.003614030677470675, DLoss: 0.00022472271584774716, A1: 0.0009044105688686117, A2: 0.0009709198957559486, A3: 0.0008885435456498024, A4: 0.0006254339594244646\n",
      "Epoch 279, Total Loss: 0.00445855617320128, DLoss: 0.001073027022886725, A1: 0.0009169030820988733, A2: 0.0009713887131676364, A3: 0.0008859805521234937, A4: 0.0006112568099690014\n",
      "Epoch 280, Total Loss: 0.005826299088618147, DLoss: 0.002421303214743437, A1: 0.0009264279633043771, A2: 0.000980331456935346, A3: 0.0008879577112049282, A4: 0.0006102787328059507\n",
      "Epoch 281, Validation Loss Weighted: 1.595048427581787, | Validation Unweighted Avg: 0.000818752683699131\n",
      "Epoch 281, Total Loss: 0.004645533377399922, DLoss: 0.0012404605099039724, A1: 0.0009325717525412487, A2: 0.0009729397895915264, A3: 0.0008864008454669271, A4: 0.0006131604690257187\n",
      "Epoch 282, Total Loss: 0.0038701246426750334, DLoss: 0.00047452660857578104, A1: 0.0009249690535439599, A2: 0.0009751001577755729, A3: 0.0008829722565286085, A4: 0.0006125565690602095\n",
      "Epoch 283, Total Loss: 0.0037279305727927503, DLoss: 0.0003336039473073139, A1: 0.0009221786227499251, A2: 0.0009724293292678836, A3: 0.0008853332173802384, A4: 0.000614385470116345\n",
      "Epoch 284, Total Loss: 0.004841446966350883, DLoss: 0.0014315364878381264, A1: 0.000922410209457005, A2: 0.0009694298704868164, A3: 0.0008898471296788557, A4: 0.0006282232796448542\n",
      "Epoch 285, Total Loss: 0.0046462863059291105, DLoss: 0.0012545626187189058, A1: 0.0009193859907492714, A2: 0.0009694516480584009, A3: 0.0008859210850170546, A4: 0.000616964962389168\n",
      "Epoch 286, Total Loss: 0.0041055579890698106, DLoss: 0.0007185093689258792, A1: 0.0009213863085216706, A2: 0.0009714565744574925, A3: 0.0008853579312389312, A4: 0.0006088477801106571\n",
      "Epoch 287, Total Loss: 0.004094934473555027, DLoss: 0.0007110681546857284, A1: 0.0009165063290063037, A2: 0.000966477480468794, A3: 0.0008876602131646326, A4: 0.0006132222992949788\n",
      "Epoch 288, Total Loss: 0.0047106311622139236, DLoss: 0.0013307675336611945, A1: 0.0009168641233558943, A2: 0.0009645390670564004, A3: 0.0008839041930452202, A4: 0.000614556244454434\n",
      "Epoch 289, Total Loss: 0.004310035653774817, DLoss: 0.0009164371464232152, A1: 0.000918979144671539, A2: 0.0009751884333209918, A3: 0.0008874389835847813, A4: 0.0006119919507889129\n",
      "Epoch 290, Total Loss: 0.003902196931382853, DLoss: 0.0005388159649142339, A1: 0.0009150572528824341, A2: 0.0009655884322062941, A3: 0.0008853314493101359, A4: 0.0005974038251141875\n",
      "Epoch 291, Validation Loss Weighted: 1.618730902671814, | Validation Unweighted Avg: 0.0007737161940895021\n",
      "Epoch 291, Total Loss: 0.0038014467781150333, DLoss: 0.0004165545805087525, A1: 0.0009115969331022907, A2: 0.0009698540055234679, A3: 0.0008885291207310398, A4: 0.000614912128819676\n",
      "Epoch 292, Total Loss: 0.005158606663793431, DLoss: 0.0017700476385057713, A1: 0.0009079957462931402, A2: 0.0009754751092466243, A3: 0.0008916897285499462, A4: 0.0006133984548713753\n",
      "Epoch 293, Total Loss: 0.004178102266715458, DLoss: 0.0007876862346827032, A1: 0.0009137861399866862, A2: 0.0009687508866735781, A3: 0.0008937310360248706, A4: 0.0006141479759972671\n",
      "Epoch 294, Total Loss: 0.004397630270018867, DLoss: 0.0009844934638749692, A1: 0.000918205801727626, A2: 0.0009728629859285899, A3: 0.0008916651595241505, A4: 0.0006304028449387096\n",
      "Epoch 295, Total Loss: 0.003982466035350543, DLoss: 0.0006200575368480217, A1: 0.0009064793986908626, A2: 0.0009689478552920925, A3: 0.0008791472324012581, A4: 0.0006078339997491805\n",
      "Epoch 296, Total Loss: 0.0038114758029802365, DLoss: 0.00041729958748791513, A1: 0.0009240470424608495, A2: 0.0009733679525673076, A3: 0.0008830107302049906, A4: 0.0006137505023306486\n",
      "Epoch 297, Total Loss: 0.004639875656059964, DLoss: 0.0012444361594217215, A1: 0.0009077340848472721, A2: 0.0009712977546769252, A3: 0.0008915367548173559, A4: 0.0006248708964387168\n",
      "Epoch 298, Total Loss: 0.004061546334890987, DLoss: 0.0006869875694312875, A1: 0.000906800439468663, A2: 0.0009720447229111331, A3: 0.0008824611351532109, A4: 0.0006132524644602777\n",
      "Epoch 299, Total Loss: 0.0042163063574910415, DLoss: 0.0008371843879021154, A1: 0.0009051859313000353, A2: 0.0009727079355054064, A3: 0.0008866666886144065, A4: 0.0006145614004357082\n",
      "Epoch 300, Total Loss: 0.009625726702109783, DLoss: 0.006176635738070631, A1: 0.0009276180394656628, A2: 0.0009967907421510477, A3: 0.0008984268516335512, A4: 0.0006262553880353814\n",
      "Epoch 301, Validation Loss Weighted: 1.5919240713119507, | Validation Unweighted Avg: 0.0007056997856125236\n",
      "Epoch 301, Total Loss: 0.004006485238261865, DLoss: 0.0006222901635655118, A1: 0.0009114644133433103, A2: 0.0009734604029476528, A3: 0.0008902751861176677, A4: 0.0006089950779038521\n",
      "Epoch 302, Total Loss: 0.00342494317461718, DLoss: 5.627304050325289e-05, A1: 0.0009120387880991323, A2: 0.0009669133678492223, A3: 0.0008883026277689252, A4: 0.0006014153280623512\n",
      "Epoch 303, Total Loss: 0.003436903270779691, DLoss: 5.5306604802105785e-05, A1: 0.000911167351879075, A2: 0.0009707376529935242, A3: 0.000886977988274008, A4: 0.0006127136486466869\n",
      "Epoch 304, Total Loss: 0.0035232043422597187, DLoss: 0.00013276238390641415, A1: 0.0009088939765330327, A2: 0.0009717672644765943, A3: 0.0008900603709000941, A4: 0.0006197203465655382\n",
      "Epoch 305, Total Loss: 0.003755702884187816, DLoss: 0.0003831400871025504, A1: 0.0009178399384133875, A2: 0.0009651077374304474, A3: 0.0008829325449989317, A4: 0.0006066825852175581\n",
      "Epoch 306, Total Loss: 0.004369642906187272, DLoss: 0.0009949568509413935, A1: 0.0009065607916289247, A2: 0.0009673837094706786, A3: 0.0008835672217814631, A4: 0.0006171743372988203\n",
      "Epoch 307, Total Loss: 0.004599903662231306, DLoss: 0.0012254473199639257, A1: 0.000910712042780853, A2: 0.0009607452470018787, A3: 0.0008841684039949045, A4: 0.000618830652251745\n",
      "Epoch 308, Total Loss: 0.004355871695059415, DLoss: 0.0009715716994171751, A1: 0.0009116769243953306, A2: 0.000970556695690241, A3: 0.0008904645705921542, A4: 0.0006116018120172322\n",
      "Epoch 309, Total Loss: 0.0036152380241798133, DLoss: 0.0002367294808581474, A1: 0.0009088980976746991, A2: 0.0009700789766156958, A3: 0.0008872588761794925, A4: 0.0006122725669022408\n",
      "Epoch 310, Total Loss: 0.003485825740112473, DLoss: 0.00012774794986398775, A1: 0.0009088040043665883, A2: 0.0009677314553280006, A3: 0.0008798639652220366, A4: 0.0006016783876268825\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Monitor gradients before clipping and stepping\u001b[39;00m\n\u001b[1;32m     50\u001b[0m all_gradients\u001b[38;5;241m.\u001b[39mappend(image_world\u001b[38;5;241m.\u001b[39mstore_gradients(model))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_gradients = []\n",
    "model.train()  \n",
    "\n",
    "for epoch in range(3000):  # epochs\n",
    "    avg_loss = 0\n",
    "    avg_decode_loss, avg_first_loss, avg_second_loss, avg_third_loss, avg_fourth_loss = 0, 0, 0, 0, 0\n",
    "    raw_gradients = []\n",
    "\n",
    "    for t in random.sample(train_data, len(train_data)):  # sample through all data in random order each epoch\n",
    "        # Get reconstruction loss to help ground abstract state\n",
    "        decoded_values, transition_probabilities = model(t[0])\n",
    "        decode_loss = F.mse_loss(decoded_values[0], t[0], reduction='sum')\n",
    "\n",
    "        # Flatten transition probabilities to then weigh with loss of each predicted state at each layer\n",
    "        first = transition_probabilities[0].view(-1,1,1,1)\n",
    "        second = transition_probabilities[1].view(-1,1,1,1)\n",
    "        third = transition_probabilities[2].view(-1,1,1,1)\n",
    "        fourth = transition_probabilities[3].view(-1,1,1,1)\n",
    "\n",
    "        #Weighted Transitions\n",
    "        # first_loss = (F.mse_loss(decoded_values[1], t[1], reduction='none') * first).sum()\n",
    "        # second_loss = (F.mse_loss(decoded_values[2], t[2], reduction='none') * second).sum()\n",
    "        # third_loss = (F.mse_loss(decoded_values[3], t[3], reduction='none') * third).sum()\n",
    "        # fourth_loss = (F.mse_loss(decoded_values[4], t[4], reduction='none') * fourth).sum()\n",
    "\n",
    "        #Greedy Policy (Squeezing to eliminate batch and channel dimensions)\n",
    "        first_loss = (F.mse_loss(decoded_values[1][first.argmax()].squeeze(0),t[1].squeeze(0).squeeze(0)))\n",
    "        second_loss = (F.mse_loss(decoded_values[2][second.argmax()].squeeze(0),t[2].squeeze(0).squeeze(0)))\n",
    "        third_loss = (F.mse_loss(decoded_values[3][third.argmax()].squeeze(0),t[3].squeeze(0).squeeze(0)))\n",
    "        fourth_loss = (F.mse_loss(decoded_values[4][fourth.argmax()].squeeze(0),t[4].squeeze(0).squeeze(0)))\n",
    "\n",
    "        total_loss = first_loss + second_loss  + third_loss  + fourth_loss + decode_loss\n",
    "\n",
    "        # break if total loss is nan\n",
    "        if torch.isnan(total_loss):\n",
    "            raise ValueError(\"NAN LOSS\")\n",
    "\n",
    "\n",
    "        avg_decode_loss += decode_loss.item()\n",
    "        avg_first_loss += first_loss.item()\n",
    "        avg_second_loss += second_loss.item()\n",
    "        avg_third_loss += third_loss.item()\n",
    "        avg_fourth_loss += fourth_loss.item()\n",
    "        avg_loss += total_loss.item()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        # Monitor gradients before clipping and stepping\n",
    "        all_gradients.append(image_world.store_gradients(model))\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0: \n",
    "        #print just validation\n",
    "        print(f\"Epoch {epoch + 1}, Validation Loss Weighted: {image_world.validate(model, valid_data,weighted=True)}, | Validation Unweighted Avg: {image_world.validate(model, valid_data,weighted=False)/5}\")\n",
    "\n",
    "    #Individual Lossses\n",
    "    avg_decode_loss = avg_decode_loss / len(train_data)\n",
    "    avg_first_loss = avg_first_loss / len(train_data)\n",
    "    avg_second_loss = avg_second_loss / len(train_data)\n",
    "    avg_third_loss = avg_third_loss / len(train_data)\n",
    "    avg_fourth_loss = avg_fourth_loss / len(train_data)\n",
    "    #Full Loss\n",
    "    avg_train_loss = avg_loss / len(train_data)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Total Loss: {avg_train_loss}, DLoss: {avg_decode_loss}, A1: {avg_first_loss}, A2: {avg_second_loss}, A3: {avg_third_loss}, A4: {avg_fourth_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "#torch.save(model.state_dict(), 'model.pth')\n",
    "#load model\n",
    "#model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition_fun: 0.0\n",
      "decoder.fc.weight: 0.05\n",
      "decoder.fc.bias: 0.01\n",
      "decoder.deconv1.weight: 0.07\n",
      "decoder.deconv1.bias: 0.02\n",
      "decoder.deconv2.weight: 0.07\n",
      "decoder.deconv2.bias: 0.03\n",
      "decoder.final_conv.weight: 0.17\n",
      "decoder.final_conv.bias: 0.04\n",
      "encoder.cnn_encoder.conv1.weight: 0.01\n",
      "encoder.cnn_encoder.conv1.bias: 0.01\n",
      "encoder.cnn_encoder.bn1.weight: 0.0\n",
      "encoder.cnn_encoder.bn1.bias: 0.0\n",
      "encoder.cnn_encoder.conv2.weight: 0.02\n",
      "encoder.cnn_encoder.conv2.bias: 0.04\n",
      "encoder.cnn_encoder.bn2.weight: 0.01\n",
      "encoder.cnn_encoder.bn2.bias: 0.01\n",
      "encoder.cnn_encoder.conv3.weight: 0.04\n",
      "encoder.cnn_encoder.conv3.bias: 0.02\n",
      "encoder.cnn_encoder.bn3.weight: 0.0\n",
      "encoder.cnn_encoder.bn3.bias: 0.01\n",
      "encoder.cnn_encoder.residual_conv.weight: 0.02\n",
      "encoder.cnn_encoder.residual_conv.bias: 0.01\n",
      "encoder.linear.weight: 0.06\n",
      "encoder.linear.bias: 0.01\n"
     ]
    }
   ],
   "source": [
    "for name, num in all_gradients[-1]:\n",
    "    print(name +':', round(num, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>View Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      " tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "Action: 0\n",
      "Next State:\n",
      " tensor([[-0., -0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0., -0.],\n",
      "        [-0.,  0., -0., -0., -0.,  0., -0.,  0., -0.,  0., -0.,  0.],\n",
      "        [-0., -0., -0.,  0., -0.,  0., -0.,  0., -0., -0., -0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0., -0.,  0.,  0.],\n",
      "        [-0., -0.,  0., -0., -0.,  0.,  0., -0., -0., -0., -0., -0.],\n",
      "        [ 0.,  0., -0., -0.,  0.,  0., -1., -0.,  0., -0., -0.,  0.],\n",
      "        [-0., -0.,  0.,  0., -0., -0.,  0., -0.,  0., -0.,  0.,  0.],\n",
      "        [-0.,  0., -0.,  0., -0.,  0., -0., -0., -0.,  1., -0., -0.],\n",
      "        [-0., -0., -0., -0.,  0., -0., -0., -0.,  0., -0.,  0., -0.],\n",
      "        [ 0., -0., -0.,  0., -0., -0.,  0.,  0., -0.,  0., -0., -0.],\n",
      "        [-0.,  0., -0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0., -0.]])\n",
      "Max Value Decoded: (12, 13)\n"
     ]
    }
   ],
   "source": [
    "image_world.action_viewer(model,start_state=train_data[0][0],actions = [0],shrink=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Path\n",
      "12 13\n",
      "11 13\n",
      "11 12\n",
      "11 11\n",
      "11 10\n",
      "Original:\n",
      " tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "Action: 1\n",
      "Next State:\n",
      " tensor([[ 0., -0.,  0.,  0., -0.,  0.,  0., -0., -0., -0., -0.,  0.],\n",
      "        [-0.,  0.,  0., -0., -0., -0.,  0.,  0., -0., -0., -0.,  0.],\n",
      "        [ 0., -0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0.,  0., -0.],\n",
      "        [-0.,  0., -0.,  0., -0.,  0., -0.,  0., -0., -0.,  0.,  0.],\n",
      "        [-0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0.],\n",
      "        [-0.,  0., -0.,  0., -0., -0., -0.,  0.,  0., -0.,  0., -0.],\n",
      "        [-0.,  0.,  0.,  0., -0.,  0., -1., -0., -0.,  0.,  0.,  0.],\n",
      "        [-0., -0., -0.,  0., -0.,  0., -0., -0.,  0.,  0., -0.,  0.],\n",
      "        [-0.,  0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0., -0., -0.],\n",
      "        [ 0.,  0., -0., -0., -0.,  0., -0.,  0., -0.,  0., -0., -0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0., -0.],\n",
      "        [-0.,  0., -0., -0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.]])\n",
      "Max Value Decoded: (11, 13)\n",
      "Action: 2\n",
      "Next State:\n",
      " tensor([[-0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0.,  0., -0., -0.],\n",
      "        [ 0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0., -0., -0.,  0.],\n",
      "        [ 0.,  0., -0.,  0., -0., -0.,  0.,  0.,  0., -0., -0., -0.],\n",
      "        [-0., -0., -0.,  0., -0., -0., -0., -0.,  0., -0.,  0., -0.],\n",
      "        [-0., -0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0., -0.,  0.],\n",
      "        [-0., -0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0., -0., -0.],\n",
      "        [ 0.,  0.,  0.,  0., -0.,  0., -1., -0.,  0.,  0.,  0.,  0.],\n",
      "        [-0.,  0., -0.,  0., -0., -0., -0., -0.,  0.,  0., -0.,  0.],\n",
      "        [ 0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., -0.,  0., -0.,  0.,  0., -0., -0., -0.,  0.,  0.],\n",
      "        [ 0., -0., -0., -0.,  0., -0.,  0., -0.,  0., -0., -0.,  0.],\n",
      "        [ 0.,  0.,  0., -0., -0.,  0., -0., -0.,  0.,  0., -0.,  0.]])\n",
      "Max Value Decoded: (11, 12)\n",
      "Action: 2\n",
      "Next State:\n",
      " tensor([[-0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0.,  0., -0., -0.],\n",
      "        [ 0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0., -0., -0.,  0.],\n",
      "        [ 0.,  0., -0.,  0., -0., -0.,  0.,  0.,  0., -0., -0., -0.],\n",
      "        [-0., -0., -0.,  0., -0., -0., -0., -0.,  0., -0.,  0., -0.],\n",
      "        [-0., -0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0., -0.,  0.],\n",
      "        [-0., -0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0., -0., -0.],\n",
      "        [ 0.,  0.,  0.,  0., -0.,  0., -1., -0.,  0.,  0.,  0.,  0.],\n",
      "        [-0.,  0., -0.,  0., -0., -0., -0., -0.,  0.,  0., -0.,  0.],\n",
      "        [ 0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0., -0.,  0., -0.,  0.,  0., -0., -0., -0.,  0.,  0.],\n",
      "        [ 0., -0., -0., -0.,  0., -0.,  0., -0.,  0., -0., -0.,  0.],\n",
      "        [ 0.,  0.,  0., -0., -0.,  0., -0., -0.,  0.,  0., -0.,  0.]])\n",
      "Max Value Decoded: (11, 12)\n",
      "Action: 3\n",
      "Next State:\n",
      " tensor([[-0.,  0.,  0.,  0., -0., -0., -0.,  0., -0.,  0.,  0.,  0.],\n",
      "        [-0., -0.,  0.,  0., -0., -0.,  0., -0., -0.,  0., -0.,  0.],\n",
      "        [-0.,  0., -0.,  0., -0., -0.,  0.,  0.,  0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0.,  0.,  0., -0.,  0., -0., -0.,  0.,  0.],\n",
      "        [ 0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0., -0., -0.],\n",
      "        [ 0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0., -0., -0.],\n",
      "        [ 0., -0., -0., -0.,  0., -0., -1., -0.,  0., -0.,  0., -0.],\n",
      "        [-0., -0.,  0., -0., -0.,  0.,  0.,  0.,  0., -0., -0.,  0.],\n",
      "        [ 0., -0.,  0.,  0., -0., -0.,  0.,  0., -0.,  0., -0., -0.],\n",
      "        [-0.,  0., -0.,  0., -0.,  0., -0., -0., -0.,  0.,  0., -0.],\n",
      "        [-0., -0., -0., -0., -0.,  0., -0.,  0.,  0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0., -0., -0., -0., -0.,  0.,  0., -0.,  0.]])\n",
      "Max Value Decoded: (10, 12)\n"
     ]
    }
   ],
   "source": [
    "image_world.view_greedy_path(model,start_state=train_data[0],shrink=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Action 1    0.527086\n",
       "Action 2    1.263159\n",
       "Action 3    0.943524\n",
       "Action 4    1.067746\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_df = image_world.get_action_df(model,valid_data)\n",
    "action_df.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action 1</th>\n",
       "      <th>Action 2</th>\n",
       "      <th>Action 3</th>\n",
       "      <th>Action 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Action 1  Action 2  Action 3  Action 4\n",
       "0         0         0         1         0\n",
       "1         1         0         0         1\n",
       "2         0         1         3         1\n",
       "3         0         1         0         2\n",
       "4         1         0         1         0\n",
       "5         0         0         1         0\n",
       "6         1         0         2         2\n",
       "7         0         1         3         1\n",
       "8         0         1         0         2\n",
       "9         0         0         1         0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
