{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn import Parameter\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from treeQN.treeqn_traj_simplest import TreeQN\n",
    "import image_world\n",
    "import random\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = image_world.get_data(size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0025)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(20,20)\n",
    "b = torch.zeros(20,20)\n",
    "b[0][0] = 1\n",
    "F.mse_loss(a,b) #Target Loss should be at least less than this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition Transition\n"
     ]
    }
   ],
   "source": [
    "input_shape = train_data[0][0].shape# minimum size #train_data[0][0].shape\n",
    "num_actions = 4\n",
    "tree_depth = 4\n",
    "embedding_dim = 64\n",
    "gamma = 1 \n",
    "decode_dropout = 0.5\n",
    "t1 =False#True is Einsum. False +dx \n",
    "model = TreeQN(input_shape=input_shape, num_actions=num_actions, tree_depth=tree_depth, embedding_dim=embedding_dim, gamma=gamma,decode_dropout=decode_dropout,t1=t1)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Optional: Pretrain Autoencoder</h1>\n",
    "(Doesn't seem necessary, useful for testing if decoding Z is possible though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_world.train_autoencoder(model,optimizer,train_data,valid_data,epochs=100,lambda_reg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #freeze encoder and decoder\n",
    "# for param in model.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "# for param in model.decoder.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test_autoencode ability\n",
    "# def test_autoencoder(model,valid_data):\n",
    "#     with torch.no_grad():\n",
    "#         test_sample = random.choice(valid_data)\n",
    "#         encoding = model.encoder(test_sample[0])\n",
    "#         decoding = model.decoder(encoding)\n",
    "#         print('Original:\\n', test_sample[0].numpy()[0][0][4:-4, 4:-4])\n",
    "#         print('Reconstructed:\\n', np.round(decoding.numpy()[0][0][4:-4, 4:-4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/Desktop/TreeQN/BIB/TreeQN_Approach/Notebooks/image_gridworld/image_world.py:129: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([4, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  first_loss = (F.mse_loss(decoded_values[1], t[1], reduction='none') * first).sum().item()\n",
      "/home/mike/Desktop/TreeQN/BIB/TreeQN_Approach/Notebooks/image_gridworld/image_world.py:130: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([16, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  second_loss = (F.mse_loss(decoded_values[2], t[2], reduction='none') * second).sum().item()\n",
      "/home/mike/Desktop/TreeQN/BIB/TreeQN_Approach/Notebooks/image_gridworld/image_world.py:131: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([64, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  third_loss = (F.mse_loss(decoded_values[3], t[3], reduction='none') * third).sum().item()\n",
      "/home/mike/Desktop/TreeQN/BIB/TreeQN_Approach/Notebooks/image_gridworld/image_world.py:132: UserWarning: Using a target size (torch.Size([1, 1, 20, 20])) that is different to the input size (torch.Size([256, 1, 20, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  fourth_loss = (F.mse_loss(decoded_values[4], t[4], reduction='none') * fourth).sum().item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss Weighted: 7.439789772033691, | Validation Unweighted Avg: 0.19274857640266418\n",
      "Epoch 1, Total Loss: 1.2369908419522373, DLoss: 1.2200149381702596, A1: 0.003669295147781006, A2: 0.004197894159535116, A3: 0.004483438099057159, A4: 0.004625277180986648\n",
      "Epoch 2, Total Loss: 0.7894586377523162, DLoss: 0.7737732619047165, A1: 0.003151914122810757, A2: 0.0038053735889579084, A3: 0.004252355118197474, A4: 0.004475733521394432\n",
      "Epoch 3, Total Loss: 0.2339936514808373, DLoss: 0.2177554703994908, A1: 0.0035026688657870345, A2: 0.003916942335622893, A3: 0.004283590494147079, A4: 0.004534979539246044\n",
      "Epoch 4, Total Loss: 0.0473525602539832, DLoss: 0.031303209073799236, A1: 0.0035697479129091583, A2: 0.003853677781510421, A3: 0.004185600163923068, A4: 0.004440325301733206\n",
      "Epoch 5, Total Loss: 0.024513576967133716, DLoss: 0.00882391597154889, A1: 0.0035227768652310427, A2: 0.003758428745310415, A3: 0.004075952674346892, A4: 0.0043325025948103175\n",
      "Epoch 6, Total Loss: 0.018834892694245684, DLoss: 0.0035697142805226826, A1: 0.0034674075273373587, A2: 0.0036453077315606853, A3: 0.003951929878993806, A4: 0.004200533274772831\n",
      "Epoch 7, Total Loss: 0.0246381896039979, DLoss: 0.009795307047368789, A1: 0.003424519227436659, A2: 0.003538016001270576, A3: 0.003822743443941528, A4: 0.0040576039072634145\n",
      "Epoch 8, Total Loss: 0.030298232621597972, DLoss: 0.01580817876859907, A1: 0.0033918948293748226, A2: 0.0034542596560310232, A3: 0.0037124525575729255, A4: 0.003931446772449734\n",
      "Epoch 9, Total Loss: 0.0211368386676027, DLoss: 0.0070866517489776015, A1: 0.003337999753950333, A2: 0.0033576357792216268, A3: 0.0035842288983985783, A4: 0.0037703224119137635\n",
      "Epoch 10, Total Loss: 0.018119856605136937, DLoss: 0.0045754101128708995, A1: 0.0032795219269411806, A2: 0.0032458167586644944, A3: 0.0034376009706069124, A4: 0.003581506838302382\n",
      "Epoch 11, Validation Loss Weighted: 5.063249111175537, | Validation Unweighted Avg: 0.004354264587163925\n",
      "Epoch 11, Total Loss: 0.01646349559622732, DLoss: 0.003539396465666042, A1: 0.0032104718033224345, A2: 0.003112521997271952, A3: 0.0032598622958175836, A4: 0.0033412429652261463\n",
      "Epoch 12, Total Loss: 0.02886496780186214, DLoss: 0.01638088998892768, A1: 0.0031760154440152373, A2: 0.0030229760873639448, A3: 0.003128915127705444, A4: 0.003156171230048957\n",
      "Epoch 13, Total Loss: 0.023982373701239176, DLoss: 0.011905492483426563, A1: 0.003142145138487897, A2: 0.0029335886758582834, A3: 0.003012004945511845, A4: 0.002989142344714227\n",
      "Epoch 14, Total Loss: 0.03371214682066982, DLoss: 0.0219467920166525, A1: 0.0031219488507221367, A2: 0.002879696359476921, A3: 0.002928028856827454, A4: 0.0028356807041828606\n",
      "Epoch 15, Total Loss: 0.015485366924919865, DLoss: 0.004240713814347559, A1: 0.0030497061797756363, A2: 0.002774254807313396, A3: 0.0027916600260968235, A4: 0.002629032120404934\n",
      "Epoch 16, Total Loss: 0.014324175981296735, DLoss: 0.0035524045103970405, A1: 0.0029798435841009697, A2: 0.002676477751017294, A3: 0.002672780165448785, A4: 0.0024426699383184314\n",
      "Epoch 17, Total Loss: 0.014632661500945688, DLoss: 0.00418456405856308, A1: 0.0029250360072844406, A2: 0.002606693739918145, A3: 0.002592303495938805, A4: 0.0023240641770164738\n",
      "Epoch 18, Total Loss: 0.022857695331119678, DLoss: 0.012550232227129693, A1: 0.002900939610448073, A2: 0.0025783058954402804, A3: 0.0025552953889762813, A4: 0.002272922221825204\n",
      "Epoch 19, Total Loss: 0.030267959185452623, DLoss: 0.019942854659166186, A1: 0.0029098093647255815, A2: 0.0025889861397445204, A3: 0.002550574212165719, A4: 0.0022757349514656447\n",
      "Epoch 20, Total Loss: 0.024383614529770885, DLoss: 0.014217298456721685, A1: 0.0028736315784044565, A2: 0.0025518179193816403, A3: 0.0025166997029869393, A4: 0.0022241669373629105\n",
      "Epoch 21, Validation Loss Weighted: 4.028733730316162, | Validation Unweighted Avg: 0.002682457212358713\n",
      "Epoch 21, Total Loss: 0.017364214623177592, DLoss: 0.00735891007008666, A1: 0.0028298821691846985, A2: 0.0025135427014902232, A3: 0.002484049650163136, A4: 0.0021778300674420528\n",
      "Epoch 22, Total Loss: 0.012633012324063615, DLoss: 0.0027772317139103755, A1: 0.002782211722594432, A2: 0.0024833384933034804, A3: 0.0024564013742333786, A4: 0.0021338290519038725\n",
      "Epoch 23, Total Loss: 0.015135066473687236, DLoss: 0.005381994831259363, A1: 0.0027468098285184663, A2: 0.00246143934359266, A3: 0.0024385074100626465, A4: 0.002106315015540035\n",
      "Epoch 24, Total Loss: 0.017712512197480962, DLoss: 0.008006353173616596, A1: 0.0027314039158888837, A2: 0.0024552215049466627, A3: 0.002427564146505161, A4: 0.002091969479806721\n",
      "Epoch 25, Total Loss: 0.02968024935742671, DLoss: 0.019977474568242375, A1: 0.0027290782721882518, A2: 0.0024581133165735413, A3: 0.0024305063352750783, A4: 0.0020850768180521715\n",
      "Epoch 26, Total Loss: 0.018875015416944568, DLoss: 0.009252003373959186, A1: 0.002711626481984488, A2: 0.002436700401912359, A3: 0.0024189316360703245, A4: 0.0020557534986768255\n",
      "Epoch 27, Total Loss: 0.011113486002961344, DLoss: 0.0016316888235451189, A1: 0.002662411427379332, A2: 0.0023964912694116884, A3: 0.0023976302472874522, A4: 0.002025264193600213\n",
      "Epoch 28, Total Loss: 0.019003835811533712, DLoss: 0.009583773226768243, A1: 0.002641374196603217, A2: 0.0023825696119191974, A3: 0.0023868433305655012, A4: 0.002009275465124202\n",
      "Epoch 29, Total Loss: 0.01909111700105396, DLoss: 0.009723608183611015, A1: 0.0026325081654993646, A2: 0.0023680930803741582, A3: 0.0023775498477996075, A4: 0.0019893577417612754\n",
      "Epoch 30, Total Loss: 0.022755277351560917, DLoss: 0.013383575061082162, A1: 0.0026407223321836103, A2: 0.002367398733886975, A3: 0.0023763501826165752, A4: 0.0019872310931201686\n",
      "Epoch 31, Validation Loss Weighted: 3.7878472805023193, | Validation Unweighted Avg: 0.0034343514125794172\n",
      "Epoch 31, Total Loss: 0.0218663212927905, DLoss: 0.012560151133749803, A1: 0.0026239681025882334, A2: 0.0023563418341588905, A3: 0.002367426358713684, A4: 0.0019584338503508742\n",
      "Epoch 32, Total Loss: 0.01857464325783605, DLoss: 0.009327999064275488, A1: 0.0026062469559044324, A2: 0.002339764424091713, A3: 0.0023614632928828623, A4: 0.0019391695196232335\n",
      "Epoch 33, Total Loss: 0.01545590920458463, DLoss: 0.006291946967460469, A1: 0.0025871820261024617, A2: 0.002320343164451928, A3: 0.0023497440534728494, A4: 0.0019066929938906635\n",
      "Epoch 34, Total Loss: 0.015027668576856905, DLoss: 0.005948579144685275, A1: 0.002564480260480195, A2: 0.0023004796922164545, A3: 0.002336003227074715, A4: 0.0018781263164286924\n",
      "Epoch 35, Total Loss: 0.014840722126378254, DLoss: 0.005832682706849565, A1: 0.002549588985063813, A2: 0.0022801471339665693, A3: 0.0023271409699439326, A4: 0.0018511623609811067\n",
      "Epoch 36, Total Loss: 0.02507676973684945, DLoss: 0.016104757802201098, A1: 0.0025415541350164196, A2: 0.00227804649356668, A3: 0.0023271462932991033, A4: 0.0018252649180464109\n",
      "Epoch 37, Total Loss: 0.04505024083297361, DLoss: 0.03592039613921026, A1: 0.002609089444476095, A2: 0.002319200323704122, A3: 0.002344826387707144, A4: 0.001856728408231654\n",
      "Epoch 38, Total Loss: 0.011738175665959716, DLoss: 0.0028223463671483017, A1: 0.002559471416117793, A2: 0.0022581783837681128, A3: 0.0023161599528975785, A4: 0.0017820195416623557\n",
      "Epoch 39, Total Loss: 0.009475085310722617, DLoss: 0.000680864760579425, A1: 0.0025256803068755703, A2: 0.0022300346982030367, A3: 0.0023011353343132543, A4: 0.0017373702160760082\n",
      "Epoch 40, Total Loss: 0.008990325296128338, DLoss: 0.0003085501777604804, A1: 0.002494122935670682, A2: 0.002203563309740275, A3: 0.002285710170822726, A4: 0.001698378689417785\n",
      "Epoch 41, Validation Loss Weighted: 3.6218810081481934, | Validation Unweighted Avg: 0.00637986371293664\n",
      "Epoch 41, Total Loss: 0.015286773692985827, DLoss: 0.006655524428523759, A1: 0.0024876682405275377, A2: 0.0021920267381409014, A3: 0.0022787656116468663, A4: 0.0016727887649639426\n",
      "Epoch 42, Total Loss: 0.01746319891376929, DLoss: 0.008834884203811684, A1: 0.002496817030689933, A2: 0.0021895699155389924, A3: 0.0022732106418433514, A4: 0.0016687171269123527\n",
      "Epoch 43, Total Loss: 0.011963929752395911, DLoss: 0.003445850258735432, A1: 0.0024716218637133185, A2: 0.0021626112262972375, A3: 0.0022676153912801634, A4: 0.0016162310187196867\n",
      "Epoch 44, Total Loss: 0.010752056337977675, DLoss: 0.0023482688391926456, A1: 0.002446078945120627, A2: 0.0021371038995725527, A3: 0.002244087656832893, A4: 0.0015765170288928362\n",
      "Epoch 45, Total Loss: 0.0295303445732729, DLoss: 0.021076029690977355, A1: 0.002468373185150664, A2: 0.0021459835152861408, A3: 0.0022589738078584724, A4: 0.0015809844255934215\n",
      "Epoch 46, Total Loss: 0.023612428604709832, DLoss: 0.01521050483517518, A1: 0.0024733514249832794, A2: 0.0021368686194446953, A3: 0.00225569897712293, A4: 0.001536004795343615\n",
      "Epoch 47, Total Loss: 0.018468912060118536, DLoss: 0.010145713755892674, A1: 0.002457063187929717, A2: 0.002117032075660642, A3: 0.0022359356040727685, A4: 0.0015131674725456501\n",
      "Epoch 48, Total Loss: 0.011929945160888814, DLoss: 0.0037123277227113826, A1: 0.00243302137400447, A2: 0.0021005708412055605, A3: 0.0022211816013706, A4: 0.0014628436622760174\n",
      "Epoch 49, Total Loss: 0.00861853147348897, DLoss: 0.0005302756551662407, A1: 0.002407277659089728, A2: 0.0020696635760197586, A3: 0.0021935689321253447, A4: 0.0014177456895016472\n",
      "Epoch 50, Total Loss: 0.008706033953719518, DLoss: 0.0007209322848377104, A1: 0.002393839211965149, A2: 0.0020432049212765627, A3: 0.002166273063895377, A4: 0.001381784449966455\n",
      "Epoch 51, Validation Loss Weighted: 3.335111618041992, | Validation Unweighted Avg: 0.004579697735607624\n",
      "Epoch 51, Total Loss: 0.012436052339828828, DLoss: 0.004535948853696358, A1: 0.0023913096086206763, A2: 0.0020245443341660907, A3: 0.0021512075771831654, A4: 0.0013330419725124639\n",
      "Epoch 52, Total Loss: 0.01719005673869767, DLoss: 0.009314011099261485, A1: 0.0023833614202554932, A2: 0.0020169716005594552, A3: 0.002147639360786839, A4: 0.0013280732588927177\n",
      "Epoch 53, Total Loss: 0.012766966668211602, DLoss: 0.004991531978091436, A1: 0.0023784129033711824, A2: 0.001989604457048699, A3: 0.0021260445233730768, A4: 0.0012813727633329108\n",
      "Epoch 54, Total Loss: 0.020480168735693124, DLoss: 0.012723567393417894, A1: 0.0023811380209570582, A2: 0.001976941631239077, A3: 0.002124137376879596, A4: 0.0012743843653218144\n",
      "Epoch 55, Total Loss: 0.014128612900491465, DLoss: 0.00646843057713175, A1: 0.0023690543910065156, A2: 0.0019562559847889297, A3: 0.0020987892581615596, A4: 0.0012360826191566462\n",
      "Epoch 56, Total Loss: 0.01266911763867194, DLoss: 0.005107879853808713, A1: 0.0023478426155634224, A2: 0.001931610405550931, A3: 0.00208408675071868, A4: 0.0011976980327744967\n",
      "Epoch 57, Total Loss: 0.009823053973642262, DLoss: 0.002345674670769685, A1: 0.0023258701665326955, A2: 0.0019019347477958284, A3: 0.002082026838219132, A4: 0.0011675475986769677\n",
      "Epoch 58, Total Loss: 0.009911410184577108, DLoss: 0.00252115556120936, A1: 0.0023265862932682715, A2: 0.0018689389954405751, A3: 0.0020552530540788377, A4: 0.0011394762866654095\n",
      "Epoch 59, Total Loss: 0.016506651615385306, DLoss: 0.009132007203086025, A1: 0.0023252879523418166, A2: 0.0018594063924286853, A3: 0.00204742486292327, A4: 0.0011425251154419543\n",
      "Epoch 60, Total Loss: 0.013224837023087523, DLoss: 0.005915716324340213, A1: 0.002313518372300843, A2: 0.0018299850460607558, A3: 0.002040330424841324, A4: 0.001125286902375096\n",
      "Epoch 61, Validation Loss Weighted: 3.1011390686035156, | Validation Unweighted Avg: 0.0020955142099410295\n",
      "Epoch 61, Total Loss: 0.011078844727440314, DLoss: 0.0038781041841668244, A1: 0.002291272370017726, A2: 0.001804588942269964, A3: 0.0020095748302992435, A4: 0.0010953044570421546\n",
      "Epoch 62, Total Loss: 0.011085793119855225, DLoss: 0.003956007654117208, A1: 0.002279670885234902, A2: 0.0017824261946688322, A3: 0.001990063832437789, A4: 0.0010776245166198351\n",
      "Epoch 63, Total Loss: 0.009025895771231841, DLoss: 0.002005361305319556, A1: 0.0022557509533891623, A2: 0.0017546289904699238, A3: 0.0019563302011440763, A4: 0.001053824334601152\n",
      "Epoch 64, Total Loss: 0.009824350535530936, DLoss: 0.002872196890563249, A1: 0.0022371282255996697, A2: 0.001735317023386332, A3: 0.0019322696443520148, A4: 0.0010474387123725717\n",
      "Epoch 65, Total Loss: 0.017610511380586436, DLoss: 0.010647587818543384, A1: 0.0022516334614589473, A2: 0.0017519783003742552, A3: 0.0019215697168626568, A4: 0.0010377421185363677\n",
      "Epoch 66, Total Loss: 0.013137171195227315, DLoss: 0.006252630764290437, A1: 0.0022494874503040177, A2: 0.0017252932083052159, A3: 0.0018957430941306732, A4: 0.0010140166779323905\n",
      "Epoch 67, Total Loss: 0.013927113061601465, DLoss: 0.007107570098014548, A1: 0.0022295210838571868, A2: 0.0017132353042887355, A3: 0.0018771092317448083, A4: 0.0009996773557345892\n",
      "Epoch 68, Total Loss: 0.01105106010271067, DLoss: 0.004353290215989215, A1: 0.0022050054516346957, A2: 0.0016815932102459059, A3: 0.0018485090330200777, A4: 0.0009626622307802212\n",
      "Epoch 69, Total Loss: 0.008168412804264913, DLoss: 0.0015999044905170609, A1: 0.0021770243416540324, A2: 0.0016489070281915535, A3: 0.0018076470502736894, A4: 0.0009349298843351955\n",
      "Epoch 70, Total Loss: 0.010002844780683518, DLoss: 0.0034959158385886855, A1: 0.0021608846553135665, A2: 0.001645495703401552, A3: 0.0017800265531563624, A4: 0.0009205220384914852\n",
      "Epoch 71, Validation Loss Weighted: 2.8756589889526367, | Validation Unweighted Avg: 0.0018646462121978402\n",
      "Epoch 71, Total Loss: 0.011316238817843523, DLoss: 0.00486458693326726, A1: 0.002151179798370735, A2: 0.0016319354787066747, A3: 0.001754779118875211, A4: 0.0009137574783711567\n",
      "Epoch 72, Total Loss: 0.016652627191929656, DLoss: 0.010201860971441917, A1: 0.0021573518339375202, A2: 0.0016472069554517722, A3: 0.0017401692737952213, A4: 0.0009060381610073488\n",
      "Epoch 73, Total Loss: 0.010647432673299177, DLoss: 0.0042870453346536535, A1: 0.0021438038220036437, A2: 0.0016103415202285926, A3: 0.0017180848178792406, A4: 0.0008881571709273637\n",
      "Epoch 74, Total Loss: 0.006778355151287873, DLoss: 0.0005497167349941182, A1: 0.002111355865649371, A2: 0.0015788409361531112, A3: 0.0016735427832992916, A4: 0.0008648988278847273\n",
      "Epoch 75, Total Loss: 0.007897781106558713, DLoss: 0.0017549848428395555, A1: 0.002084252679064362, A2: 0.0015730849107388745, A3: 0.001636825026203455, A4: 0.0008486336062064352\n",
      "Epoch 76, Total Loss: 0.013103616163557904, DLoss: 0.006957881557305386, A1: 0.0020879140446512876, A2: 0.0015809698210267182, A3: 0.0016302706837781112, A4: 0.0008465800768383567\n",
      "Epoch 77, Total Loss: 0.016600799044086175, DLoss: 0.010437709986449177, A1: 0.0021007749701807783, A2: 0.001582261256937107, A3: 0.001628622476180846, A4: 0.0008514303700146477\n",
      "Epoch 78, Total Loss: 0.009751278289001096, DLoss: 0.00372735808262157, A1: 0.002067839709343389, A2: 0.0015466502790910785, A3: 0.0015814809028101577, A4: 0.0008279493121583734\n",
      "Epoch 79, Total Loss: 0.008287712895650078, DLoss: 0.002315772418494982, A1: 0.002046585589943623, A2: 0.0015294878486946056, A3: 0.0015789720277428966, A4: 0.0008168950071690646\n",
      "Epoch 80, Total Loss: 0.010386786725245078, DLoss: 0.004461876940662147, A1: 0.0020294989755546503, A2: 0.0015312212334141473, A3: 0.0015525594808753918, A4: 0.0008116300960616421\n",
      "Epoch 81, Validation Loss Weighted: 2.727385997772217, | Validation Unweighted Avg: 0.0022399681620299816\n",
      "Epoch 81, Total Loss: 0.013846077282108704, DLoss: 0.007959700732449577, A1: 0.002030726517973976, A2: 0.0015224299785173074, A3: 0.0015292402706108988, A4: 0.0008039797804403034\n",
      "Epoch 82, Total Loss: 0.007734812274363569, DLoss: 0.0019516919214683267, A1: 0.0020078834596047685, A2: 0.0014940849751424552, A3: 0.0014946191267385571, A4: 0.0007865328006366987\n",
      "Epoch 83, Total Loss: 0.007314313296228647, DLoss: 0.0016216342703211757, A1: 0.0019916478401600297, A2: 0.0014811759600309993, A3: 0.0014556398620532657, A4: 0.0007642153463993137\n",
      "Epoch 84, Total Loss: 0.008482579932421108, DLoss: 0.0028609451352332887, A1: 0.0019650006873152136, A2: 0.001465229563340968, A3: 0.0014228135910922322, A4: 0.0007685909717772368\n",
      "Epoch 85, Total Loss: 0.010551398658108982, DLoss: 0.0049849071961943995, A1: 0.0019602252181026747, A2: 0.0014454138589578426, A3: 0.0013970281669489024, A4: 0.0007638242273308358\n",
      "Epoch 86, Total Loss: 0.010031198713378134, DLoss: 0.004493386275994867, A1: 0.001948450818996538, A2: 0.001440019882548685, A3: 0.0013808005685198375, A4: 0.0007685412050870416\n",
      "Epoch 87, Total Loss: 0.007317978107709099, DLoss: 0.0018862153821167093, A1: 0.0019321394819681618, A2: 0.0014075709705304524, A3: 0.0013416922468670897, A4: 0.000750360050237346\n",
      "Epoch 88, Total Loss: 0.009545315233778886, DLoss: 0.004150383177263641, A1: 0.001937062863726169, A2: 0.00139234784133309, A3: 0.0013194289902457967, A4: 0.0007460923814505804\n",
      "Epoch 89, Total Loss: 0.01232580901652744, DLoss: 0.006937234654205068, A1: 0.001927003825337372, A2: 0.001392765366472304, A3: 0.0013172454788053238, A4: 0.0007515596897891638\n",
      "Epoch 90, Total Loss: 0.008426798706535588, DLoss: 0.0031318456648229834, A1: 0.0019312197305473753, A2: 0.0013527563376225192, A3: 0.001277970108309422, A4: 0.00073300686407575\n",
      "Epoch 91, Validation Loss Weighted: 2.4958322048187256, | Validation Unweighted Avg: 0.001742490567266941\n",
      "Epoch 91, Total Loss: 0.009466296029065482, DLoss: 0.0042134669117909985, A1: 0.0019168109453113918, A2: 0.0013402760924443348, A3: 0.0012694825563812628, A4: 0.0007262595341506478\n",
      "Epoch 92, Total Loss: 0.007803660569797186, DLoss: 0.00264557174496903, A1: 0.0018868819117249752, A2: 0.0013107648495034399, A3: 0.0012382233727046034, A4: 0.0007222186974765711\n",
      "Epoch 93, Total Loss: 0.008983987384602767, DLoss: 0.0038777453114479696, A1: 0.0018800547293556685, A2: 0.0012925812536691823, A3: 0.0012125382861334153, A4: 0.000721067823211673\n",
      "Epoch 94, Total Loss: 0.009906895276667042, DLoss: 0.004864400962833315, A1: 0.0018515289390713654, A2: 0.0012807256665176034, A3: 0.0011979822277102027, A4: 0.0007122575304740828\n",
      "Epoch 95, Total Loss: 0.010863012718883428, DLoss: 0.005834532606769988, A1: 0.0018241387254833666, A2: 0.0012808748257653365, A3: 0.0012005010812903161, A4: 0.000722965509438919\n",
      "Epoch 96, Total Loss: 0.006059340391286903, DLoss: 0.0012019355112980437, A1: 0.001761293466816741, A2: 0.0012446128839986737, A3: 0.0011526759690075944, A4: 0.0006988225648290774\n",
      "Epoch 97, Total Loss: 0.005363152755572545, DLoss: 0.0005335497243901376, A1: 0.0017773431167535654, A2: 0.001224683188113638, A3: 0.0011245572559842416, A4: 0.0007030194678505227\n",
      "Epoch 98, Total Loss: 0.009703333444089036, DLoss: 0.0049131429329546256, A1: 0.001748140461594713, A2: 0.0012206482568713412, A3: 0.001122944223937917, A4: 0.0006984575718723301\n",
      "Epoch 99, Total Loss: 0.015014643427407878, DLoss: 0.010140859740204178, A1: 0.0017823078188071535, A2: 0.0012308310995153575, A3: 0.0011345789466180247, A4: 0.0007260657880992353\n",
      "Epoch 100, Total Loss: 0.006106239081021737, DLoss: 0.0014155748803287597, A1: 0.0017237690970597957, A2: 0.0011866371188344518, A3: 0.0010858506549001586, A4: 0.0006944072931715213\n",
      "Epoch 101, Validation Loss Weighted: 2.370051383972168, | Validation Unweighted Avg: 0.001294048153795302\n",
      "Epoch 101, Total Loss: 0.005376850617837838, DLoss: 0.0007828246534038705, A1: 0.001687269187161953, A2: 0.0011693899035559628, A3: 0.0010659125981460833, A4: 0.0006714542808615741\n",
      "Epoch 102, Total Loss: 0.008173709671245888, DLoss: 0.0035502438015431502, A1: 0.001704754743100652, A2: 0.0011596142959950323, A3: 0.001061365464226004, A4: 0.0006977313653227281\n",
      "Epoch 103, Total Loss: 0.010526901674562727, DLoss: 0.005892023440207016, A1: 0.001705667237481314, A2: 0.0011587420269444754, A3: 0.0010625545975265348, A4: 0.0007079143427703953\n",
      "Epoch 104, Total Loss: 0.007883156011071564, DLoss: 0.0033607832074101845, A1: 0.0016642209049843421, A2: 0.0011366188171615993, A3: 0.0010454808399531018, A4: 0.0006760522144097856\n",
      "Epoch 105, Total Loss: 0.006014433649728413, DLoss: 0.001519216370930239, A1: 0.0016520663677841764, A2: 0.001129815507473805, A3: 0.0010251514227341183, A4: 0.0006881840161771503\n",
      "Epoch 106, Total Loss: 0.004976784864398227, DLoss: 0.0005413829969091404, A1: 0.001629763115091588, A2: 0.0011223215851879848, A3: 0.0010078312921739945, A4: 0.0006754858586976876\n",
      "Epoch 107, Total Loss: 0.0052141017650931395, DLoss: 0.0008553959552955348, A1: 0.0015909548149052584, A2: 0.0010977166443560484, A3: 0.0009854013516318943, A4: 0.0006846330114554307\n",
      "Epoch 108, Total Loss: 0.009029896060978485, DLoss: 0.004675076819099062, A1: 0.0015764713899856858, A2: 0.001093679811641447, A3: 0.0009911124853136822, A4: 0.0006935555477287959\n",
      "Epoch 109, Total Loss: 0.017425654381936925, DLoss: 0.012976902307656763, A1: 0.001605464049895421, A2: 0.001119162235012151, A3: 0.0010191202362792948, A4: 0.0007050055285700099\n",
      "Epoch 110, Total Loss: 0.009873347107680853, DLoss: 0.005442407229003782, A1: 0.0016132921693497338, A2: 0.0011182007461849768, A3: 0.0010107632473095277, A4: 0.000688683689143297\n",
      "Epoch 111, Validation Loss Weighted: 2.2178423404693604, | Validation Unweighted Avg: 0.001088020158931613\n",
      "Epoch 111, Total Loss: 0.005202195470073175, DLoss: 0.0009226834021815607, A1: 0.0015569533249204556, A2: 0.0010872115506076212, A3: 0.0009694265685208269, A4: 0.0006659206329211223\n",
      "Epoch 112, Total Loss: 0.004849633641689169, DLoss: 0.0006350928188112448, A1: 0.0015151182655245067, A2: 0.001063378355254165, A3: 0.0009600668000612958, A4: 0.0006759774028316976\n",
      "Epoch 113, Total Loss: 0.005046963304895061, DLoss: 0.0008886781020588718, A1: 0.0014869059801805467, A2: 0.0010564496190371838, A3: 0.0009497708589399487, A4: 0.0006651587398333835\n",
      "Epoch 114, Total Loss: 0.006152525092528032, DLoss: 0.002062330517120162, A1: 0.0014666961375703316, A2: 0.0010412158268908653, A3: 0.0009275384011165112, A4: 0.0006547442047535283\n",
      "Epoch 115, Total Loss: 0.007656606618018652, DLoss: 0.003579599795855095, A1: 0.0014559515479380604, A2: 0.0010301111747222339, A3: 0.000926780367444735, A4: 0.0006641637422944768\n",
      "Epoch 116, Total Loss: 0.008727233268489893, DLoss: 0.004614384948085485, A1: 0.0014565274337655864, A2: 0.0010290033118177564, A3: 0.0009489774017062948, A4: 0.0006783401912798581\n",
      "Epoch 117, Total Loss: 0.006624486692122776, DLoss: 0.0025942109770271185, A1: 0.0014310063671225427, A2: 0.0010125389934811128, A3: 0.0009299314523972995, A4: 0.0006567989233190019\n",
      "Epoch 118, Total Loss: 0.007007001127666709, DLoss: 0.0029807389408895027, A1: 0.0014122323409537785, A2: 0.001001741667410401, A3: 0.0009308984889767916, A4: 0.0006813896820775964\n",
      "Epoch 119, Total Loss: 0.0065980161259755156, DLoss: 0.002601360881685237, A1: 0.0013941050340442664, A2: 0.0009925145347634415, A3: 0.0009325913389065219, A4: 0.0006774443162199036\n",
      "Epoch 120, Total Loss: 0.005026757718455469, DLoss: 0.0011086726495374353, A1: 0.0013677075795633508, A2: 0.0009924799679455728, A3: 0.0009042338133440353, A4: 0.000653663702839615\n",
      "Epoch 121, Validation Loss Weighted: 2.1503961086273193, | Validation Unweighted Avg: 0.001834727474488318\n",
      "Epoch 121, Total Loss: 0.006613302193793722, DLoss: 0.0027153336561406664, A1: 0.0013578836191630796, A2: 0.0009810853677746226, A3: 0.0009069113417023222, A4: 0.0006520882169173671\n",
      "Epoch 122, Total Loss: 0.008271605007096448, DLoss: 0.004339135328402997, A1: 0.0013539898885937874, A2: 0.0009917562646561155, A3: 0.0009140562297530281, A4: 0.0006726672880507646\n",
      "Epoch 123, Total Loss: 0.010237169890685684, DLoss: 0.006234713649874638, A1: 0.0013863872065981427, A2: 0.0010300472227920015, A3: 0.000914156201774445, A4: 0.0006718656088527165\n",
      "Epoch 124, Total Loss: 0.005883396483047611, DLoss: 0.0019605670746056024, A1: 0.0013599920507245274, A2: 0.0010160543974713867, A3: 0.0008922779178895077, A4: 0.0006545050016856376\n",
      "Epoch 125, Total Loss: 0.004530337846848521, DLoss: 0.000636262184773593, A1: 0.0013497325102269481, A2: 0.0010049547340630935, A3: 0.0008880973896752535, A4: 0.0006512909994357457\n",
      "Epoch 126, Total Loss: 0.0050110500332349065, DLoss: 0.0011308753004083303, A1: 0.0013264467532942282, A2: 0.0010044180614138234, A3: 0.0008917002884448845, A4: 0.0006576096344774255\n",
      "Epoch 127, Total Loss: 0.00671307454414835, DLoss: 0.002859839390988451, A1: 0.0013151944300096312, A2: 0.0010107582118342345, A3: 0.0008824656378155024, A4: 0.0006448168820663175\n",
      "Epoch 128, Total Loss: 0.014877727848414162, DLoss: 0.010894950957365588, A1: 0.0013646136864157268, A2: 0.0010298772816895508, A3: 0.0009146365534186109, A4: 0.0006736493141281905\n",
      "Epoch 129, Total Loss: 0.004975376100628637, DLoss: 0.0010963658188715769, A1: 0.0013266079122496938, A2: 0.0010111449315445497, A3: 0.0008911966873380483, A4: 0.0006500607499550494\n",
      "Epoch 130, Total Loss: 0.003988439788703214, DLoss: 0.00017422153293657837, A1: 0.0013018742900881492, A2: 0.0009989762051356428, A3: 0.0008724468979132573, A4: 0.0006409208537992196\n",
      "Epoch 131, Validation Loss Weighted: 2.0142598152160645, | Validation Unweighted Avg: 0.0010477898176759481\n",
      "Epoch 131, Total Loss: 0.00399997982180635, DLoss: 0.00025691776122171593, A1: 0.0012784473608586598, A2: 0.0009718538358191092, A3: 0.000859657915845508, A4: 0.0006331029201150655\n",
      "Epoch 132, Total Loss: 0.005073804617330263, DLoss: 0.001307397192365236, A1: 0.0012640983488050883, A2: 0.0009727095834353928, A3: 0.000873322021801538, A4: 0.0006562774715100452\n",
      "Epoch 133, Total Loss: 0.013582858752819118, DLoss: 0.009679330228457482, A1: 0.0013154882086382712, A2: 0.000995629720538008, A3: 0.0009007718664758034, A4: 0.0006916386972410361\n",
      "Epoch 134, Total Loss: 0.006174228456008925, DLoss: 0.0023887638027769173, A1: 0.0012917382891993673, A2: 0.000977799426453368, A3: 0.0008699651850375548, A4: 0.0006459617495569215\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m avg_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Monitor gradients before clipping and stepping\u001b[39;00m\n\u001b[1;32m     50\u001b[0m all_gradients\u001b[38;5;241m.\u001b[39mappend(image_world\u001b[38;5;241m.\u001b[39mstore_gradients(model))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_gradients = []\n",
    "model.train()  \n",
    "\n",
    "for epoch in range(3000):  # epochs\n",
    "    avg_loss = 0\n",
    "    avg_decode_loss, avg_first_loss, avg_second_loss, avg_third_loss, avg_fourth_loss = 0, 0, 0, 0, 0\n",
    "    raw_gradients = []\n",
    "\n",
    "    for t in random.sample(train_data, len(train_data)):  # sample through all data in random order each epoch\n",
    "        # Get reconstruction loss to help ground abstract state\n",
    "        decoded_values, transition_probabilities = model(t[0])\n",
    "        decode_loss = F.mse_loss(decoded_values[0], t[0], reduction='sum')\n",
    "\n",
    "        # Flatten transition probabilities to then weigh with loss of each predicted state at each layer\n",
    "        first = transition_probabilities[0].view(-1,1,1,1)\n",
    "        second = transition_probabilities[1].view(-1,1,1,1)\n",
    "        third = transition_probabilities[2].view(-1,1,1,1)\n",
    "        fourth = transition_probabilities[3].view(-1,1,1,1)\n",
    "\n",
    "        #Weighted Transitions\n",
    "        # first_loss = (F.mse_loss(decoded_values[1], t[1], reduction='none') * first).sum()\n",
    "        # second_loss = (F.mse_loss(decoded_values[2], t[2], reduction='none') * second).sum()\n",
    "        # third_loss = (F.mse_loss(decoded_values[3], t[3], reduction='none') * third).sum()\n",
    "        # fourth_loss = (F.mse_loss(decoded_values[4], t[4], reduction='none') * fourth).sum()\n",
    "\n",
    "        #Greedy Policy (Squeezing to eliminate batch and channel dimensions)\n",
    "        first_loss = (F.mse_loss(decoded_values[1][first.argmax()].squeeze(0),t[1].squeeze(0).squeeze(0)))\n",
    "        second_loss = (F.mse_loss(decoded_values[2][second.argmax()].squeeze(0),t[2].squeeze(0).squeeze(0)))\n",
    "        third_loss = (F.mse_loss(decoded_values[3][third.argmax()].squeeze(0),t[3].squeeze(0).squeeze(0)))\n",
    "        fourth_loss = (F.mse_loss(decoded_values[4][fourth.argmax()].squeeze(0),t[4].squeeze(0).squeeze(0)))\n",
    "\n",
    "        total_loss = first_loss + second_loss  + third_loss  + fourth_loss + decode_loss\n",
    "\n",
    "        # break if total loss is nan\n",
    "        if torch.isnan(total_loss):\n",
    "            raise ValueError(\"NAN LOSS\")\n",
    "\n",
    "\n",
    "        avg_decode_loss += decode_loss.item()\n",
    "        avg_first_loss += first_loss.item()\n",
    "        avg_second_loss += second_loss.item()\n",
    "        avg_third_loss += third_loss.item()\n",
    "        avg_fourth_loss += fourth_loss.item()\n",
    "        avg_loss += total_loss.item()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        # Monitor gradients before clipping and stepping\n",
    "        all_gradients.append(image_world.store_gradients(model))\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0: \n",
    "        #print just validation\n",
    "        print(f\"Epoch {epoch + 1}, Validation Loss Weighted: {image_world.validate(model, valid_data,weighted=True)}, | Validation Unweighted Avg: {image_world.validate(model, valid_data,weighted=False)/5}\")\n",
    "\n",
    "    #Individual Lossses\n",
    "    avg_decode_loss = avg_decode_loss / len(train_data)\n",
    "    avg_first_loss = avg_first_loss / len(train_data)\n",
    "    avg_second_loss = avg_second_loss / len(train_data)\n",
    "    avg_third_loss = avg_third_loss / len(train_data)\n",
    "    avg_fourth_loss = avg_fourth_loss / len(train_data)\n",
    "    #Full Loss\n",
    "    avg_train_loss = avg_loss / len(train_data)\n",
    "    \n",
    "    # avg_decode_loss = (avg_decode_loss / len(train_data))/0.0025\n",
    "    # avg_first_loss = (avg_first_loss / len(train_data))/0.0025\n",
    "    # avg_second_loss = (avg_second_loss / len(train_data))/0.0025\n",
    "    # avg_third_loss = (avg_third_loss / len(train_data))/0.0025\n",
    "    # avg_fourth_loss = (avg_fourth_loss / len(train_data))/0.0025\n",
    "    # #Full Loss\n",
    "    # avg_train_loss = (avg_loss / len(train_data))/0.0025    \n",
    "    # if (avg_decode_loss + avg_first_loss + avg_second_loss + avg_third_loss + avg_fourth_loss) < 2:\n",
    "    #     break\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Total Loss: {avg_train_loss}, DLoss: {avg_decode_loss}, A1: {avg_first_loss}, A2: {avg_second_loss}, A3: {avg_third_loss}, A4: {avg_fourth_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "#torch.save(model.state_dict(), 'model.pth')\n",
    "#load model\n",
    "#model.load_state_dict(torch.load('model_1500.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition_fun: 0.0\n",
      "decoder.fc.weight: 0.1\n",
      "decoder.fc.bias: 0.01\n",
      "decoder.deconv1.weight: 0.14\n",
      "decoder.deconv1.bias: 0.02\n",
      "decoder.deconv2.weight: 0.15\n",
      "decoder.deconv2.bias: 0.05\n",
      "decoder.final_conv.weight: 0.29\n",
      "decoder.final_conv.bias: 0.04\n",
      "encoder.cnn_encoder.conv1.weight: 0.02\n",
      "encoder.cnn_encoder.conv1.bias: 0.03\n",
      "encoder.cnn_encoder.bn1.weight: 0.0\n",
      "encoder.cnn_encoder.bn1.bias: 0.0\n",
      "encoder.cnn_encoder.conv2.weight: 0.07\n",
      "encoder.cnn_encoder.conv2.bias: 0.06\n",
      "encoder.cnn_encoder.bn2.weight: 0.01\n",
      "encoder.cnn_encoder.bn2.bias: 0.03\n",
      "encoder.cnn_encoder.conv3.weight: 0.07\n",
      "encoder.cnn_encoder.conv3.bias: 0.03\n",
      "encoder.cnn_encoder.bn3.weight: 0.01\n",
      "encoder.cnn_encoder.bn3.bias: 0.01\n",
      "encoder.cnn_encoder.residual_conv.weight: 0.02\n",
      "encoder.cnn_encoder.residual_conv.bias: 0.02\n",
      "encoder.linear.weight: 0.14\n",
      "encoder.linear.bias: 0.01\n"
     ]
    }
   ],
   "source": [
    "for name, num in all_gradients[-1]:\n",
    "    print(name +':', round(num, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>View Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Original Start State:\n",
      " tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "Max True Original Start State: (7, 8)\n",
      "Decoded Next States\n",
      "Action: 0\n",
      "Next State:\n",
      " tensor([[ 0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0.],\n",
      "        [ 0.,  0., -0., -0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0., -0.,  0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0.,  0.,  0.,  0.,  0., -0.,  0., -0.,  0.],\n",
      "        [ 0.,  0.,  0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0.],\n",
      "        [-0.,  0.,  0.,  0., -0.,  0., -1., -0., -0., -0.,  0.,  0.],\n",
      "        [ 0., -0.,  0., -0., -0., -0.,  0., -0., -0., -0., -0.,  0.],\n",
      "        [ 0.,  0., -0., -0., -0.,  0., -0., -0., -0., -0.,  0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0., -0., -0., -0., -0., -0.],\n",
      "        [-0., -0., -0., -0.,  0.,  0., -0., -0.,  0., -0., -0., -0.],\n",
      "        [ 0.,  0., -0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0.,  0.]])\n",
      "Max Value Decoded: (8, 8)\n",
      "Action: 1\n",
      "Next State:\n",
      " tensor([[ 0., -0., -0., -0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0.],\n",
      "        [ 0.,  0.,  0., -0.,  0., -0., -0.,  0., -0.,  0., -0.,  0.],\n",
      "        [ 0., -0., -0., -0.,  0., -0.,  0., -0.,  0., -0., -0.,  0.],\n",
      "        [ 0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.],\n",
      "        [ 0.,  0.,  0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0., -0., -0.,  0., -0.,  0., -0., -0.],\n",
      "        [ 0., -0.,  0.,  0., -0.,  0., -1.,  0., -0.,  0.,  0.,  0.],\n",
      "        [ 0., -0., -0.,  0.,  0.,  0.,  0., -0., -0., -0., -0.,  0.],\n",
      "        [-0.,  0., -0.,  0., -0., -0., -0., -0.,  0., -0., -0., -0.],\n",
      "        [-0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0., -0.,  0.,  0.],\n",
      "        [-0., -0., -0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0., -0.]])\n",
      "Max Value Decoded: (7, 9)\n",
      "Action: 2\n",
      "Next State:\n",
      " tensor([[ 0.,  0.,  0., -0., -0., -0.,  0., -0.,  0.,  0., -0.,  0.],\n",
      "        [ 0., -0., -0., -0.,  0.,  0., -0.,  0., -0.,  0., -0.,  0.],\n",
      "        [ 0.,  0., -0., -0., -0., -0.,  0., -0.,  0., -0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0., -0., -0., -0.],\n",
      "        [ 0.,  0., -0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0.],\n",
      "        [ 0., -0., -0.,  0., -0.,  0.,  0.,  0., -0.,  0., -0., -0.],\n",
      "        [-0.,  0.,  0.,  0., -0.,  0., -1., -0., -0.,  0.,  0.,  0.],\n",
      "        [ 0., -0.,  0., -0.,  0.,  0., -0., -0., -0., -0.,  0.,  0.],\n",
      "        [ 0., -0.,  0., -0.,  0., -0., -0., -0.,  0., -0.,  0., -0.],\n",
      "        [ 0., -0., -0.,  0., -0.,  0.,  0., -0., -0.,  0., -0., -0.],\n",
      "        [ 0., -0., -0., -0.,  0.,  0., -0.,  0., -0.,  0., -0., -0.],\n",
      "        [ 0.,  0., -0.,  0.,  0.,  0., -0., -0.,  0.,  0., -0.,  0.]])\n",
      "Max Value Decoded: (8, 8)\n",
      "Action: 3\n",
      "Next State:\n",
      " tensor([[ 0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0.],\n",
      "        [ 0., -0., -0., -0.,  0., -0.,  0., -0., -0.,  0., -0.,  0.],\n",
      "        [ 0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0., -0.,  0., -0.],\n",
      "        [-0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0.],\n",
      "        [ 0.,  0., -0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.],\n",
      "        [-0.,  0.,  0.,  0., -0.,  0., -1., -0., -0., -0.,  0., -0.],\n",
      "        [ 0., -0.,  0., -0.,  0., -0., -0., -0., -0.,  0., -0.,  0.],\n",
      "        [-0., -0., -0., -0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0.],\n",
      "        [-0., -0.,  0., -0.,  0., -0., -0., -0.,  0.,  0.,  0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "Max Value Decoded: (8, 8)\n"
     ]
    }
   ],
   "source": [
    "image_world.action_viewer(model,start_state=train_data[0][0],actions = [0,1,2,3],shrink=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Path\n",
      "7 8\n",
      "8 8\n",
      "8 9\n",
      "8 10\n",
      "9 10\n",
      "True Original Start State:\n",
      " tensor([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0., -1.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "Max True Original Start State: (7, 8)\n",
      "Decoded Next States\n",
      "Action: 3\n",
      "Next State:\n",
      " tensor([[ 0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0.],\n",
      "        [ 0., -0., -0., -0.,  0., -0.,  0., -0., -0.,  0., -0.,  0.],\n",
      "        [ 0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0., -0.,  0., -0.],\n",
      "        [-0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0.],\n",
      "        [ 0.,  0., -0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.],\n",
      "        [-0.,  0.,  0.,  0., -0.,  0., -1., -0., -0., -0.,  0., -0.],\n",
      "        [ 0., -0.,  0., -0.,  0., -0., -0., -0., -0.,  0., -0.,  0.],\n",
      "        [-0., -0., -0., -0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0.],\n",
      "        [-0., -0.,  0., -0.,  0., -0., -0., -0.,  0.,  0.,  0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "Max Value Decoded: (8, 8)\n",
      "Action: 1\n",
      "Next State:\n",
      " tensor([[ 0., -0., -0., -0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0.],\n",
      "        [ 0.,  0.,  0., -0.,  0., -0., -0.,  0., -0.,  0., -0.,  0.],\n",
      "        [ 0., -0., -0., -0.,  0., -0.,  0., -0.,  0., -0., -0.,  0.],\n",
      "        [ 0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.],\n",
      "        [ 0.,  0.,  0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0., -0., -0.,  0., -0.,  0., -0., -0.],\n",
      "        [ 0., -0.,  0.,  0., -0.,  0., -1.,  0., -0.,  0.,  0.,  0.],\n",
      "        [ 0., -0., -0.,  0.,  0.,  0.,  0., -0., -0., -0., -0.,  0.],\n",
      "        [-0.,  0., -0.,  0., -0., -0., -0., -0.,  0., -0., -0., -0.],\n",
      "        [-0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0., -0.,  0.,  0.],\n",
      "        [-0., -0., -0.,  0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0., -0.]])\n",
      "Max Value Decoded: (7, 9)\n",
      "Action: 3\n",
      "Next State:\n",
      " tensor([[ 0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0.],\n",
      "        [ 0., -0., -0., -0.,  0., -0.,  0., -0., -0.,  0., -0.,  0.],\n",
      "        [ 0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0., -0.,  0., -0.],\n",
      "        [-0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0.],\n",
      "        [ 0.,  0., -0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.],\n",
      "        [-0.,  0.,  0.,  0., -0.,  0., -1., -0., -0., -0.,  0., -0.],\n",
      "        [ 0., -0.,  0., -0.,  0., -0., -0., -0., -0.,  0., -0.,  0.],\n",
      "        [-0., -0., -0., -0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0.],\n",
      "        [-0., -0.,  0., -0.,  0., -0., -0., -0.,  0.,  0.,  0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "Max Value Decoded: (8, 8)\n",
      "Action: 3\n",
      "Next State:\n",
      " tensor([[ 0., -0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0.],\n",
      "        [ 0., -0., -0., -0.,  0., -0.,  0., -0., -0.,  0., -0.,  0.],\n",
      "        [ 0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0., -0.,  0., -0.],\n",
      "        [-0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0., -0., -0.],\n",
      "        [ 0.,  0., -0., -0.,  0.,  0., -0.,  0.,  0.,  0., -0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0.],\n",
      "        [-0.,  0.,  0.,  0., -0.,  0., -1., -0., -0., -0.,  0., -0.],\n",
      "        [ 0., -0.,  0., -0.,  0., -0., -0., -0., -0.,  0., -0.,  0.],\n",
      "        [-0., -0., -0., -0.,  0.,  0., -0., -0.,  0., -0.,  0.,  0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0.],\n",
      "        [-0., -0.,  0., -0.,  0., -0., -0., -0.,  0.,  0.,  0., -0.],\n",
      "        [-0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
      "Max Value Decoded: (8, 8)\n"
     ]
    }
   ],
   "source": [
    "image_world.view_greedy_path(model,start_state=train_data[0],shrink=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Action 1    1.322873\n",
       "Action 2    0.630054\n",
       "Action 3    1.450892\n",
       "Action 4    1.317476\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_df = image_world.get_action_df(model,valid_data)\n",
    "action_df.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action 1</th>\n",
       "      <th>Action 2</th>\n",
       "      <th>Action 3</th>\n",
       "      <th>Action 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Action 1  Action 2  Action 3  Action 4\n",
       "0         3         1         3         1\n",
       "1         2         3         1         3\n",
       "2         3         2         0         3\n",
       "3         0         1         3         2\n",
       "4         3         3         3         3\n",
       "5         1         2         1         3\n",
       "6         0         2         1         3\n",
       "7         3         1         3         1\n",
       "8         2         3         1         3\n",
       "9         2         3         1         3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_rows = action_df.drop_duplicates()\n",
    "len(action_df), len(unique_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
