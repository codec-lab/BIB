{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.functional import F\n",
    "import random\n",
    "from treeqn_traj_simplest_2 import TreeQN\n",
    "from torch.optim import Adam\n",
    "from image_world_2 import get_train_test_data\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03999999910593033\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(5,5)\n",
    "b = torch.zeros(5,5)\n",
    "b[0][0] = 1\n",
    "baseline_loss = F.mse_loss(a,b).item()\n",
    "print(baseline_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Checkerboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_train_trajectories, stacked_test_trajectories = get_train_test_data(distance=4) #distance agent starts from goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Einsum Transiton\n"
     ]
    }
   ],
   "source": [
    "input_shape = stacked_train_trajectories[0].shape# minimum size #train_data[0][0].shape\n",
    "num_actions = 4\n",
    "tree_depth = 4\n",
    "embedding_dim = 64\n",
    "gamma = 1 \n",
    "decode_dropout = 0.5\n",
    "t1 =True#True is Einsum. False +dx \n",
    "model = TreeQN(input_shape=input_shape, num_actions=num_actions, tree_depth=tree_depth, embedding_dim=embedding_dim, gamma=gamma,decode_dropout=decode_dropout,t1=t1)\n",
    "cuz_encoder = model.encoder\n",
    "cuz_decoder = model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = cuz_encoder(stacked_train_trajectories[0])\n",
    "# print(z.shape)\n",
    "# z = cuz_decoder(z)\n",
    "# print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Autoencoder Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam(list(cuz_encoder.parameters())+list(cuz_decoder.parameters()), lr=1e-4,weight_decay=1e-5)\n",
    "# max_val_loss = 1000\n",
    "# max_val_iter=-1\n",
    "# for i in range(100): \n",
    "#     cuz_decoder.train()\n",
    "#     avg_loss =0\n",
    "#     train_count = 0\n",
    "#     valid_count = 0\n",
    "#     for traj in random.sample(stacked_train_trajectories, len(stacked_train_trajectories)):\n",
    "#         noisy_traj = traj.clone() + torch.randn_like(traj) * 0.1\n",
    "#         train_count +=1\n",
    "#         encodeded_tensor = cuz_encoder(noisy_traj)\n",
    "#         decoded_tensor = cuz_decoder(encodeded_tensor)#model.decoder(encodeded_tensor)\n",
    "#         loss = F.mse_loss(decoded_tensor, noisy_traj)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         avg_loss += loss.item()\n",
    "#     if i % 10 ==0:\n",
    "#         cuz_decoder.eval()\n",
    "#         valid_loss = 0\n",
    "#         with torch.no_grad():\n",
    "#             for traj in stacked_test_trajectories:\n",
    "#                 valid_count +=1\n",
    "#                 encodeded_tensor = cuz_encoder(traj)\n",
    "#                 decoded_tensor = cuz_decoder(encodeded_tensor)#model.decoder(encodeded_tensor)\n",
    "#                 loss = F.mse_loss(decoded_tensor, traj)\n",
    "#                 valid_loss += loss.item()\n",
    "#         if (valid_loss/valid_count) < max_val_loss:\n",
    "#             max_val_loss = valid_loss/valid_count\n",
    "#             max_val_iter = i\n",
    "#         print('VALID LOSS',i,(valid_loss/valid_count)/baseline_loss)\n",
    "#         print('Train Loss',i,(avg_loss/train_count)/baseline_loss,'\\n') #Want less than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "# for traj in stacked_test_trajectories:\n",
    "#     for tensor in traj:\n",
    "#         cuz_decoder.eval()\n",
    "#         tensor = tensor.unsqueeze(0)\n",
    "#         encodeded_tensor = cuz_encoder(tensor)\n",
    "#         decoded_tensor = cuz_decoder(encodeded_tensor) \n",
    "#         loss = F.mse_loss(torch.round(decoded_tensor), tensor)\n",
    "#         losses.append(loss.item())\n",
    "# print(pd.Series(losses).value_counts())\n",
    "# test_train = stacked_test_trajectories[0][0].unsqueeze(0)\n",
    "# encodeded_tensor = cuz_encoder(test_train)\n",
    "# decoded_tensor = cuz_decoder(encodeded_tensor)\n",
    "# print(torch.round(test_train[0]))\n",
    "# print(torch.round(decoded_tensor[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Train IRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss 2.622971264503768\n",
      "Epoch 1, Total Loss: 3.746865301824388, DLoss: 4.3535253031823995, A1: 3.968190602963609, A2: 3.60785776574821, A3: 3.6984902594474187, A4: 3.106262592108342\n",
      "Epoch 2, Total Loss: 2.374169383824219, DLoss: 2.327236233071188, A1: 2.476067557181094, A2: 2.576101445861136, A3: 2.583318334144133, A4: 1.9081233531619557\n",
      "Epoch 3, Total Loss: 1.9501329271009245, DLoss: 1.821340847850587, A1: 2.110959266285146, A2: 2.293524402951651, A3: 2.163997848100339, A4: 1.360842300047582\n",
      "Epoch 4, Total Loss: 1.7974400933546308, DLoss: 1.6134775611503909, A1: 1.966388443593308, A2: 2.1402133448011145, A3: 2.066536860236719, A4: 1.2005842593199278\n",
      "Epoch 5, Total Loss: 1.6887659452637067, DLoss: 1.4791449250003765, A1: 1.8317938271136003, A2: 2.0220130926190145, A3: 1.9775171346522111, A4: 1.1333607349335968\n"
     ]
    }
   ],
   "source": [
    "all_gradients = []\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "weighted=False\n",
    "for epoch in range(3000):  # epochs\n",
    "    avg_loss = 0\n",
    "    avg_decode_loss, avg_first_loss, avg_second_loss, avg_third_loss, avg_fourth_loss = 0, 0, 0, 0, 0\n",
    "    model.train()\n",
    "    for traj in random.sample(stacked_train_trajectories, len(stacked_train_trajectories)):  # sample through all data in random order each epoch\n",
    "        # Get reconstruction loss to help ground abstract state\n",
    "        ###NO NOISE YET!\n",
    "        decoded_values, transition_probabilities = model(traj[0].unsqueeze(0))\n",
    "        decode_loss = F.mse_loss(decoded_values[0], traj[0].unsqueeze(0))\n",
    "\n",
    "        # Flatten transition probabilities to then weigh with loss of each predicted state at each layer\n",
    "        first = transition_probabilities[0].view(-1,1,1,1)\n",
    "        second = transition_probabilities[1].view(-1,1,1,1)\n",
    "        third = transition_probabilities[2].view(-1,1,1,1)\n",
    "        fourth = transition_probabilities[3].view(-1,1,1,1)\n",
    "\n",
    "        if weighted:\n",
    "            #Weighted Transitions\n",
    "            first_loss = (F.mse_loss(decoded_values[1], traj[1].unsqueeze(0), reduction='none') * first).sum()\n",
    "            second_loss = (F.mse_loss(decoded_values[2], traj[2].unsqueeze(0), reduction='none') * second).sum()\n",
    "            third_loss = (F.mse_loss(decoded_values[3], traj[3].unsqueeze(0), reduction='none') * third).sum()\n",
    "            fourth_loss = (F.mse_loss(decoded_values[4], traj[4].unsqueeze(0), reduction='none') * fourth).sum()\n",
    "        else:\n",
    "            #Greedy Policy (Squeezing to eliminate batch and channel dimensions)\n",
    "            first_loss = (F.mse_loss(decoded_values[1][first.argmax()].squeeze(0),traj[1].squeeze(0)))\n",
    "            second_loss = (F.mse_loss(decoded_values[2][second.argmax()].squeeze(0),traj[2].squeeze(0)))\n",
    "            third_loss = (F.mse_loss(decoded_values[3][third.argmax()].squeeze(0),traj[3].squeeze(0)))\n",
    "            fourth_loss = (F.mse_loss(decoded_values[4][fourth.argmax()].squeeze(0),traj[4].squeeze(0)))\n",
    "\n",
    "        total_loss = first_loss + second_loss  + third_loss  + fourth_loss + decode_loss\n",
    "\n",
    "        # break if total loss is nan\n",
    "        if torch.isnan(total_loss):\n",
    "            raise ValueError(\"NAN LOSS\")\n",
    "\n",
    "\n",
    "        avg_decode_loss += decode_loss.item()\n",
    "        avg_first_loss += first_loss.item()\n",
    "        avg_second_loss += second_loss.item()\n",
    "        avg_third_loss += third_loss.item()\n",
    "        avg_fourth_loss += fourth_loss.item()\n",
    "        avg_loss += total_loss.item()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        # Monitor gradients before clipping and stepping\n",
    "        #all_gradients.append(image_world.store_gradients(model))\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0: \n",
    "        model.eval()\n",
    "        avg_val_loss = 0\n",
    "        for traj in stacked_test_trajectories:\n",
    "            with torch.no_grad():\n",
    "                decoded_values, transition_probabilities = model(traj[0].unsqueeze(0))\n",
    "                decode_loss = F.mse_loss(decoded_values[0], traj[0].unsqueeze(0))\n",
    "\n",
    "                # Flatten transition probabilities to then weigh with loss of each predicted state at each layer\n",
    "                first = transition_probabilities[0].view(-1,1,1,1)\n",
    "                second = transition_probabilities[1].view(-1,1,1,1)\n",
    "                third = transition_probabilities[2].view(-1,1,1,1)\n",
    "                fourth = transition_probabilities[3].view(-1,1,1,1)\n",
    "\n",
    "                if weighted:\n",
    "                    #Weighted Transitions\n",
    "                    first_loss = (F.mse_loss(decoded_values[1], traj[1].unsqueeze(0), reduction='none') * first).sum()\n",
    "                    second_loss = (F.mse_loss(decoded_values[2], traj[2].unsqueeze(0), reduction='none') * second).sum()\n",
    "                    third_loss = (F.mse_loss(decoded_values[3], traj[3].unsqueeze(0), reduction='none') * third).sum()\n",
    "                    fourth_loss = (F.mse_loss(decoded_values[4], traj[4].unsqueeze(0), reduction='none') * fourth).sum()\n",
    "                else:\n",
    "                    #Greedy Policy (Squeezing to eliminate batch and channel dimensions)\n",
    "                    first_loss = (F.mse_loss(decoded_values[1][first.argmax()].squeeze(0),traj[1].squeeze(0)))\n",
    "                    second_loss = (F.mse_loss(decoded_values[2][second.argmax()].squeeze(0),traj[2].squeeze(0)))\n",
    "                    third_loss = (F.mse_loss(decoded_values[3][third.argmax()].squeeze(0),traj[3].squeeze(0)))\n",
    "                    fourth_loss = (F.mse_loss(decoded_values[4][fourth.argmax()].squeeze(0),traj[4].squeeze(0)))\n",
    "\n",
    "                avg_val_loss += (first_loss + second_loss  + third_loss  + fourth_loss + decode_loss).item()\n",
    "        print(\"Val Loss\", (avg_val_loss/(len(stacked_test_trajectories)*5))/baseline_loss)\n",
    "\n",
    "        #print just validation\n",
    "\n",
    "    #Individual Lossses\n",
    "    avg_decode_loss = (avg_decode_loss / len(stacked_train_trajectories))/baseline_loss\n",
    "    avg_first_loss = (avg_first_loss / len(stacked_train_trajectories))/baseline_loss\n",
    "    avg_second_loss = (avg_second_loss / len(stacked_train_trajectories))/baseline_loss\n",
    "    avg_third_loss = (avg_third_loss / len(stacked_train_trajectories))/baseline_loss\n",
    "    avg_fourth_loss = (avg_fourth_loss / len(stacked_train_trajectories))/baseline_loss\n",
    "    #Full Loss\n",
    "    avg_total_loss = (avg_loss / (len(stacked_train_trajectories)*5))/baseline_loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Total Loss: {avg_total_loss}, DLoss: {avg_decode_loss}, A1: {avg_first_loss}, A2: {avg_second_loss}, A3: {avg_third_loss}, A4: {avg_fourth_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
